{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYQiprE0pcVD"
   },
   "source": [
    "# CLIP zero-shot Evaluation\n",
    "This short notebook implements the dataset split into base and novel categories (see project assignment) and runs the zero-shot evaluation with CLIP.\n",
    "Feel free to copy the code contained in this notebook or to directly use this notebook as starting point for you project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UzXtFjhh7iOS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai_clip in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: ftfy in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (6.3.1)\n",
      "Requirement already satisfied: regex in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from ftfy->openai_clip) (0.2.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->openai_clip) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# we need to install clip as it is not pre-installed\n",
    "# you are also free to use open_clip which provide more models\n",
    "# https://github.com/mlfoundations/open_clip\n",
    "%pip install openai_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QtqdSOr8qqOn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from typing import TypeVar, cast, TypedDict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class CategoryLabel(TypedDict):\n",
    "    id: int\n",
    "    name: str\n",
    "    novel: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2353MHw1p24h"
   },
   "source": [
    "## Dataset Loading\n",
    "Let's get the data directly from torchvision as we have seen during labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M_1CrUhZpVCq"
   },
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return (\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], train),\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], val),\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJI_a5EizA5a"
   },
   "source": [
    "## Base and Novel categories\n",
    "To split in base and novel categories we list all dataset classes, and count their number (we already know it's 102 but let's do it properly).\n",
    "Then, we just allocate the first half to base categories and the remaining half to novel ones.\n",
    "We can do this because we are simulating a real world application, but keep in mind this will not happen out there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nfq51vd8q_5a"
   },
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "E = TypeVar(\"E\")\n",
    "def base_novel_categories(dataset: Dataset[tuple[T,E]]):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(l for _, l in dataset)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvDdoYQr2fIu"
   },
   "source": [
    "## Inspect Classes\n",
    "Let's now visualize which are the base and novel classes.\n",
    "To do so, we first get a dummy test set (without augmentations) as we are just interested in the dataset labels. Then, we split it useing `base_novel_categories`.\n",
    "Finally, we use the hard-coded CLASS_NAMES to print the class in natural language.\n",
    "\n",
    "> Note: the list of class names was only recently added to `torchvision.datasets.Flowers102`. To avoid useless errors that can occour to you, we decided to also provide such a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1743597666022,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "veGGpNDctCgR",
    "outputId": "72b17648-b4ee-42da-eb6e-37dfef3ef2f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Class Names: [(0, 'pink primrose'), (1, 'hard-leaved pocket orchid'), (2, 'canterbury bells'), (3, 'sweet pea'), (4, 'english marigold'), (5, 'tiger lily'), (6, 'moon orchid'), (7, 'bird of paradise'), (8, 'monkshood'), (9, 'globe thistle'), (10, 'snapdragon'), (11, \"colt's foot\"), (12, 'king protea'), (13, 'spear thistle'), (14, 'yellow iris'), (15, 'globe-flower'), (16, 'purple coneflower'), (17, 'peruvian lily'), (18, 'balloon flower'), (19, 'giant white arum lily'), (20, 'fire lily'), (21, 'pincushion flower'), (22, 'fritillary'), (23, 'red ginger'), (24, 'grape hyacinth'), (25, 'corn poppy'), (26, 'prince of wales feathers'), (27, 'stemless gentian'), (28, 'artichoke'), (29, 'sweet william'), (30, 'carnation'), (31, 'garden phlox'), (32, 'love in the mist'), (33, 'mexican aster'), (34, 'alpine sea holly'), (35, 'ruby-lipped cattleya'), (36, 'cape flower'), (37, 'great masterwort'), (38, 'siam tulip'), (39, 'lenten rose'), (40, 'barbeton daisy'), (41, 'daffodil'), (42, 'sword lily'), (43, 'poinsettia'), (44, 'bolero deep blue'), (45, 'wallflower'), (46, 'marigold'), (47, 'buttercup'), (48, 'oxeye daisy'), (49, 'common dandelion'), (50, 'petunia')]\n",
      "Novel Class Names: [(51, 'wild pansy'), (52, 'primula'), (53, 'sunflower'), (54, 'pelargonium'), (55, 'bishop of llandaff'), (56, 'gaura'), (57, 'geranium'), (58, 'orange dahlia'), (59, 'pink-yellow dahlia?'), (60, 'cautleya spicata'), (61, 'japanese anemone'), (62, 'black-eyed susan'), (63, 'silverbush'), (64, 'californian poppy'), (65, 'osteospermum'), (66, 'spring crocus'), (67, 'bearded iris'), (68, 'windflower'), (69, 'tree poppy'), (70, 'gazania'), (71, 'azalea'), (72, 'water lily'), (73, 'rose'), (74, 'thorn apple'), (75, 'morning glory'), (76, 'passion flower'), (77, 'lotus'), (78, 'toad lily'), (79, 'anthurium'), (80, 'frangipani'), (81, 'clematis'), (82, 'hibiscus'), (83, 'columbine'), (84, 'desert-rose'), (85, 'tree mallow'), (86, 'magnolia'), (87, 'cyclamen'), (88, 'watercress'), (89, 'canna lily'), (90, 'hippeastrum'), (91, 'bee balm'), (92, 'ball moss'), (93, 'foxglove'), (94, 'bougainvillea'), (95, 'camellia'), (96, 'mallow'), (97, 'mexican petunia'), (98, 'bromelia'), (99, 'blanket flower'), (100, 'trumpet creeper'), (101, 'blackberry lily')]\n"
     ]
    }
   ],
   "source": [
    "base_classes, novel_classes = base_novel_categories(get_data()[2])\n",
    "\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "NAMES_TO_INDEX = {x: i for i,x in enumerate(CLASS_NAMES)}\n",
    "print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
    "print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8puO1VNpzwvi"
   },
   "source": [
    "## Split Dataset\n",
    "The next step is to actually split the dataset into the base and novel categories we extract from `base_novel_categories`.\n",
    "To split the data we need the dataset (obviously) and the list of base classes. If the sample label is not part of the base categories, then it must be part of the novel ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "msOszMs2zRRu"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "E = TypeVar(\"E\")\n",
    "def split_data(dataset: Dataset[tuple[T,E]], base_classes: list[int]):\n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    # we create a set of base classes to compute the test below in O(1)\n",
    "    # this is optional and can be removed\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    labels = [l for _,l in dataset]\n",
    "    for sample_id, label in enumerate(labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    # here we create the dataset subsets\n",
    "    # the torch Subset is just a wrapper around the dataset\n",
    "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
    "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
    "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "    base_dataset = Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQZT22rE8hBw"
   },
   "source": [
    "## Extract k shots\n",
    "As the dataset already provides 10 train and validation shots, we do not need to extract them.\n",
    "Beaware that Few-Shot Adaptation papers must do this operation as most datasets count significantly more samples in both the training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KpbPRLr7WL_"
   },
   "source": [
    "## Load CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5162,
     "status": "ok",
     "timestamp": 1743597617669,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "Sh6uLZRT7YJx",
    "outputId": "25ef91c9-9879-4f50-d1eb-d2c53c194498"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM9H14899ses"
   },
   "source": [
    "## Load and Prepare Data\n",
    "Here we get the three dataset split and pass clip pre-defined augmentations.\n",
    "Then, we compute base and novel categories (in this case is redundand as we already did it before).\n",
    "Finally, se split the three datasets into base and novel categories.\n",
    "As we want to use the novel categories only for the test set, we drop `train_novel` and `val_novel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# defining the templates that we are going to use\n",
    "prompt_template: list[Callable[[str], str]] = [\n",
    "    # wrong prompt to see if is discarded\n",
    "    lambda x: f\"a photo of a {CLASS_NAMES[(NAMES_TO_INDEX[x]+1)%5]}, a type of flower.\",\n",
    "    lambda x: f\"a photo of a {x}, a type of flower.\",\n",
    "    lambda x: f\"a {x} flower.\",\n",
    "    lambda x: f\"a photo of some {x}, a type of flower.\",\n",
    "    lambda x: f\"some {x} flowers.\",\n",
    "    lambda x: f\"a close-up of a {x} flower.\",\n",
    "    lambda x: f\"an image of a {x} blossom.\",\n",
    "    lambda x: f\"a beautiful {x} in bloom.\",\n",
    "    lambda x: f\"a bunch of {x} flowers.\",\n",
    "    lambda x: f\"a macro shot of a {x} flower.\",\n",
    "    lambda x: f\"a single {x} flower.\",\n",
    "    lambda x: f\"fresh {x} flowers in a garden.\",\n",
    "    lambda x: f\"a {x} flower in the wild.\",\n",
    "    lambda x: f\"a botanical photograph of a {x}.\",\n",
    "    lambda x: f\"a vibrant {x} bloom.\",\n",
    "    lambda x: f\"a {x} plant with flowers.\",\n",
    "    lambda x: f\"a {x} growing in nature.\",\n",
    "    lambda x: f\"a {x} flower in sunlight.\",\n",
    "    lambda x: f\"a colorful {x} flower close-up.\",\n",
    "    lambda x: f\"a {x}, commonly found in gardens.\",\n",
    "    lambda x: f\"wild {x} flowers blooming.\",\n",
    "    lambda x: f\"a garden filled with {x} flowers.\",\n",
    "    lambda x: f\"floral photography featuring a {x}.\",\n",
    "    lambda x: f\"an aesthetic photo of a {x}.\",\n",
    "    lambda x: f\"a {x} flower in full bloom.\",\n",
    "\n",
    "    # Descriptive\n",
    "    lambda x: f\"a large blooming {x}.\",\n",
    "    lambda x: f\"a freshly picked {x}.\",\n",
    "    lambda x: f\"a wilted {x} flower.\",\n",
    "    lambda x: f\"a {x} with dewdrops on its petals.\",\n",
    "    lambda x: f\"a delicate {x} on a green stem.\",\n",
    "    lambda x: f\"a colorful bouquet with {x}.\",\n",
    "\n",
    "    # Scientific-ish\n",
    "    lambda x: f\"a botanical illustration of {x}.\",\n",
    "    lambda x: f\"a herbarium specimen of {x}.\",\n",
    "    lambda x: f\"field photo of {x} species.\",\n",
    "    lambda x: f\"{x} photographed for a flora study.\",\n",
    "    lambda x: f\"a study sample of the {x} flower.\",\n",
    "    lambda x: f\"{x} genus flower in bloom.\",\n",
    "\n",
    "    # Casual / Internet Style\n",
    "    lambda x: f\"my favorite flower: the {x}.\",\n",
    "    lambda x: f\"saw a {x} today!\",\n",
    "    lambda x: f\"check out this {x} flower!\",\n",
    "    lambda x: f\"flowers like {x} are amazing.\",\n",
    "    lambda x: f\"the {x} is blooming this season.\",\n",
    "\n",
    "    # Photographic / Artistic\n",
    "    lambda x: f\"an artistic photo of a {x}.\",\n",
    "    lambda x: f\"film photo of a {x} flower.\",\n",
    "    lambda x: f\"a {x} in black and white.\",\n",
    "    lambda x: f\"the silhouette of a {x} in sunset light.\",\n",
    "    lambda x: f\"a {x} flower in a vintage vase.\",\n",
    "    lambda x: f\"an abstract painting of a {x}.\",\n",
    "    lambda x: f\"macro photography of a {x} blossom.\",\n",
    "\n",
    "    # Poetic or Metaphorical\n",
    "    lambda x: f\"a {x}, soft as a whisper.\",\n",
    "    lambda x: f\"a {x} dancing in the wind.\",\n",
    "    lambda x: f\"petals of the {x}, kissed by rain.\",\n",
    "    lambda x: f\"a lonely {x} on a quiet morning.\",\n",
    "    lambda x: f\"a {x} symbolizing peace and beauty.\",\n",
    "    lambda x: f\"like a {x} in springtime.\",\n",
    "\n",
    "    # Contextual / Scene-based\n",
    "    lambda x: f\"a {x} in a wildflower meadow.\",\n",
    "    lambda x: f\"a {x} flower on a wedding table.\",\n",
    "    lambda x: f\"{x} flowers in a forest clearing.\",\n",
    "    lambda x: f\"a {x} growing beside a stone path.\",\n",
    "    lambda x: f\"{x} blossoms in a city garden.\",\n",
    "    lambda x: f\"{x} petals scattered on the ground.\",\n",
    "\n",
    "    # class specific templates\n",
    "    # pink primrose\n",
    "    lambda x: f\"a photo of a {x}, a delicate flower with soft pink petals.\",\n",
    "    lambda x: f\"an image of a {x}, a blooming plant often found in springtime gardens.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its pale pink blossoms and gentle appearance.\",\n",
    "\n",
    "    # hard-leaved pocket orchid\n",
    "    lambda x: f\"a photo of a {x}, a tropical orchid with stiff, glossy leaves.\",\n",
    "    lambda x: f\"an image of a {x}, an exotic flower with waxy petals and leathery foliage.\",\n",
    "    lambda x: f\"a botanical image of a {x}, an orchid species with hard, durable leaves.\",\n",
    "\n",
    "    # canterbury bells\n",
    "    lambda x: f\"a photo of a {x}, a bell-shaped flower in shades of purple and blue.\",\n",
    "    lambda x: f\"an image of a {x}, known for its tall spikes of bell-like blossoms.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cottage garden flower with cup-shaped blooms.\",\n",
    "\n",
    "    # sweet pea\n",
    "    lambda x: f\"a photo of a {x}, a fragrant flower with delicate, ruffled petals.\",\n",
    "    lambda x: f\"an image of a {x}, often grown for its pastel colors and pleasant scent.\",\n",
    "    lambda x: f\"a close-up of a {x}, a climbing plant with butterfly-shaped flowers.\",\n",
    "\n",
    "    # english marigold\n",
    "    lambda x: f\"a photo of a {x}, a bright orange or yellow flower with daisy-like blooms.\",\n",
    "    lambda x: f\"an image of a {x}, known for its healing properties and sunny appearance.\",\n",
    "    lambda x: f\"a close-up of a {x}, a calendula flower common in herb gardens.\",\n",
    "\n",
    "    # tiger lily\n",
    "    lambda x: f\"a photo of a {x}, an orange flower with dark spots and recurved petals.\",\n",
    "    lambda x: f\"an image of a {x}, a wild-looking lily with dramatic coloring.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its bold, tiger-striped blooms.\",\n",
    "\n",
    "    # moon orchid\n",
    "    lambda x: f\"a photo of a {x}, an elegant white orchid with a moon-like glow.\",\n",
    "    lambda x: f\"an image of a {x}, a phalaenopsis flower often found in tropical climates.\",\n",
    "    lambda x: f\"a close-up of a {x}, a soft and symmetrical flower with wide petals.\",\n",
    "\n",
    "    # bird of paradise\n",
    "    lambda x: f\"a photo of a {x}, a tropical flower resembling a colorful bird.\",\n",
    "    lambda x: f\"an image of a {x}, known for its bright orange and blue petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, an exotic bloom that looks like a flying bird.\",\n",
    "\n",
    "    # monkshood\n",
    "    lambda x: f\"a photo of a {x}, a hooded purple flower with toxic properties.\",\n",
    "    lambda x: f\"an image of a {x}, often called wolfsbane, with dark violet petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a tall plant with helmet-shaped blooms.\",\n",
    "\n",
    "    # globe thistle\n",
    "    lambda x: f\"a photo of a {x}, a spherical flower with spiky blue petals.\",\n",
    "    lambda x: f\"an image of a {x}, known for its round shape and thistle-like texture.\",\n",
    "    lambda x: f\"a close-up of a {x}, a unique ornamental flower with a metallic hue.\",\n",
    "\n",
    "    # snapdragon\n",
    "    lambda x: f\"a photo of a {x}, a colorful flower that resembles a dragon's mouth.\",\n",
    "    lambda x: f\"an image of a {x}, known for its vertical clusters of blooming petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a common garden flower with hinged, snout-like blooms.\",\n",
    "\n",
    "    # colt's foot\n",
    "    lambda x: f\"a photo of a {x}, a yellow wildflower that appears before its leaves.\",\n",
    "    lambda x: f\"an image of a {x}, a small flower resembling a dandelion in early spring.\",\n",
    "    lambda x: f\"a close-up of a {x}, a plant with hoof-shaped leaves and bright yellow blooms.\",\n",
    "\n",
    "    # king protea\n",
    "    lambda x: f\"a photo of a {x}, a large flower with a spiky crown-like appearance.\",\n",
    "    lambda x: f\"an image of a {x}, a South African bloom with a central cone and pink petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a striking flower often used in bold arrangements.\",\n",
    "\n",
    "    # spear thistle\n",
    "    lambda x: f\"a photo of a {x}, a spiny plant with purple tufted blooms.\",\n",
    "    lambda x: f\"an image of a {x}, known for its sharp leaves and thistle head.\",\n",
    "    lambda x: f\"a close-up of a {x}, a wildflower with prickly stems and a vibrant purple flower.\",\n",
    "\n",
    "    # yellow iris\n",
    "    lambda x: f\"a photo of a {x}, a bright yellow iris with upright petals.\",\n",
    "    lambda x: f\"an image of a {x}, commonly found near water, with sword-shaped leaves.\",\n",
    "    lambda x: f\"a close-up of a {x}, an elegant flower with golden hues and frilled edges.\",\n",
    "\n",
    "    # globe-flower\n",
    "    lambda x: f\"a photo of a {x}, a round yellow bloom resembling a buttercup.\",\n",
    "    lambda x: f\"an image of a {x}, a spherical flower found in alpine meadows.\",\n",
    "    lambda x: f\"a close-up of a {x}, a glowing, globe-shaped flower with dense petals.\",\n",
    "\n",
    "    # purple coneflower\n",
    "    lambda x: f\"a photo of a {x}, a daisy-like flower with purple petals and a spiky cone.\",\n",
    "    lambda x: f\"an image of a {x}, often used in herbal remedies and garden borders.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its downward-sloping petals and orange center.\",\n",
    "\n",
    "    # peruvian lily\n",
    "    lambda x: f\"a photo of a {x}, a spotted flower with multiple colorful petals.\",\n",
    "    lambda x: f\"an image of a {x}, a long-lasting bloom used in cut flower arrangements.\",\n",
    "    lambda x: f\"a close-up of a {x}, a lily-like flower with striped inner petals.\",\n",
    "\n",
    "    # balloon flower\n",
    "    lambda x: f\"a photo of a {x}, a flower bud that inflates like a balloon before opening.\",\n",
    "    lambda x: f\"an image of a {x}, a star-shaped bloom in shades of blue or purple.\",\n",
    "    lambda x: f\"a close-up of a {x}, a unique flower with puffy unopened buds.\",\n",
    "\n",
    "    # giant white arum lily\n",
    "    lambda x: f\"a photo of a {x}, a large white flower with a trumpet-like shape.\",\n",
    "    lambda x: f\"an image of a {x}, also known as a calla lily, with a central yellow spadix.\",\n",
    "    lambda x: f\"a close-up of a {x}, an elegant flower with smooth white petals.\",\n",
    "\n",
    "    # fire lily\n",
    "    lambda x: f\"a photo of a {x}, a bright red or orange lily with curled petals.\",\n",
    "    lambda x: f\"an image of a {x}, a dramatic flower known for its flame-like appearance.\",\n",
    "    lambda x: f\"a close-up of a {x}, a fiery-looking lily with backward-bending petals.\",\n",
    "\n",
    "    # pincushion flower\n",
    "    lambda x: f\"a photo of a {x}, a flower with a domed center and delicate fringe.\",\n",
    "    lambda x: f\"an image of a {x}, often purple or lavender, resembling a pin-filled cushion.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its central disk and lace-like petals.\",\n",
    "\n",
    "    # fritillary\n",
    "    lambda x: f\"a photo of a {x}, a checkered bell-shaped flower often in purple tones.\",\n",
    "    lambda x: f\"an image of a {x}, a rare flower with a distinctive petal pattern.\",\n",
    "    lambda x: f\"a close-up of a {x}, a delicate wildflower with hanging, nodding blooms.\",\n",
    "\n",
    "    # red ginger\n",
    "    lambda x: f\"a photo of a {x}, a tropical flower with bright red bracts.\",\n",
    "    lambda x: f\"an image of a {x}, known for its bold color and upright flower spikes.\",\n",
    "    lambda x: f\"a close-up of a {x}, a striking plant native to rainforests.\",\n",
    "\n",
    "    # grape hyacinth\n",
    "    lambda x: f\"a photo of a {x}, a small bulbous plant with clusters of blue-purple flowers.\",\n",
    "    lambda x: f\"an image of a {x}, resembling tiny grapes arranged on a spike.\",\n",
    "    lambda x: f\"a close-up of a {x}, a spring flower with densely packed florets.\",\n",
    "\n",
    "    # corn poppy\n",
    "    lambda x: f\"a photo of a {x}, a bright red flower with papery petals and a dark center.\",\n",
    "    lambda x: f\"an image of a {x}, a wild poppy often found in meadows and fields.\",\n",
    "    lambda x: f\"a close-up of a {x}, a flower symbolizing remembrance and resilience.\",\n",
    "\n",
    "    # prince of wales feathers\n",
    "    lambda x: f\"a photo of a {x}, a spiky flower head resembling a feathery plume.\",\n",
    "    lambda x: f\"an image of a {x}, known for its upright purple-pink floral spikes.\",\n",
    "    lambda x: f\"a close-up of a {x}, a member of the amaranth family with plume-like blooms.\",\n",
    "\n",
    "    # stemless gentian\n",
    "    lambda x: f\"a photo of a {x}, a vivid blue flower growing close to the ground.\",\n",
    "    lambda x: f\"an image of a {x}, a low-growing gentian with trumpet-shaped petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a mountain flower with intensely blue blossoms.\",\n",
    "\n",
    "    # artichoke\n",
    "    lambda x: f\"a photo of a {x}, a thistle-like plant with edible buds and purple flowers.\",\n",
    "    lambda x: f\"an image of a {x}, a spiky flower head that blooms into a vibrant violet.\",\n",
    "    lambda x: f\"a close-up of a {x}, a large budding flower with a layered appearance.\",\n",
    "\n",
    "    # sweet william\n",
    "    lambda x: f\"a photo of a {x}, a cluster of small flowers in pink, red, or white.\",\n",
    "    lambda x: f\"an image of a {x}, a garden flower known for its fringed petal edges.\",\n",
    "    lambda x: f\"a close-up of a {x}, a fragrant bloom often used in cottage gardens.\",\n",
    "\n",
    "    # carnation\n",
    "    lambda x: f\"a photo of a {x}, a ruffled flower commonly seen in bouquets.\",\n",
    "    lambda x: f\"an image of a {x}, a traditional bloom with a clove-like scent.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its layered petals and vibrant color range.\",\n",
    "\n",
    "    # garden phlox\n",
    "    lambda x: f\"a photo of a {x}, a tall plant with clusters of pink or purple blooms.\",\n",
    "    lambda x: f\"an image of a {x}, a perennial flower found in cottage-style gardens.\",\n",
    "    lambda x: f\"a close-up of a {x}, a phlox with star-shaped petals growing in dense bunches.\",\n",
    "\n",
    "    # love in the mist\n",
    "    lambda x: f\"a photo of a {x}, a blue flower surrounded by fine, feathery foliage.\",\n",
    "    lambda x: f\"an image of a {x}, a delicate flower with a misty, lace-like background.\",\n",
    "    lambda x: f\"a close-up of a {x}, a whimsical bloom with soft, threadlike leaves.\",\n",
    "\n",
    "    # mexican aster\n",
    "    lambda x: f\"a photo of a {x}, a daisy-like flower with bright pink or purple petals.\",\n",
    "    lambda x: f\"an image of a {x}, a tall annual bloom often seen in wildflower fields.\",\n",
    "    lambda x: f\"a close-up of a {x}, a lightweight flower with yellow centers and soft petals.\",\n",
    "\n",
    "    # alpine sea holly\n",
    "    lambda x: f\"a photo of a {x}, a spiky blue flower with thistle-like bracts.\",\n",
    "    lambda x: f\"an image of a {x}, a unique alpine plant with metallic-colored petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a flower with a cone center and pointed star-shaped sepals.\",\n",
    "\n",
    "    # ruby-lipped cattleya\n",
    "    lambda x: f\"a photo of a {x}, an orchid with bold purple lips and pastel petals.\",\n",
    "    lambda x: f\"an image of a {x}, a showy flower with ruby-colored accents on its lip.\",\n",
    "    lambda x: f\"a close-up of a {x}, a fragrant orchid with elaborate ruffled petals.\",\n",
    "\n",
    "    # cape flower\n",
    "    lambda x: f\"a photo of a {x}, a brightly colored South African flower with daisy form.\",\n",
    "    lambda x: f\"an image of a {x}, known for its vivid hues and sun-tracking behavior.\",\n",
    "    lambda x: f\"a close-up of a {x}, a vibrant flower with a dark center and radiating petals.\",\n",
    "\n",
    "    # great masterwort\n",
    "    lambda x: f\"a photo of a {x}, a flower with a central cluster surrounded by papery bracts.\",\n",
    "    lambda x: f\"an image of a {x}, a perennial bloom with intricate star-like heads.\",\n",
    "    lambda x: f\"a close-up of a {x}, a soft-colored flower with detailed florets in the center.\",\n",
    "\n",
    "    # siam tulip\n",
    "    lambda x: f\"a photo of a {x}, a tropical flower with pink petals and green bracts.\",\n",
    "    lambda x: f\"an image of a {x}, also known as Curcuma, with cone-shaped blossoms.\",\n",
    "    lambda x: f\"a close-up of a {x}, a vibrant Thai flower with tulip-like structure.\",\n",
    "\n",
    "    # lenten rose\n",
    "    lambda x: f\"a photo of a {x}, a spring flower with nodding blooms and muted colors.\",\n",
    "    lambda x: f\"an image of a {x}, a hellebore plant with leathery leaves and soft petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cold-hardy flower blooming in late winter to early spring.\",\n",
    "\n",
    "    # barbeton daisy\n",
    "    lambda x: f\"a photo of a {x}, a brightly colored daisy native to South Africa.\",\n",
    "    lambda x: f\"an image of a {x}, known for its vivid red, orange, or pink petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cheerful flower with a prominent central disk.\",\n",
    "\n",
    "    # daffodil\n",
    "    lambda x: f\"a photo of a {x}, a trumpet-shaped flower with yellow or white petals.\",\n",
    "    lambda x: f\"an image of a {x}, one of the first blooms of spring with a central corona.\",\n",
    "    lambda x: f\"a close-up of a {x}, a classic bulb flower symbolizing renewal and hope.\",\n",
    "\n",
    "    # sword lily\n",
    "    lambda x: f\"a photo of a {x}, a tall flower with sword-like leaves and vertical blooms.\",\n",
    "    lambda x: f\"an image of a {x}, commonly known as gladiolus with stacked florets.\",\n",
    "    lambda x: f\"a close-up of a {x}, a showy flower in a rainbow of colors on long spikes.\",\n",
    "\n",
    "    # poinsettia\n",
    "    lambda x: f\"a photo of a {x}, a festive plant with red or white leaf-like bracts.\",\n",
    "    lambda x: f\"an image of a {x}, often used in winter displays with green foliage and colorful tops.\",\n",
    "    lambda x: f\"a close-up of a {x}, a holiday flower with bright petal-like leaves.\",\n",
    "\n",
    "    # bolero deep blue\n",
    "    lambda x: f\"a photo of a {x}, a compact flower with deep violet petals and ruffled texture.\",\n",
    "    lambda x: f\"an image of a {x}, a variety of pansy known for its rich blue coloring.\",\n",
    "    lambda x: f\"a close-up of a {x}, a velvety flower with intricate patterns and deep hues.\",\n",
    "\n",
    "    # wallflower\n",
    "    lambda x: f\"a photo of a {x}, a small clustered flower known for growing on walls or rocky soil.\",\n",
    "    lambda x: f\"an image of a {x}, a plant with fragrant blooms in yellow, orange, or red.\",\n",
    "    lambda x: f\"a close-up of a {x}, a simple flower with four-petal blossoms and warm colors.\",\n",
    "\n",
    "    # marigold\n",
    "    lambda x: f\"a photo of a {x}, a vibrant flower with layers of orange or yellow petals.\",\n",
    "    lambda x: f\"an image of a {x}, known for its strong scent and decorative garden use.\",\n",
    "    lambda x: f\"a close-up of a {x}, a sun-loving flower with round, bushy blooms.\",\n",
    "\n",
    "    # buttercup\n",
    "    lambda x: f\"a photo of a {x}, a shiny yellow flower with cup-shaped petals.\",\n",
    "    lambda x: f\"an image of a {x}, a wildflower with a simple structure and glossy surface.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cheerful bloom with golden overlapping petals.\",\n",
    "\n",
    "    # oxeye daisy\n",
    "    lambda x: f\"a photo of a {x}, a white-petaled daisy with a yellow central disk.\",\n",
    "    lambda x: f\"an image of a {x}, a classic meadow flower with a tall, slender stem.\",\n",
    "    lambda x: f\"a close-up of a {x}, a widespread wildflower resembling a common daisy.\",\n",
    "\n",
    "    # common dandelion\n",
    "    lambda x: f\"a photo of a {x}, a yellow flower with toothed leaves and fluffy seed heads.\",\n",
    "    lambda x: f\"an image of a {x}, a weedy plant known for its puffball seed dispersal.\",\n",
    "    lambda x: f\"a close-up of a {x}, a golden flower head made of many tiny florets.\",\n",
    "\n",
    "    # petunia\n",
    "    lambda x: f\"a photo of a {x}, a funnel-shaped flower often used in hanging baskets.\",\n",
    "    lambda x: f\"an image of a {x}, a colorful bloom available in many vibrant shades.\",\n",
    "    lambda x: f\"a close-up of a {x}, a soft, velvety flower with a wide trumpet-like shape.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TVrYUYTv9ttM"
   },
   "outputs": [],
   "source": [
    "# get the three datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# split classes into base and novel\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "# split the three datasets\n",
    "train_base, _ = split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def load_text_features(categories: list[CategoryLabel]):\n",
    "    \"\"\"\n",
    "    size: num_prompts x num_categories x input_size\"]\n",
    "    \"\"\"\n",
    "    text_inputs = [\n",
    "        clip.tokenize(\n",
    "            [template(c['name']) for c in categories]\n",
    "        ).to(device)\n",
    "        for template in prompt_template\n",
    "    ]\n",
    "\n",
    "    text_features_array: list[torch.Tensor] = [\n",
    "        model.encode_text(x)\n",
    "        for x in text_inputs\n",
    "    ]\n",
    "\n",
    "    # shape: num_prompts x num_classes x embedding_size\n",
    "    text_features = torch.stack([\n",
    "        x/x.norm(dim=-1,keepdim=True)\n",
    "        for x in text_features_array\n",
    "    ])\n",
    "\n",
    "    # shape: num_prompts x embedding_size x num_classes\n",
    "    text_features = text_features.permute(0,2,1)\n",
    "\n",
    "    return text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_EMBEDDING_SIZE = int(model.encode_text(clip.tokenize(\"foo\").to(device)).shape[-1])\n",
    "NUMBER_OF_PROMPTS = len(prompt_template)\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class ModelWeightingModel(nn.Module):\n",
    "    def __init__(self, classes_for_training: list[CategoryLabel]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_classes = [x for x in classes_for_training if not x[\"novel\"]]\n",
    "        self.update_indexing_mask(classes_for_training)\n",
    "        self.num_base_classes = len(self.base_classes)\n",
    "        self.l1 = nn.Linear(CLIP_EMBEDDING_SIZE, 200, dtype=model.dtype)\n",
    "        # one output weight for each prompt, and for each class, plus one for novel classes\n",
    "        self.l2 = nn.Linear(200, NUMBER_OF_PROMPTS * (self.num_base_classes + 1), dtype=model.dtype)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        \"\"\"\n",
    "        input: [batch_size x clip_embedding_size] = the image-generated embeddings\n",
    "        \"\"\"\n",
    "        x: torch.Tensor = self.l1(input)\n",
    "        x = self.l2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        batch_size = input.shape[0]\n",
    "        x = x.reshape(batch_size, NUMBER_OF_PROMPTS, self.num_base_classes+1)\n",
    "        x = x[:,:,self.indexing_mask]\n",
    "        return x\n",
    "\n",
    "    def update_indexing_mask(self, classes: list[CategoryLabel]):\n",
    "\n",
    "        id_to_index = {x[\"id\"]: i for i,x in enumerate(self.base_classes)}\n",
    "\n",
    "        def get_index(label: CategoryLabel):\n",
    "            # novel categories are always mapped to the generic re-weighter (the last one)\n",
    "            # as we haven't learned a model for them\n",
    "            if label[\"novel\"]:\n",
    "                return len(self.base_classes)\n",
    "            # base classes instead have their specialized re-weighter at the corresponding index\n",
    "            return id_to_index[label[\"id\"]]\n",
    "\n",
    "        # the indexing mask will be used to index into the output of the layer\n",
    "        # and build a tensor that has the same size as the number of categories\n",
    "        self.indexing_mask = torch.Tensor([\n",
    "            get_index(x) for x in classes\n",
    "        ]).type(torch.long)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generic\n",
    "import random\n",
    "\n",
    "\n",
    "base_classes_label: list[CategoryLabel] = [\n",
    "    {\n",
    "        \"id\": x,\n",
    "        \"name\": CLASS_NAMES[x],\n",
    "        \"novel\": False\n",
    "    }\n",
    "    for x in base_classes\n",
    "]\n",
    "\n",
    "novel_classes_label: list[CategoryLabel] = [\n",
    "    {\n",
    "        \"id\": x,\n",
    "        \"name\": CLASS_NAMES[x],\n",
    "        \"novel\": True\n",
    "    }\n",
    "    for x in novel_classes\n",
    "]\n",
    "\n",
    "all_classes_labels = base_classes_label + novel_classes_label\n",
    "\n",
    "base_and_virtual_novel_classes_labels = base_classes_label + [\n",
    "    {\n",
    "        \"id\": x + len(all_classes_labels),\n",
    "        \"name\": CLASS_NAMES[x],\n",
    "        \"novel\": False\n",
    "    }\n",
    "    for x in base_classes\n",
    "]\n",
    "\n",
    "train_base_and_virtual_novel = train_base\n",
    "T = TypeVar(\"T\")\n",
    "E = TypeVar(\"E\")\n",
    "class VirtualNovelDataset(Dataset, Generic[T,E]):\n",
    "    def __init__(self, dataset: Dataset[tuple[T,E]], num_novel_classes: int, to_add: E):\n",
    "        self._dataset_len: int = len(dataset) #type: ignore\n",
    "        self._dataset = dataset\n",
    "        self._virtual_novel: list[tuple[T,E]] = []\n",
    "        indexes = random.sample(range(self._dataset_len), num_novel_classes)\n",
    "        for index in indexes:\n",
    "            data, label = dataset[index]\n",
    "            new_label: E = label + to_add #type: ignore\n",
    "            self._virtual_novel.append((data, new_label))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._dataset_len + len(self._virtual_novel)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[T,E]:\n",
    "        # non virtual sample\n",
    "        if index < self._dataset_len:\n",
    "            return self._dataset[index]\n",
    "        # virtual sample:\n",
    "        else:\n",
    "            return self._virtual_novel[index - self._dataset_len]\n",
    "            \n",
    "train_base_and_virtual_novel = VirtualNovelDataset(train_base, 200, len(all_classes_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([214, 512, 102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [02:34<00:00,  3.85s/it, loss=0.1262]\n"
     ]
    }
   ],
   "source": [
    "import clip.model\n",
    "from typing import cast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(\n",
    "        clip: clip.model.CLIP,\n",
    "        weighter: ModelWeightingModel ,\n",
    "        dataset: Dataset[tuple[torch.Tensor, int]],\n",
    "        categories: list[CategoryLabel],\n",
    "        batch_size: int,\n",
    "        num_steps: int,\n",
    "        device: torch.device | str,\n",
    "        batch_size_multiplier: int = 1\n",
    "    ):\n",
    "    clip.eval()\n",
    "    weighter.train()\n",
    "\n",
    "\n",
    "    # optimizer = torch.optim.AdamW(params = weighter.parameters())\n",
    "    optimizer = torch.optim.SGD(params = weighter.parameters(), lr=5)\n",
    "\n",
    "\n",
    "    contig_cat2idx = {cat[\"id\"]: idx for idx, cat in enumerate(categories)}\n",
    "    # size: num_prompts x clip_embedding_size x num_categories\"]\n",
    "    text_features = load_text_features(categories)\n",
    "    print(text_features.shape)\n",
    "    # size: num_category, tell me what class every text feature is referring to\n",
    "\n",
    "\n",
    "    num_prompts = text_features.shape[0]\n",
    "    num_classes = text_features.shape[2]\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    progress_bar = tqdm(range(num_steps), \"Steps\") \n",
    "    for _ in progress_bar:\n",
    "        image: torch.Tensor\n",
    "        target: torch.Tensor\n",
    "        losses: list[float] = []\n",
    "        for i, (image, target) in enumerate(dataloader):\n",
    "\n",
    "\n",
    "            # shape: [batch_size]\n",
    "            target = torch.Tensor([contig_cat2idx[cast(int,t.item())] for t in target]).long()\n",
    "\n",
    "\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                image_features: torch.Tensor = clip.encode_image(image)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "            # shape: [batch_size x num_prompts x clip_embedding_size x num_classes]\n",
    "            masked_text_features = text_features \\\n",
    "                .clone() \\\n",
    "                .unsqueeze(0) \\\n",
    "                .expand(\n",
    "                    len(target), # batch size\n",
    "                    num_prompts,\n",
    "                    CLIP_EMBEDDING_SIZE,\n",
    "                    num_classes\n",
    "                ).clone()\n",
    "\n",
    "            #shape: [batch_size]\n",
    "            class_to_zero = (target + len(categories) / 2) % len(categories)\n",
    "            class_to_zero = class_to_zero.long()\n",
    "            #shape: [batch_size x num_classes]\n",
    "            class_to_zero = torch.nn.functional.one_hot(class_to_zero, num_classes).bool()\n",
    "\n",
    "            # # shape: [batch_size x num_classes x num_prompts x clip_embedding_size]\n",
    "            masked_text_features_eff = masked_text_features.permute(0,3,2,1)\n",
    "            masked_text_features_eff[class_to_zero,:,:] = 0\n",
    "            # shape: [batch_size x num_prompts x clip_embedding_size x num_classes]\n",
    "            masked_text_features_eff = masked_text_features_eff.permute(0,3,2,1)\n",
    "\n",
    "            # Alternative option\n",
    "            # masked_text_features[target >= 51,:,:,:51] = 0\n",
    "            # masked_text_features[target < 51,:,:,51:] = 0\n",
    "\n",
    "            # b = batch, e = embedding, p = prompts, c = classes\n",
    "            # shape: [batch_size x num_prompts x num_classes]\n",
    "            scores = torch.einsum(\"be,bpec -> bpc\", image_features, masked_text_features.detach())\n",
    "\n",
    "            # shape: [batch_size x num_prompts x num_classes]\n",
    "            weights: torch.Tensor = weighter(image_features.detach())\n",
    "\n",
    "            # reweighing scores\n",
    "            # scores = (weights * scores.permute(2,0,1)).permute(1,2,0)\n",
    "            scores *= weights\n",
    "\n",
    "            # shape: [batch_size x num_classes]\n",
    "            out = torch.sum(scores, dim=1)\n",
    "\n",
    "            loss: torch.Tensor = loss_fn(out, target)\n",
    "            losses.append(loss.item())\n",
    "            loss /= batch_size_multiplier\n",
    "            loss.backward()\n",
    "            if (i+1) % batch_size_multiplier == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        final_loss = np.average(losses)\n",
    "        progress_bar.set_postfix(loss=f\"{final_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "weighter = ModelWeightingModel(base_and_virtual_novel_classes_labels).to(device)\n",
    "\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    weighter,\n",
    "    train_base_and_virtual_novel,\n",
    "    base_and_virtual_novel_classes_labels,\n",
    "    32,\n",
    "    40,\n",
    "    device,\n",
    "    4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcgMwr3J9VIg"
   },
   "source": [
    "## Compute Zero-Shot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49199,
     "status": "ok",
     "timestamp": 1743597826648,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "7uhblkvm9US4",
    "outputId": "a8b36190-e0c5-401b-830b-9a48711d934f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:12<00:00,  6.10it/s]\n",
      "ðŸ§  Zero-shot evaluation on Novel Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115/115 [00:15<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Base classes accuracy: 94.38%\n",
      "ðŸ” Novel classes accuracy: 78.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # we don't want gradients\n",
    "def eval(\n",
    "        clip: clip.model.CLIP,\n",
    "        weighter: ModelWeightingModel ,\n",
    "        dataset: Dataset[tuple[torch.Tensor, int]],\n",
    "        categories: list[CategoryLabel],\n",
    "        batch_size: int,\n",
    "        device: torch.device | str,\n",
    "        label = \"\"\n",
    "    ):\n",
    "    # let's set the model in evaluation mode\n",
    "    clip.eval()\n",
    "    weighter.eval()\n",
    "\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {cat[\"id\"]: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "    text_features = load_text_features(categories)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        image_features = clip.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # shape: [batch_size x num_prompts x num_classes]\n",
    "        scores: torch.Tensor = torch.matmul(image_features, text_features).permute(1,0,2).detach()\n",
    "\n",
    "        # shape: [ batch_size x (num_prompts x num_classes)]\n",
    "        weights: torch.Tensor = weighter(image_features.detach())\n",
    "\n",
    "        # reweighing scores\n",
    "        # scores = (weights * scores.permute(2,0,1)).permute(1,2,0)\n",
    "        scores *= weights\n",
    "\n",
    "        out = torch.sum(scores, dim=1)\n",
    "        predicted_class = out.argmax(dim=-1)\n",
    "        \n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset) #type: ignore\n",
    "    return accuracy\n",
    "\n",
    "# weighter.update_indexing_mask(base_classes_label)\n",
    "# base_accuracy = eval(model, weighter, dataset=test_base, categories=base_classes_label, batch_size=32, device=device, label=\"ðŸ§  Zero-shot evaluation on Base Classes\")\n",
    "\n",
    "# weighter.update_indexing_mask(novel_classes_label)\n",
    "# novel_accuracy = eval(model, weighter, dataset=test_novel, categories=novel_classes_label, batch_size=32, device=device, label=\"ðŸ§  Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "weighter.update_indexing_mask(all_classes_labels)\n",
    "total_accuracy = eval(model, weighter, dataset=test_set, categories=all_classes_labels, batch_size=32, device=device, label=\"ðŸ§  Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "# print(f\"ðŸ” Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "# print(f\"ðŸ” Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
    "print(f\"ðŸ” Novel classes accuracy: {total_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baYfLKNdfbUR"
   },
   "source": [
    "## Harmonic Mean\n",
    "Few-Shot Adaptations papers usually report the Harmonic Mean.\n",
    "The harmonic mean tends to mitigate the impact of large outliers (base accuracy) and aggravate the impact of small ones (novel accuracy).\n",
    "Thus, achieving very high base accuracies at the expense of the novel accuracy will be penalized by the HM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1743597665969,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "rKAXR7hlfbUR",
    "outputId": "e00e50f4-3b0f-4e79-ed09-e8e82cc53668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Harmonic Mean: 85.50%\n"
     ]
    }
   ],
   "source": [
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    hm = numerator / denominator\n",
    "    return hm\n",
    "\n",
    "print(f\"ðŸ” Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
