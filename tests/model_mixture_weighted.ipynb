{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYQiprE0pcVD"
   },
   "source": [
    "# CLIP Weighted Model Mixture - Few shot learning technique\n",
    "This notebook implement a novel strategy for clip few shot adaptation.\n",
    "\n",
    "Our strategy involve the creation of multiple \"models\" where the output is then weighted\n",
    "to then create an \"average guess\" that should be, on generally more accurate than any individual model.\n",
    "\n",
    "So far this is a pretty common technique, however there are two key differences that make this approach novel:\n",
    "1. Each model is not a learned model, but is essentially just a prompt. That is then passed through the clip text encoder.\n",
    "The prompts are NOT learned, instead they are manually crafted base on domain knowledge.\n",
    "2. The \"averaging\" is not a simple average, but is a weighted average, and the weights are provided by a \"Re-Weighting Model\"\n",
    "This model is a simple neural network that take as input the clip image embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QtqdSOr8qqOn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai_clip in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: ftfy in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (6.3.1)\n",
      "Requirement already satisfied: regex in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from ftfy->openai_clip) (0.2.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->openai_clip) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai_clip\n",
    "\n",
    "from torchvision.datasets import Flowers102\n",
    "import random\n",
    "import torch\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from typing import TypeVar, cast, TypedDict, Generic\n",
    "import random\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from typing import Callable\n",
    "from torch import nn\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import clip.model\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "This section include a few configurable parameters of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to use the default train-test spit provided by torchvision or not\n",
    "# > *If set to True*: we will have the default split where each training class has 10 samples\n",
    "# > *if set to False* the number of shot will be defined by the `NUMBER_OF_SHOTS` constants\n",
    "#   this is done to make it easier to compare our algorithm with other papers, that often uses 16 as the default number of shots\n",
    "USE_DEFAULT_SPLIT = False\n",
    "# The number of shot to use in learning. Only has effect if `USE_DEFAULT_SPLIT` is set to False\n",
    "NUMBER_OF_SHOTS = 16\n",
    "\n",
    "# the device to use for training\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# underlying model to use for the clip visual encoder\n",
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "CLIP_MODEL=\"ViT-B/16\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2353MHw1p24h"
   },
   "source": [
    "### Type definitions\n",
    "A few type definition that will be used throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryLabel(TypedDict):\n",
    "    id: int\n",
    "    name: str\n",
    "    novel: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading\n",
    "\n",
    "Nothing special here, just the definition of the functions to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M_1CrUhZpVCq"
   },
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"\n",
    "    Load Flowers102 train, validation and test sets.\n",
    "    Uses the default split provided by torchvision.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return (\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], train),\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], val),\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], test)\n",
    "    )\n",
    "\n",
    "def get_data_custom_split(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"\n",
    "    Load Flowers102 train, validation and test sets.\n",
    "    Uses a custom split that allow to specify the number of items per class that will be assigned to the train set.\n",
    "    The validation set will always be empty.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    a,b,c = get_data(data_dir, transform)\n",
    "    full_dataset: Dataset[tuple[torch.Tensor, int]] = torch.utils.data.ConcatDataset([a,b,c])\n",
    "\n",
    "    labels_set = set(l for _,l in full_dataset)\n",
    "    class_to_index_dict: dict[int, list[int]] = {l: [] for l in labels_set}\n",
    "\n",
    "    for i in range(len(full_dataset)):\n",
    "        l = full_dataset[i][1]\n",
    "        class_to_index_dict[l].append(i)\n",
    "\n",
    "    train: list[int] = []\n",
    "    test: list[int] = []\n",
    "\n",
    "    for indexes in class_to_index_dict.values():\n",
    "        random.shuffle(indexes)\n",
    "        train += indexes[0:NUMBER_OF_SHOTS]\n",
    "        test += indexes[NUMBER_OF_SHOTS:]\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train)\n",
    "    validation_dataset = torch.utils.data.Subset(full_dataset, [])\n",
    "    test_dataset = torch.utils.data.Subset(full_dataset, test)\n",
    "    return (\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], train_dataset),\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], validation_dataset),\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], test_dataset)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJI_a5EizA5a"
   },
   "source": [
    "## Base and Novel categories\n",
    "Function definition to split the dataset into novel and base classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nfq51vd8q_5a"
   },
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "E = TypeVar(\"E\")\n",
    "def base_novel_categories(dataset: Dataset[tuple[T,E]]):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(l for _, l in dataset)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8puO1VNpzwvi"
   },
   "source": [
    "## Split Dataset\n",
    "The next step is to actually split the dataset into the base and novel categories we extract from `base_novel_categories`.\n",
    "To split the data we need the dataset (obviously) and the list of base classes. If the sample label is not part of the base categories, then it must be part of the novel ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "msOszMs2zRRu"
   },
   "outputs": [],
   "source": [
    "\n",
    "T = TypeVar(\"T\")\n",
    "E = TypeVar(\"E\")\n",
    "def split_data(dataset: Dataset[tuple[T,E]], base_classes: list[int]):\n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples: list[int] = []\n",
    "    novel_categories_samples: list[int] = []\n",
    "\n",
    "    # set with the base classes (so that checking existence is O(1))\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    label: int\n",
    "    for sample_id, (_, label) in enumerate(dataset): #type: ignore\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    base_dataset = Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KpbPRLr7WL_"
   },
   "source": [
    "## Load CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5162,
     "status": "ok",
     "timestamp": 1743597617669,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "Sh6uLZRT7YJx",
    "outputId": "25ef91c9-9879-4f50-d1eb-d2c53c194498"
   },
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(CLIP_MODEL, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the prompt templates\n",
    "\n",
    "This section define a list of \"prompt templates\". A template is just a function that take as input the name of a class, and return\n",
    "a string describing the underlying object.\n",
    "\n",
    "The templates defined in this section can be divided into two broad categories:\n",
    "1. **General prompts**: those prompts are generic and could be used for potentially any class.\n",
    "2. **Class specific prompts**: those prompts are specific for one category.\n",
    "\n",
    "Here you should note that there is nothing that maps the class specific prompts to the associated class,\n",
    "infact we don't worry at all about that, and we let the re-weighter take care of that.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####################################################\n",
    "################# GENERAL PROMPTS ###################\n",
    "#####################################################\n",
    "general_prompt_template: list[Callable[[str], str]] = [\n",
    "    lambda x: f\"a photo of a {x}, a type of flower.\",\n",
    "    lambda x: f\"a photo of some {x}, a type of flower.\",\n",
    "    lambda x: f\"a close-up of a {x} flower.\",\n",
    "    lambda x: f\"an image of a {x} blossom.\",\n",
    "    lambda x: f\"a beautiful {x} in bloom.\",\n",
    "    lambda x: f\"a bunch of {x} flowers.\",\n",
    "    lambda x: f\"a macro shot of a {x} flower.\",\n",
    "    lambda x: f\"a single {x} flower.\",\n",
    "    lambda x: f\"fresh {x} flowers in a garden.\",\n",
    "]\n",
    "\n",
    "#####################################################\n",
    "############## CLASS SPECIFIC PROMPTS ###############\n",
    "#####################################################\n",
    "\n",
    "class_specific_prompt_templates: list[Callable[[str], str]] = [\n",
    "    # pink primrose\n",
    "    lambda x: f\"a photo of a {x}, a delicate flower with soft pink petals.\",\n",
    "    lambda x: f\"an image of a {x}, a blooming plant often found in springtime gardens.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its pale pink blossoms and gentle appearance.\",\n",
    "\n",
    "    # hard-leaved pocket orchid\n",
    "    lambda x: f\"a photo of a {x}, a tropical orchid with stiff, glossy leaves.\",\n",
    "    lambda x: f\"an image of a {x}, an exotic flower with waxy petals and leathery foliage.\",\n",
    "    lambda x: f\"a botanical image of a {x}, an orchid species with hard, durable leaves.\",\n",
    "\n",
    "    # canterbury bells\n",
    "    lambda x: f\"a photo of a {x}, a bell-shaped flower in shades of purple and blue.\",\n",
    "    lambda x: f\"an image of a {x}, known for its tall spikes of bell-like blossoms.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cottage garden flower with cup-shaped blooms.\",\n",
    "\n",
    "    # sweet pea\n",
    "    lambda x: f\"a photo of a {x}, a fragrant flower with delicate, ruffled petals.\",\n",
    "    lambda x: f\"an image of a {x}, often grown for its pastel colors and pleasant scent.\",\n",
    "    lambda x: f\"a close-up of a {x}, a climbing plant with butterfly-shaped flowers.\",\n",
    "\n",
    "    # english marigold\n",
    "    lambda x: f\"a photo of a {x}, a bright orange or yellow flower with daisy-like blooms.\",\n",
    "    lambda x: f\"an image of a {x}, known for its healing properties and sunny appearance.\",\n",
    "    lambda x: f\"a close-up of a {x}, a calendula flower common in herb gardens.\",\n",
    "\n",
    "    # tiger lily\n",
    "    lambda x: f\"a photo of a {x}, an orange flower with dark spots and recurved petals.\",\n",
    "    lambda x: f\"an image of a {x}, a wild-looking lily with dramatic coloring.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its bold, tiger-striped blooms.\",\n",
    "\n",
    "    # moon orchid\n",
    "    lambda x: f\"a photo of a {x}, an elegant white orchid with a moon-like glow.\",\n",
    "    lambda x: f\"an image of a {x}, a phalaenopsis flower often found in tropical climates.\",\n",
    "    lambda x: f\"a close-up of a {x}, a soft and symmetrical flower with wide petals.\",\n",
    "\n",
    "    # bird of paradise\n",
    "    lambda x: f\"a photo of a {x}, a tropical flower resembling a colorful bird.\",\n",
    "    lambda x: f\"an image of a {x}, known for its bright orange and blue petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, an exotic bloom that looks like a flying bird.\",\n",
    "\n",
    "    # monkshood\n",
    "    lambda x: f\"a photo of a {x}, a hooded purple flower with toxic properties.\",\n",
    "    lambda x: f\"an image of a {x}, often called wolfsbane, with dark violet petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a tall plant with helmet-shaped blooms.\",\n",
    "\n",
    "    # globe thistle\n",
    "    lambda x: f\"a photo of a {x}, a spherical flower with spiky blue petals.\",\n",
    "    lambda x: f\"an image of a {x}, known for its round shape and thistle-like texture.\",\n",
    "    lambda x: f\"a close-up of a {x}, a unique ornamental flower with a metallic hue.\",\n",
    "\n",
    "    # snapdragon\n",
    "    lambda x: f\"a photo of a {x}, a colorful flower that resembles a dragon's mouth.\",\n",
    "    lambda x: f\"an image of a {x}, known for its vertical clusters of blooming petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a common garden flower with hinged, snout-like blooms.\",\n",
    "\n",
    "    # colt's foot\n",
    "    lambda x: f\"a photo of a {x}, a yellow wildflower that appears before its leaves.\",\n",
    "    lambda x: f\"an image of a {x}, a small flower resembling a dandelion in early spring.\",\n",
    "    lambda x: f\"a close-up of a {x}, a plant with hoof-shaped leaves and bright yellow blooms.\",\n",
    "\n",
    "    # king protea\n",
    "    lambda x: f\"a photo of a {x}, a large flower with a spiky crown-like appearance.\",\n",
    "    lambda x: f\"an image of a {x}, a South African bloom with a central cone and pink petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a striking flower often used in bold arrangements.\",\n",
    "\n",
    "    # spear thistle\n",
    "    lambda x: f\"a photo of a {x}, a spiny plant with purple tufted blooms.\",\n",
    "    lambda x: f\"an image of a {x}, known for its sharp leaves and thistle head.\",\n",
    "    lambda x: f\"a close-up of a {x}, a wildflower with prickly stems and a vibrant purple flower.\",\n",
    "\n",
    "    # yellow iris\n",
    "    lambda x: f\"a photo of a {x}, a bright yellow iris with upright petals.\",\n",
    "    lambda x: f\"an image of a {x}, commonly found near water, with sword-shaped leaves.\",\n",
    "    lambda x: f\"a close-up of a {x}, an elegant flower with golden hues and frilled edges.\",\n",
    "\n",
    "    # globe-flower\n",
    "    lambda x: f\"a photo of a {x}, a round yellow bloom resembling a buttercup.\",\n",
    "    lambda x: f\"an image of a {x}, a spherical flower found in alpine meadows.\",\n",
    "    lambda x: f\"a close-up of a {x}, a glowing, globe-shaped flower with dense petals.\",\n",
    "\n",
    "    # purple coneflower\n",
    "    lambda x: f\"a photo of a {x}, a daisy-like flower with purple petals and a spiky cone.\",\n",
    "    lambda x: f\"an image of a {x}, often used in herbal remedies and garden borders.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its downward-sloping petals and orange center.\",\n",
    "\n",
    "    # peruvian lily\n",
    "    lambda x: f\"a photo of a {x}, a spotted flower with multiple colorful petals.\",\n",
    "    lambda x: f\"an image of a {x}, a long-lasting bloom used in cut flower arrangements.\",\n",
    "    lambda x: f\"a close-up of a {x}, a lily-like flower with striped inner petals.\",\n",
    "\n",
    "    # balloon flower\n",
    "    lambda x: f\"a photo of a {x}, a flower bud that inflates like a balloon before opening.\",\n",
    "    lambda x: f\"an image of a {x}, a star-shaped bloom in shades of blue or purple.\",\n",
    "    lambda x: f\"a close-up of a {x}, a unique flower with puffy unopened buds.\",\n",
    "\n",
    "    # giant white arum lily\n",
    "    lambda x: f\"a photo of a {x}, a large white flower with a trumpet-like shape.\",\n",
    "    lambda x: f\"an image of a {x}, also known as a calla lily, with a central yellow spadix.\",\n",
    "    lambda x: f\"a close-up of a {x}, an elegant flower with smooth white petals.\",\n",
    "\n",
    "    # fire lily\n",
    "    lambda x: f\"a photo of a {x}, a bright red or orange lily with curled petals.\",\n",
    "    lambda x: f\"an image of a {x}, a dramatic flower known for its flame-like appearance.\",\n",
    "    lambda x: f\"a close-up of a {x}, a fiery-looking lily with backward-bending petals.\",\n",
    "\n",
    "    # pincushion flower\n",
    "    lambda x: f\"a photo of a {x}, a flower with a domed center and delicate fringe.\",\n",
    "    lambda x: f\"an image of a {x}, often purple or lavender, resembling a pin-filled cushion.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its central disk and lace-like petals.\",\n",
    "\n",
    "    # fritillary\n",
    "    lambda x: f\"a photo of a {x}, a checkered bell-shaped flower often in purple tones.\",\n",
    "    lambda x: f\"an image of a {x}, a rare flower with a distinctive petal pattern.\",\n",
    "    lambda x: f\"a close-up of a {x}, a delicate wildflower with hanging, nodding blooms.\",\n",
    "\n",
    "    # red ginger\n",
    "    lambda x: f\"a photo of a {x}, a tropical flower with bright red bracts.\",\n",
    "    lambda x: f\"an image of a {x}, known for its bold color and upright flower spikes.\",\n",
    "    lambda x: f\"a close-up of a {x}, a striking plant native to rainforests.\",\n",
    "\n",
    "    # grape hyacinth\n",
    "    lambda x: f\"a photo of a {x}, a small bulbous plant with clusters of blue-purple flowers.\",\n",
    "    lambda x: f\"an image of a {x}, resembling tiny grapes arranged on a spike.\",\n",
    "    lambda x: f\"a close-up of a {x}, a spring flower with densely packed florets.\",\n",
    "\n",
    "    # corn poppy\n",
    "    lambda x: f\"a photo of a {x}, a bright red flower with papery petals and a dark center.\",\n",
    "    lambda x: f\"an image of a {x}, a wild poppy often found in meadows and fields.\",\n",
    "    lambda x: f\"a close-up of a {x}, a flower symbolizing remembrance and resilience.\",\n",
    "\n",
    "    # prince of wales feathers\n",
    "    lambda x: f\"a photo of a {x}, a spiky flower head resembling a feathery plume.\",\n",
    "    lambda x: f\"an image of a {x}, known for its upright purple-pink floral spikes.\",\n",
    "    lambda x: f\"a close-up of a {x}, a member of the amaranth family with plume-like blooms.\",\n",
    "\n",
    "    # stemless gentian\n",
    "    lambda x: f\"a photo of a {x}, a vivid blue flower growing close to the ground.\",\n",
    "    lambda x: f\"an image of a {x}, a low-growing gentian with trumpet-shaped petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a mountain flower with intensely blue blossoms.\",\n",
    "\n",
    "    # artichoke\n",
    "    lambda x: f\"a photo of a {x}, a thistle-like plant with edible buds and purple flowers.\",\n",
    "    lambda x: f\"an image of a {x}, a spiky flower head that blooms into a vibrant violet.\",\n",
    "    lambda x: f\"a close-up of a {x}, a large budding flower with a layered appearance.\",\n",
    "\n",
    "    # sweet william\n",
    "    lambda x: f\"a photo of a {x}, a cluster of small flowers in pink, red, or white.\",\n",
    "    lambda x: f\"an image of a {x}, a garden flower known for its fringed petal edges.\",\n",
    "    lambda x: f\"a close-up of a {x}, a fragrant bloom often used in cottage gardens.\",\n",
    "\n",
    "    # carnation\n",
    "    lambda x: f\"a photo of a {x}, a ruffled flower commonly seen in bouquets.\",\n",
    "    lambda x: f\"an image of a {x}, a traditional bloom with a clove-like scent.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its layered petals and vibrant color range.\",\n",
    "\n",
    "    # garden phlox\n",
    "    lambda x: f\"a photo of a {x}, a tall plant with clusters of pink or purple blooms.\",\n",
    "    lambda x: f\"an image of a {x}, a perennial flower found in cottage-style gardens.\",\n",
    "    lambda x: f\"a close-up of a {x}, a phlox with star-shaped petals growing in dense bunches.\",\n",
    "\n",
    "    # love in the mist\n",
    "    lambda x: f\"a photo of a {x}, a blue flower surrounded by fine, feathery foliage.\",\n",
    "    lambda x: f\"an image of a {x}, a delicate flower with a misty, lace-like background.\",\n",
    "    lambda x: f\"a close-up of a {x}, a whimsical bloom with soft, threadlike leaves.\",\n",
    "\n",
    "    # mexican aster\n",
    "    lambda x: f\"a photo of a {x}, a daisy-like flower with bright pink or purple petals.\",\n",
    "    lambda x: f\"an image of a {x}, a tall annual bloom often seen in wildflower fields.\",\n",
    "    lambda x: f\"a close-up of a {x}, a lightweight flower with yellow centers and soft petals.\",\n",
    "\n",
    "    # alpine sea holly\n",
    "    lambda x: f\"a photo of a {x}, a spiky blue flower with thistle-like bracts.\",\n",
    "    lambda x: f\"an image of a {x}, a unique alpine plant with metallic-colored petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a flower with a cone center and pointed star-shaped sepals.\",\n",
    "\n",
    "    # ruby-lipped cattleya\n",
    "    lambda x: f\"a photo of a {x}, an orchid with bold purple lips and pastel petals.\",\n",
    "    lambda x: f\"an image of a {x}, a showy flower with ruby-colored accents on its lip.\",\n",
    "    lambda x: f\"a close-up of a {x}, a fragrant orchid with elaborate ruffled petals.\",\n",
    "\n",
    "    # cape flower\n",
    "    lambda x: f\"a photo of a {x}, a brightly colored South African flower with daisy form.\",\n",
    "    lambda x: f\"an image of a {x}, known for its vivid hues and sun-tracking behavior.\",\n",
    "    lambda x: f\"a close-up of a {x}, a vibrant flower with a dark center and radiating petals.\",\n",
    "\n",
    "    # great masterwort\n",
    "    lambda x: f\"a photo of a {x}, a flower with a central cluster surrounded by papery bracts.\",\n",
    "    lambda x: f\"an image of a {x}, a perennial bloom with intricate star-like heads.\",\n",
    "    lambda x: f\"a close-up of a {x}, a soft-colored flower with detailed florets in the center.\",\n",
    "\n",
    "    # siam tulip\n",
    "    lambda x: f\"a photo of a {x}, a tropical flower with pink petals and green bracts.\",\n",
    "    lambda x: f\"an image of a {x}, also known as Curcuma, with cone-shaped blossoms.\",\n",
    "    lambda x: f\"a close-up of a {x}, a vibrant Thai flower with tulip-like structure.\",\n",
    "\n",
    "    # lenten rose\n",
    "    lambda x: f\"a photo of a {x}, a spring flower with nodding blooms and muted colors.\",\n",
    "    lambda x: f\"an image of a {x}, a hellebore plant with leathery leaves and soft petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cold-hardy flower blooming in late winter to early spring.\",\n",
    "\n",
    "    # barbeton daisy\n",
    "    lambda x: f\"a photo of a {x}, a brightly colored daisy native to South Africa.\",\n",
    "    lambda x: f\"an image of a {x}, known for its vivid red, orange, or pink petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cheerful flower with a prominent central disk.\",\n",
    "\n",
    "    # daffodil\n",
    "    lambda x: f\"a photo of a {x}, a trumpet-shaped flower with yellow or white petals.\",\n",
    "    lambda x: f\"an image of a {x}, one of the first blooms of spring with a central corona.\",\n",
    "    lambda x: f\"a close-up of a {x}, a classic bulb flower symbolizing renewal and hope.\",\n",
    "\n",
    "    # sword lily\n",
    "    lambda x: f\"a photo of a {x}, a tall flower with sword-like leaves and vertical blooms.\",\n",
    "    lambda x: f\"an image of a {x}, commonly known as gladiolus with stacked florets.\",\n",
    "    lambda x: f\"a close-up of a {x}, a showy flower in a rainbow of colors on long spikes.\",\n",
    "\n",
    "    # poinsettia\n",
    "    lambda x: f\"a photo of a {x}, a festive plant with red or white leaf-like bracts.\",\n",
    "    lambda x: f\"an image of a {x}, often used in winter displays with green foliage and colorful tops.\",\n",
    "    lambda x: f\"a close-up of a {x}, a holiday flower with bright petal-like leaves.\",\n",
    "\n",
    "    # bolero deep blue\n",
    "    lambda x: f\"a photo of a {x}, a compact flower with deep violet petals and ruffled texture.\",\n",
    "    lambda x: f\"an image of a {x}, a variety of pansy known for its rich blue coloring.\",\n",
    "    lambda x: f\"a close-up of a {x}, a velvety flower with intricate patterns and deep hues.\",\n",
    "\n",
    "    # wallflower\n",
    "    lambda x: f\"a photo of a {x}, a small clustered flower known for growing on walls or rocky soil.\",\n",
    "    lambda x: f\"an image of a {x}, a plant with fragrant blooms in yellow, orange, or red.\",\n",
    "    lambda x: f\"a close-up of a {x}, a simple flower with four-petal blossoms and warm colors.\",\n",
    "\n",
    "    # marigold\n",
    "    lambda x: f\"a photo of a {x}, a vibrant flower with layers of orange or yellow petals.\",\n",
    "    lambda x: f\"an image of a {x}, known for its strong scent and decorative garden use.\",\n",
    "    lambda x: f\"a close-up of a {x}, a sun-loving flower with round, bushy blooms.\",\n",
    "\n",
    "    # buttercup\n",
    "    lambda x: f\"a photo of a {x}, a shiny yellow flower with cup-shaped petals.\",\n",
    "    lambda x: f\"an image of a {x}, a wildflower with a simple structure and glossy surface.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cheerful bloom with golden overlapping petals.\",\n",
    "\n",
    "    # oxeye daisy\n",
    "    lambda x: f\"a photo of a {x}, a white-petaled daisy with a yellow central disk.\",\n",
    "    lambda x: f\"an image of a {x}, a classic meadow flower with a tall, slender stem.\",\n",
    "    lambda x: f\"a close-up of a {x}, a widespread wildflower resembling a common daisy.\",\n",
    "\n",
    "    # common dandelion\n",
    "    lambda x: f\"a photo of a {x}, a yellow flower with toothed leaves and fluffy seed heads.\",\n",
    "    lambda x: f\"an image of a {x}, a weedy plant known for its puffball seed dispersal.\",\n",
    "    lambda x: f\"a close-up of a {x}, a golden flower head made of many tiny florets.\",\n",
    "\n",
    "    # petunia\n",
    "    lambda x: f\"a photo of a {x}, a funnel-shaped flower often used in hanging baskets.\",\n",
    "    lambda x: f\"an image of a {x}, a colorful bloom available in many vibrant shades.\",\n",
    "    lambda x: f\"a close-up of a {x}, a soft, velvety flower with a wide trumpet-like shape.\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text feature loading function\n",
    "\n",
    "This function take as input a list of categories, and apply clip's text processing pipeline.\n",
    "in particular:\n",
    " - It first uses the prompt templates to generate a matrix of strings (shape num_prompts x num classes)\n",
    " - Then it tokenize the matrix\n",
    " - finally it pass the tokenized prompts thought the text encoder and thus getting the text embedding\n",
    "\n",
    "the final result is a matrix having the shape: `num_prompts` x `embedding size` x `num_classes`\n",
    "\n",
    "#### Difference between base and novel classes embeddings\n",
    "\n",
    "Here one important detail that you should note, is that when we generate the prompts for \"novel\" classes, we don't use\n",
    "the prompt templates that are specific for a class, instead we just use the general one, and we repeat them\n",
    "until we reach the desired size of the vector.\n",
    "\n",
    "Initially we reached the desired size with a simple padding prompt, such as \"a picture\", however\n",
    "by doing so, we forced the re-weighter to learn that novel classes, should on average have higher weights\n",
    "than base classes. This was because they had more \"bad\" prompts (i.e. padding prompts like \"a picture\"),\n",
    "and therefore needed higher weights to compensate. Even tho, this could be learned by the re-weighter\n",
    "we found that if we used a repetition of \"good\" prompts for the padding, the accuracy improved.\n",
    "We suspect this is due to the fact that we are removing some constraints on the re-weighter, and\n",
    "therefore it has less things to learn.\n",
    "\n",
    "The reason why had to make this distinction in the first place is quite interesting, but we don't have the necessary\n",
    "context to explain it at this point, so we will elaborate on this further in the last section of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def load_text_features(categories: list[CategoryLabel]):\n",
    "    \"\"\"\n",
    "    return size: num_prompts x num_categories x input_size\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # prompts for base classes (include both general and class specific templates)\n",
    "    prompts_for_base_classes = general_prompt_template + class_specific_prompt_templates\n",
    "    \n",
    "    # prompts for novel classes (are obtained with the repetition of general templates)\n",
    "    multiplier = len(prompts_for_base_classes) // len(general_prompt_template) + 1\n",
    "    prompts_for_novel_classes = general_prompt_template * multiplier\n",
    "    prompts_for_novel_classes = prompts_for_novel_classes[0:len(prompts_for_base_classes)]\n",
    "\n",
    "    def get_prompts_for_one_class(category: CategoryLabel) -> list[str]:\n",
    "        if category[\"novel\"]:\n",
    "            prompts = prompts_for_novel_classes\n",
    "        else:\n",
    "            prompts = prompts_for_base_classes\n",
    "        name = category[\"name\"]\n",
    "        return [t(name) for t in prompts ]\n",
    "    \n",
    "\n",
    "    text_inputs = [\n",
    "        clip.tokenize(\n",
    "            get_prompts_for_one_class(category)\n",
    "        ).to(DEVICE)\n",
    "        for category in categories\n",
    "    ]\n",
    "\n",
    "    text_features_array: list[torch.Tensor] = [\n",
    "        model.encode_text(x)\n",
    "        for x in text_inputs\n",
    "    ]\n",
    "\n",
    "    # shape: num_classes x num_prompts x embedding_size\n",
    "    text_features = torch.stack([\n",
    "        x/x.norm(dim=-1,keepdim=True)\n",
    "        for x in text_features_array\n",
    "    ])\n",
    "\n",
    "    # shape: num_prompts x embedding_size x num_classes\n",
    "    text_features = text_features.permute(1,2,0)\n",
    "\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TVrYUYTv9ttM"
   },
   "outputs": [],
   "source": [
    "if USE_DEFAULT_SPLIT:\n",
    "    train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "else:\n",
    "    train_set, val_set, test_set = get_data_custom_split(transform=preprocess)\n",
    "\n",
    "# split classes into base and novel\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "# split the three datasets\n",
    "train_base, _ = split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Weighter model\n",
    "\n",
    "This is the core model at the base of our architecture.\n",
    "\n",
    "It is a small FFNN that has the following Input/Outputs\n",
    " - **Input**: A one dimensional tensor (or two if considering batch size) that is\n",
    " the visual embedding of the image (i.e. the output of the visual encoder)\n",
    " - **Output**: A two dimensional tensor (or three if considering batch size) that is\n",
    " shaped `number of prompts` x `number of classes`\n",
    "\n",
    "The output will then be used to re-weight the various prompts so to obtain a single score\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "To put it in a more mathematical way, if we define the following notation:\n",
    " - $T_{ij}$: The text embedding of the class `j` generated with the prompt template `i`\n",
    " - $V$: The visual embedding of the image we are trying to classify\n",
    " - $W = Reweighter(V)$: The output of the re-weighter (shaped `number of prompts` x `number of classes` )\n",
    " - $w_{ij}$: This is the weight (i.e. estimated relative importance) that the prompt template `i` with class `j` has, in particular for the classification of the image that generated `V`\n",
    " - $s_j$: The absolute (i.e. weighted) score for the class `j`\n",
    "\n",
    "We can then use the aforementioned notation to express the formula of $s_j$\n",
    "$$\n",
    "s_j =  \\sum_{i=1}^{n\\_prompts} w_{ij} (T_{ij} \\cdot V)\n",
    "$$\n",
    "where $(T_{ij} \\cdot V)$ is the dot product between the text and the image embedding representing the similarity\n",
    "\n",
    "\n",
    "Finally we can get the classifier's output by taking the absolute score with the highest value\n",
    "$$\n",
    "class = argmax_j(s_j)\n",
    "$$\n",
    "\n",
    "\n",
    "#### A note on novel classes\n",
    "\n",
    "At this point a question may emerge:\n",
    "> If your model need to output a set of weights for each class, how will you be able to generalize to novel classes?\n",
    "\n",
    "And the answer is quite straightforward: We add to the output of the model a set of weights that represent the \"novel class\"\n",
    "\n",
    "The way they are trained is quite interesting, but we will discuss it, when it become relevant, for now you just need to\n",
    "note that we have create a custom \"indexing\" layer at the end, that map the original model output from a dimension of `num_base_classes + 1`\n",
    "to a dimension that depends on the number of classes we are trying to classify at a certain point in time.\n",
    "\n",
    "In this example we can se that a model has being trained on 4 base classes (A,B,C and D) and than\n",
    "it is adapted to be used in a context where 3 of the base classes are still relevant (A,B and D),\n",
    "one is discarded (C) and 3 novel classes are added.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/lucaSartore/CLIP-Few-shot/refs/heads/main/images/insexing_layer.png\" alt=\"drawing\" height=\"400\"/>\n",
    "\n",
    "This is the reason why the method `update_indexing_mask` is needed. It essentially re-configure the indexing layer to adapt the re-weighter's output to a context where the number of classes has changed\n",
    "\n",
    "#### The internal architecture\n",
    "Internally it is a simple network that take as input the visual embedding; it begins by project it into a smaller dimensional space,\n",
    "and then calculating the weights for each prompt and base class.\n",
    "Novel classes instead have a separate vector of parameter that does NOT depends on the input embedding.\n",
    "\n",
    "Initially there was not this distinction, and the novel class weights also depended on the image embedding.\n",
    "but we noted that making them independent from the image embedding gave better result (more details on this will follow in the last section of this notebook).\n",
    "\n",
    "#### Parameters\n",
    "TODO: Add a section that talks about the parameters once they are being tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_EMBEDDING_SIZE = int(model.encode_text(clip.tokenize(\"foo\").to(DEVICE)).shape[-1])\n",
    "NUMBER_OF_PROMPTS = len(general_prompt_template) + len(class_specific_prompt_templates)\n",
    "\n",
    "\n",
    "class ReWeighterModel(nn.Module):\n",
    "    def __init__(self, classes_for_training: list[CategoryLabel], internal_size = 75):\n",
    "        super().__init__()\n",
    "        # we need to store the classes used for training in order to be able to update the indexing mask\n",
    "        self.base_classes = [x for x in classes_for_training if not x[\"novel\"]]\n",
    "        self.num_base_classes = len(self.base_classes)\n",
    "        # We use the update method for initialization here\n",
    "        self.update_indexing_mask(classes_for_training)\n",
    "        ####################### DESIGN_CHOICE ######################################\n",
    "        # We observed that mapping the visual embedding in a small\n",
    "        # embedding space before calculating the weights help\n",
    "        # reducing overfitting, as we are \"compressing\" the\n",
    "        # representation, and therefore reducing the model capacity.\n",
    "        # We hypnotize that a low-dimensional space does not limit\n",
    "        # much the performance of our model, as the task of re-weighting\n",
    "        # is (at least intuitively) much simpler (and therefore lower dimensional)\n",
    "        # than the task of comparing images and text\n",
    "        ############################################################################\n",
    "        # the first layer maps the visual embedding into a lower-dimensional space.\n",
    "        self.l1 = nn.Linear(CLIP_EMBEDDING_SIZE, internal_size, dtype=model.dtype)\n",
    "        # one output weight for each prompt, and for each class\n",
    "        self.l2 = nn.Linear(internal_size, NUMBER_OF_PROMPTS * (self.num_base_classes), dtype=model.dtype)\n",
    "\n",
    "        # weights for novel classes don't depends on the input\n",
    "        weights = torch.rand(NUMBER_OF_PROMPTS).type(model.dtype)\n",
    "        self.novel_classes_weights = nn.parameter.Parameter(weights)\n",
    "        # sigmoid used to normalize the weights in the 0-1 range\n",
    "        # TODO: idea, maybe allowing for negative weight can improve things?\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        \"\"\"\n",
    "        input: [batch_size x clip_embedding_size] = the image-generated embeddings\n",
    "        output: [batch_size x number_of_prompts x num_classes] = the weight estimated for each prompt-class pair\n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        # the weights for base classes\n",
    "        x: torch.Tensor = self.l1(input)\n",
    "        x = self.l2(x)\n",
    "        x = x.reshape(batch_size, NUMBER_OF_PROMPTS, self.num_base_classes)\n",
    "\n",
    "        # the weights for novel classes\n",
    "        x_novel = self.novel_classes_weights.unsqueeze(0)\n",
    "        x_novel = x_novel.reshape(1, NUMBER_OF_PROMPTS, 1)\n",
    "        x_novel = x_novel.expand(batch_size, NUMBER_OF_PROMPTS, 1)\n",
    "\n",
    "        # concatenating into a single output\n",
    "        x = torch.concat([x, x_novel], dim=-1)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        x = x[:,:,self.indexing_mask]\n",
    "        return x\n",
    "\n",
    "    def update_indexing_mask(self, classes: list[CategoryLabel]):\n",
    "        \"\"\"\n",
    "        Update the indexing mask.\n",
    "        This function need to be used after a model has being trained for a specific set of classes,\n",
    "        \"\"\"\n",
    "\n",
    "        # map the class id, to the position on on the model's last layer\n",
    "        id_to_index = {x[\"id\"]: i for i,x in enumerate(self.base_classes)}\n",
    "\n",
    "        # given a class, it returns the index inside the model's last layer that should be used\n",
    "        # to re-weight the classes's prompts.\n",
    "        def get_index(label: CategoryLabel):\n",
    "            # novel categories are always mapped to the generic re-weighter (the last one)\n",
    "            # as we haven't learned a model for them\n",
    "            if label[\"novel\"]:\n",
    "                return len(self.base_classes)\n",
    "            # base classes instead have their specialized re-weighter at the corresponding index\n",
    "            return id_to_index[label[\"id\"]]\n",
    "\n",
    "        # building the indexing mask\n",
    "        self.indexing_mask = torch.Tensor([\n",
    "            get_index(x) for x in classes\n",
    "        ]).type(torch.long)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of virtual novel samples\n",
    "We mentioned in the previous section that the re-weighter has an output dedicated to \"novel classes\", but\n",
    "how can we train it if we don't have access to the novel classes?\n",
    "\n",
    "The answer is to use \"virtual-novel classes\" were we essentially duplicate all of the base classes and create a \"novel\" version of them.\n",
    "\n",
    "So essentially, if (like in our case) the classes are 102, and classes with IDs 0 to 50 are base, and IDs 51 to 101 are novel\n",
    "we create 51 new classes, and assign them IDs 102 to 152. Those new classes will have the same images, and same class names\n",
    "as the base one, but will have a different ID, and they will be marked as \"novel\"\n",
    "\n",
    "What this means is that all of the virtual-novel classes will share a single re-weighter output (it will still be a tensor, because\n",
    "there is one weight for each prompt, but it will not discriminate between the ID 102 and 103 for example).\n",
    "\n",
    "By doing so we essentially force the re-weighter to learn a set of weight that can be used to \n",
    "discriminate all the virtual-novel classes AT THE SAME TIME, and the assumption we made here is\n",
    "that if a SINGLE set of weights is good at discriminating 51 different classes, the same set\n",
    "will also be good at discriminating 51 new un-seen classes.\n",
    "\n",
    "We think this assumption is reasonable if we also assume that the split of novel and base classes is not particularly biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class names have being added to torch vision recently, but to avoid compatibility issues we added them here.\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# list of category labels of the base classes\n",
    "base_classes_label: list[CategoryLabel] = [\n",
    "    {\n",
    "        \"id\": x,\n",
    "        \"name\": CLASS_NAMES[x],\n",
    "        \"novel\": False\n",
    "    }\n",
    "    for x in base_classes\n",
    "]\n",
    "\n",
    "# list of category labels for the novel classes\n",
    "novel_classes_label: list[CategoryLabel] = [\n",
    "    {\n",
    "        \"id\": x,\n",
    "        \"name\": CLASS_NAMES[x],\n",
    "        \"novel\": True\n",
    "    }\n",
    "    for x in novel_classes\n",
    "]\n",
    "\n",
    "# list of category labels for all classes\n",
    "all_classes_labels = base_classes_label + novel_classes_label\n",
    "\n",
    "# list of category labels for base classes plus virtual-novel classes.\n",
    "# This is the one that will be used during training, and you can see\n",
    "# that we are essentially duplicating the number of novel classes,\n",
    "# keeping the name unchanged, but shifting the ID, so to avoid overlap.\n",
    "base_and_virtual_novel_classes_labels = base_classes_label + [\n",
    "    {\n",
    "        \"id\": x + len(all_classes_labels),\n",
    "        \"name\": CLASS_NAMES[x],\n",
    "        \"novel\": True\n",
    "    }\n",
    "    for x in base_classes\n",
    "]\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "E = TypeVar(\"E\")\n",
    "class VirtualNovelDataset(Dataset, Generic[T,E]):\n",
    "    \"\"\"\n",
    "    This is a custom dataset implementation, that create \"virtual classes\"\n",
    "    In short id create \"num_novel_classes\" new training samples, that are\n",
    "    assigned a new ID (by adding \"to_add\" at the original ID)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: Dataset[tuple[T,E]], num_novel_classes: int, to_add: E):\n",
    "        self._dataset_len: int = len(dataset) #type: ignore\n",
    "        self._dataset = dataset\n",
    "        self._virtual_novel: list[tuple[T,E]] = []\n",
    "        indexes = random.sample(range(self._dataset_len), num_novel_classes)\n",
    "        for index in indexes:\n",
    "            data, label = dataset[index]\n",
    "            new_label: E = label + to_add #type: ignore\n",
    "            self._virtual_novel.append((data, new_label))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._dataset_len + len(self._virtual_novel)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[T,E]:\n",
    "        # non virtual sample\n",
    "        if index < self._dataset_len:\n",
    "            return self._dataset[index]\n",
    "        # virtual sample:\n",
    "        else:\n",
    "            return self._virtual_novel[index - self._dataset_len]\n",
    "            \n",
    "train_base_and_virtual_novel = VirtualNovelDataset(train_base, len(train_base), len(all_classes_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "This is the section where we train the re-weighter. Overall, the training is pretty standard, with only two details to note:\n",
    "1. **The masking of some input prompts:** The \"virtual-novel\" training samples, create an issue, assume we pass a picture of\n",
    "a rose, through the model twice, the first time we pass it with the label \"rose\" and the second with the label \"rose-novel\".\n",
    "Obviously the model can't predict both of them correctly at the same time, and it will have to pick one, this results in the model being\n",
    "wrong 50% of the times, and thus creating some instability during training.\n",
    "To fix this issue we simply disabled one of the prompts during training, so for example, if we are evaluating \"rose-novel\" we disable\n",
    "all the prompts associated with \"rose\" by setting the text embeddings to zero.\n",
    "The exact implementation details can be seen in the code.\n",
    "\n",
    "1. **Virtually larger batch size** We had to implement some logic (namely waiting a few batches before calling `zero_grad` and `step`)\n",
    "to simulate larger batch sizes. We found that the re-weighter is extremely hard to train, if we have small batch sizes, on the other hand\n",
    "we also found that the training is really memory intensive sometimes, therefore we we use the aforementioned technique to\n",
    "achieve the same result as large batch sizes, without the same memory requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps: 100%|| 90/90 [11:55<00:00,  7.95s/it, loss=0.5750]\n"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "        clip: clip.model.CLIP,\n",
    "        weighter: ReWeighterModel ,\n",
    "        dataset: Dataset[tuple[torch.Tensor, int]],\n",
    "        categories: list[CategoryLabel],\n",
    "        batch_size: int,\n",
    "        num_steps: int,\n",
    "        device: torch.device | str,\n",
    "        batch_size_multiplier: int = 1,\n",
    "        \n",
    "    ):\n",
    "    clip.eval()\n",
    "    weighter.train()\n",
    "\n",
    "    # todo: this may be fixed now... se if it is worth removing this logic/comment\n",
    "    # We noted that the training is a bit unstable some times.\n",
    "    # the loss start at 1.5, and gradually go down reaching 0.8, \n",
    "    # but then it jumps to 12 or something. We haven't figured out\n",
    "    # the reason of this instability yet, but we \"fixed\" it by\n",
    "    # saving the best model, and returning that. So in the worst case\n",
    "    # scenario we can at least return this.\n",
    "    best_model: ReWeighterModel | None = None\n",
    "    best_loss: float = math.inf\n",
    "\n",
    "    # Todo: Add some interesting comments on why we chose the optimizer/parameters\n",
    "    # that we did, once we are done with the hyperparameters optimization\n",
    "    # optimizer = torch.optim.AdamW(params = weighter.parameters())\n",
    "    optimizer = torch.optim.SGD(params = weighter.parameters(), lr=5)\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero.\n",
    "    # this is the same as what was done in the zero-shot example notebook:\n",
    "    #   > contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    # but is more efficient later on, as the indexing can be done natively\n",
    "    # inside tensor, without using python structures\n",
    "    contig_cat2idx = torch.zeros(1+max(cat[\"id\"] for cat in categories)).long()\n",
    "    for idx, cat in enumerate(categories):\n",
    "        contig_cat2idx[cat[\"id\"]] = idx\n",
    "    \n",
    "    # loading the text features\n",
    "    # size: num_prompts x clip_embedding_size x num_categories\"]\n",
    "    text_features = load_text_features(categories)\n",
    "\n",
    "    # these constants are useful when re-shaping some tensors later\n",
    "    num_prompts = text_features.shape[0]\n",
    "    num_classes = text_features.shape[2]\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # definition of the loss\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    progress_bar = tqdm(range(num_steps), \"Steps\") \n",
    "    for _ in progress_bar:\n",
    "        image: torch.Tensor\n",
    "        target: torch.Tensor\n",
    "        losses: list[float] = []\n",
    "        for i, (image, target) in enumerate(dataloader):\n",
    "\n",
    "\n",
    "            # Converting the class indexes to contiguous indexes\n",
    "            # Equivalent line, in the zero-shot notebook:\n",
    "            #   > target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "            # shape: [batch_size]\n",
    "            target = contig_cat2idx[target]\n",
    "\n",
    "            target = target.to(DEVICE)\n",
    "            image = image.to(DEVICE)\n",
    "\n",
    "            # calculating the image features\n",
    "            with torch.no_grad():\n",
    "                image_features: torch.Tensor = clip.encode_image(image)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "            # We need to be able to mask out some text features (to avoid issues\n",
    "            # with virtual novel classes, as explained in markdown cell)\n",
    "            # Do do so, we need to expand the text features, and add a dimension (batch size)\n",
    "            # this is because every single element in a batch will have a different target\n",
    "            # (and therefore a different input feature to mask)\n",
    "            # shape: [batch_size x num_prompts x clip_embedding_size x num_classes]\n",
    "            masked_text_features = text_features \\\n",
    "                .clone() \\\n",
    "                .unsqueeze(0) \\\n",
    "                .expand(\n",
    "                    len(target), # batch size\n",
    "                    num_prompts,\n",
    "                    CLIP_EMBEDDING_SIZE,\n",
    "                    num_classes\n",
    "                ).clone()\n",
    "\n",
    "            # the following 15 lines are used to mask the text feature in the correct place,\n",
    "            # they are a bit hard to understand, however, here you can find a equivalent snippet of code\n",
    "            # that is much more readable, even tho it is less efficient computationally\n",
    "            # Note: 51 is the number of base classes\n",
    "            # >     for i in range(51):\n",
    "            # >         masked_text_features[target == i, :, :, i+51] = 0\n",
    "            # >         masked_text_features[target == i+51, :, :, i] = 0\n",
    "                \n",
    "\n",
    "            # vector tell me which class we need to zero in each element of a batch size\n",
    "            #shape: [batch_size]\n",
    "            class_to_zero = (target + len(categories) / 2) % len(categories)\n",
    "            class_to_zero = class_to_zero.long()\n",
    "            #shape: [batch_size x num_classes]\n",
    "            # I can use the one_hot notation to obtain exactly the boolean mask I need below\n",
    "            class_to_zero = nn.functional.one_hot(class_to_zero, num_classes).bool()\n",
    "\n",
    "            # I need to invert the axis order so that the boolean mask can be used correctly\n",
    "            # shape: [batch_size x num_classes x num_prompts x clip_embedding_size]\n",
    "            masked_text_features = masked_text_features.permute(0,3,2,1)\n",
    "            # masking out the undesired text features\n",
    "            masked_text_features[class_to_zero,:,:] = 0\n",
    "            # shape: [batch_size x num_prompts x clip_embedding_size x num_classes]\n",
    "            # set teh correct order back\n",
    "            masked_text_features = masked_text_features.permute(0,3,2,1)\n",
    "\n",
    "            # computing the similarity scores (using the masked text features)\n",
    "            # b = batch, e = embedding, p = prompts, c = classes\n",
    "            # shape: [batch_size x num_prompts x num_classes]\n",
    "            scores = torch.einsum(\"be,bpec -> bpc\", image_features, masked_text_features.detach())\n",
    "\n",
    "            # computing the weights\n",
    "            # shape: [batch_size x num_prompts x num_classes]\n",
    "            weights: torch.Tensor = weighter(image_features)\n",
    "    \n",
    "            # reweighing scores\n",
    "            scores *= weights\n",
    "\n",
    "            # summing up the scores of every different prompt\n",
    "            # shape: [batch_size x num_classes]\n",
    "            out = torch.sum(scores, dim=1)\n",
    "\n",
    "            # calculating the loss, and updating the gradient\n",
    "            loss: torch.Tensor = loss_fn(out, target)\n",
    "            losses.append(loss.item())\n",
    "            # this is to avoid having different losses (and therefore\n",
    "            # different behaver of the optimizer) when the batch\n",
    "            # size multiplier changes\n",
    "            loss /= batch_size_multiplier\n",
    "            loss.backward()\n",
    "            if (i+1) % batch_size_multiplier == 0:\n",
    "                # todo: add a note on gradient clipping when the hyperparameters are fully optimized\n",
    "                # (we may even remove it, but for now it seem to work fine...)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5)  \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # repeat here, to avoid losing some information if the number\n",
    "        # of batches is not divisible by batch_size_multiplier\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)  \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # printing the current loss in the progress bar\n",
    "        final_loss = np.average(losses)\n",
    "        progress_bar.set_postfix(loss=f\"{final_loss:.4f}\")\n",
    "\n",
    "        # saving the best model to cpu\n",
    "        if final_loss < best_loss:\n",
    "            best_model = copy.deepcopy(weighter).to(\"cpu\")\n",
    "\n",
    "    # returning the best model, according to the loss evaluation.\n",
    "    assert best_model is not None\n",
    "    return best_model.to(device)\n",
    "\n",
    "\n",
    "weighter = ReWeighterModel(base_and_virtual_novel_classes_labels, 75).to(DEVICE)\n",
    "\n",
    "\n",
    "weighter = train(\n",
    "    model,\n",
    "    weighter,\n",
    "    train_base_and_virtual_novel,\n",
    "    base_and_virtual_novel_classes_labels,\n",
    "    32,\n",
    "    90,\n",
    "    DEVICE,\n",
    "    4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcgMwr3J9VIg"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "In this section we evaluate teh model.\n",
    "\n",
    "#### A note on performances.\n",
    "\n",
    "If you did't carefully think about the architecture you would be forgiven to think that\n",
    "our implementation has extremely poor performance, after all model mixture architectures\n",
    "are in general pretty slow due to the fact that they have to run multiple models.\n",
    "However you will note in this section that the performances are indistinguishable\n",
    "to the naked eye w.r.t. zero shot clip. This is because the image embedding remains only one.\n",
    "The text embedding are much more, but since they don't change we can calculate them once and\n",
    "store them.\n",
    "All that is added then is the re-weighter, and the dot products which are both really\n",
    "lite on the GPU compared to the image embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49199,
     "status": "ok",
     "timestamp": 1743597826648,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "7uhblkvm9US4",
    "outputId": "a8b36190-e0c5-401b-830b-9a48711d934f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Evaluation on Base Classes: 100%|| 84/84 [00:13<00:00,  6.09it/s]\n",
      " Evaluation on Novel Classes: 100%|| 122/122 [00:16<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Base classes accuracy: 97.16%\n",
      " Novel classes accuracy: 78.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def eval(\n",
    "        clip: clip.model.CLIP,\n",
    "        weighter: ReWeighterModel ,\n",
    "        dataset: Dataset[tuple[torch.Tensor, int]],\n",
    "        categories: list[CategoryLabel],\n",
    "        batch_size: int,\n",
    "        device: torch.device | str,\n",
    "        label = \"\"\n",
    "    ):\n",
    "    # let's set the model in evaluation mode\n",
    "    clip.eval()\n",
    "    weighter.eval()\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = torch.zeros(1+max(cat[\"id\"] for cat in categories)).long()\n",
    "    for idx, cat in enumerate(categories):\n",
    "        contig_cat2idx[cat[\"id\"]] = idx\n",
    "\n",
    "    text_features = load_text_features(categories)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "\n",
    "    image: torch.Tensor\n",
    "    target: torch.Tensor\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "\n",
    "        target = contig_cat2idx[target]\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        image_features: torch.Tensor = clip.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # shape: [batch_size x num_prompts x num_classes]\n",
    "        scores: torch.Tensor = torch.matmul(image_features, text_features).permute(1,0,2)\n",
    "\n",
    "        # shape: [ batch_size x (num_prompts x num_classes)]\n",
    "        weights: torch.Tensor = weighter(image_features)\n",
    "\n",
    "        # reweighing scores\n",
    "        scores *= weights\n",
    "\n",
    "        out = torch.sum(scores, dim=1)\n",
    "        predicted_class = out.argmax(dim=-1)\n",
    "\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset) #type: ignore\n",
    "    return accuracy\n",
    "\n",
    "weighter.update_indexing_mask(base_classes_label)\n",
    "base_accuracy = eval(model, weighter, dataset=test_base, categories=base_classes_label, batch_size=32, device=DEVICE, label=\" Evaluation on Base Classes\")\n",
    "\n",
    "weighter.update_indexing_mask(novel_classes_label)\n",
    "novel_accuracy = eval(model, weighter, dataset=test_novel, categories=novel_classes_label, batch_size=32, device=DEVICE, label=\" Evaluation on Novel Classes\")\n",
    "\n",
    "\n",
    "print(f\" Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "print(f\" Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baYfLKNdfbUR"
   },
   "source": [
    "## Harmonic Mean\n",
    "Printing the ammonic mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1743597665969,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "rKAXR7hlfbUR",
    "outputId": "e00e50f4-3b0f-4e79-ed09-e8e82cc53668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Harmonic Mean: 87.09%\n"
     ]
    }
   ],
   "source": [
    "def get_harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    hm = numerator / denominator\n",
    "    return hm\n",
    "\n",
    "\n",
    "harmonic_mean = get_harmonic_mean(base_accuracy, novel_accuracy)\n",
    "print(f\" Harmonic Mean: {harmonic_mean*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of novel and base category together\n",
    "\n",
    "There is one detail of our project that we haven't addressed yet, but we think this is really important.\n",
    "\n",
    "It could look like from the outside, that our model are actually 2 different models, one for base classes\n",
    "and one for novel classes. And to be fair, this is a reasonable doubt; After all the novel classes\n",
    "has both different prompts and different weights w.r.t. the base one.\n",
    "\n",
    "Obviously creating two separate models would be a bit cheating, after all, most\n",
    "state of the art method have novel classes accuracies that are worst than zero-shot clip\n",
    "So if they where allowed to have two different models, one for novel, and one for base classes\n",
    "they could just use zero-shot clip for novel classes and gain a few percentages in the harmonic mean.\n",
    "\n",
    "So the question now becomes:\n",
    "> What make our approach \"A single model\" instead of two separate one?\n",
    "\n",
    "We think the answer lies in the ability of the model to maintain a good\n",
    "accuracy even when base and novel classes are mixed together.\n",
    "Most paper on clip few shot adaptation only show the accuracy on base classes,\n",
    "the accuracy on novel classes, and then they compute the harmonic mean. And to be fair\n",
    "this is enough for most architectures.\n",
    "\n",
    "However in our case we think that we must also proof that our model can maintain a good\n",
    "level of accuracy even when novel and base classes are mixed, so that we can show\n",
    "that our model is indeed a single one (and not just two specialized models glued together)\n",
    "\n",
    "#### Results\n",
    "\n",
    "The results show that when evaluating with a mixed dataset we lose about 3-4% of accuracy with respect\n",
    "to the harmonic mean. A small drop in accuracy is to be expected, simply because there are more classes\n",
    "and therefore more entropy. This 3-4% drop is in line with the drop that zero-shot clip loses under the\n",
    "same circumstances. So with this, we can confidently say that our model is indeed a single model,\n",
    "and not just two models stitched together\n",
    "\n",
    "\n",
    "#### How did we get here.\n",
    "The doubt about our model being indeed two separate model was totally legit.\n",
    "Indeed one of our first version of the model did act like two separate one.\n",
    "This early prototype had a good harmonic mean, however when \n",
    "testing it with a mixed dataset the accuracy dropped significantly to 65%.\n",
    "\n",
    "In particular we noted that the model was biased towards base classes,\n",
    "and then in a mixed context the number of pictures of a novel class that\n",
    "where classified as a base class was high, while the number of picture of \n",
    "a base class classified as a novel class where low.\n",
    "\n",
    "The changes we had to make to the program to fix this issue where 3:\n",
    "\n",
    "1. **Precise masking of text features during training**\n",
    "\n",
    "    We mentioned earlier that during training, if we are handling an example of a virtual-novel class A, based on the base class B\n",
    "    we have to mask the prompts relative to B to keep the training stable.\n",
    "    In an early version we simply masked all prompt relative to base classes, when we where training on virtual-novel classes, and\n",
    "    do vice versa when the opposite scenario showed up.\n",
    "    This however had a problem: there where non examples where base and virtual-novel classes show up together, without any mask,\n",
    "    and the model did not learned properly how to behave when those where mixed.\n",
    "\n",
    "2. **Not using class-specific prompts for novel-classes, and use padding instead**\n",
    "\n",
    "    At first glance, the decision of remove class-specific prompts from novel categories may have sounded a bit weird,\n",
    "    after all, why can't the model learn to ignore them if they are not relevant? Especially considering that it already\n",
    "    does so in the case of base-classes.\n",
    "    We don't have a 100% conclusive answer on this, however we hypnotize that the weighter is still giving\n",
    "    some weight to class specific prompts (that are some times useful, due to the fact that the novel classes\n",
    "    used during training are not actually novel, and just virtually novel).\n",
    "    And so, since the weighter was expecting class specific prompts to be relevant, but then those class specific\n",
    "    prompts where no longer relevant once we switched to real novel classes, the total score for novel classes ended\n",
    "    up being a bit lower then expected. and therefore base classes where chosen more often.\n",
    "\n",
    "3. **Using learnable parameters for the novel-class weights, instead of basing them on the image embedding**\n",
    "\n",
    "    In an earlier version of our model the re-weighter treated novel and base classes in the same wey.\n",
    "    however we realized that if novel classes's weight where simply learned in place (instead of depending\n",
    "    on the image embedding) the results where better.\n",
    "    This makes sense, as we can expect the embedding of novel classes to be different form the one of\n",
    "    virtual-novel classes, and therefore by cutting that connection we managed to maintain a better accuracy\n",
    "    in the transition from virtual-novel to novel classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Evaluation on all Classes: 100%|| 205/205 [00:23<00:00,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Novel and base classes accuracy: 83.61%\n",
      " Delta WRT harmonic mean: 3.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weighter.update_indexing_mask(all_classes_labels)\n",
    "total_accuracy = eval(model, weighter, dataset=test_set, categories=all_classes_labels, batch_size=32, device=DEVICE, label=\" Evaluation on all Classes\")\n",
    "print(f\" Novel and base classes accuracy: {total_accuracy*100:.2f}%\")\n",
    "\n",
    "delta = harmonic_mean - total_accuracy\n",
    "print(f\" Delta WRT harmonic mean: {delta*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
