{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYQiprE0pcVD"
   },
   "source": [
    "# CLIP zero-shot Evaluation\n",
    "This short notebook implements the dataset split into base and novel categories (see project assignment) and runs the zero-shot evaluation with CLIP.\n",
    "Feel free to copy the code contained in this notebook or to directly use this notebook as starting point for you project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UzXtFjhh7iOS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai_clip in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: ftfy in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (6.3.1)\n",
      "Requirement already satisfied: regex in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from ftfy->openai_clip) (0.2.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->openai_clip) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# we need to install clip as it is not pre-installed\n",
    "# you are also free to use open_clip which provide more models\n",
    "# https://github.com/mlfoundations/open_clip\n",
    "%pip install openai_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QtqdSOr8qqOn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import clip\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2353MHw1p24h"
   },
   "source": [
    "## Dataset Loading\n",
    "Let's get the data directly from torchvision as we have seen during labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "M_1CrUhZpVCq"
   },
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJI_a5EizA5a"
   },
   "source": [
    "## Base and Novel categories\n",
    "To split in base and novel categories we list all dataset classes, and count their number (we already know it's 102 but let's do it properly).\n",
    "Then, we just allocate the first half to base categories and the remaining half to novel ones.\n",
    "We can do this because we are simulating a real world application, but keep in mind this will not happen out there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nfq51vd8q_5a"
   },
   "outputs": [],
   "source": [
    "def base_novel_categories(dataset):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvDdoYQr2fIu"
   },
   "source": [
    "## Inspect Classes\n",
    "Let's now visualize which are the base and novel classes.\n",
    "To do so, we first get a dummy test set (without augmentations) as we are just interested in the dataset labels. Then, we split it useing `base_novel_categories`.\n",
    "Finally, we use the hard-coded CLASS_NAMES to print the class in natural language.\n",
    "\n",
    "> Note: the list of class names was only recently added to `torchvision.datasets.Flowers102`. To avoid useless errors that can occour to you, we decided to also provide such a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1743597666022,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "veGGpNDctCgR",
    "outputId": "72b17648-b4ee-42da-eb6e-37dfef3ef2f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Class Names: [(0, 'pink primrose'), (1, 'hard-leaved pocket orchid'), (2, 'canterbury bells'), (3, 'sweet pea'), (4, 'english marigold'), (5, 'tiger lily'), (6, 'moon orchid'), (7, 'bird of paradise'), (8, 'monkshood'), (9, 'globe thistle'), (10, 'snapdragon'), (11, \"colt's foot\"), (12, 'king protea'), (13, 'spear thistle'), (14, 'yellow iris'), (15, 'globe-flower'), (16, 'purple coneflower'), (17, 'peruvian lily'), (18, 'balloon flower'), (19, 'giant white arum lily'), (20, 'fire lily'), (21, 'pincushion flower'), (22, 'fritillary'), (23, 'red ginger'), (24, 'grape hyacinth'), (25, 'corn poppy'), (26, 'prince of wales feathers'), (27, 'stemless gentian'), (28, 'artichoke'), (29, 'sweet william'), (30, 'carnation'), (31, 'garden phlox'), (32, 'love in the mist'), (33, 'mexican aster'), (34, 'alpine sea holly'), (35, 'ruby-lipped cattleya'), (36, 'cape flower'), (37, 'great masterwort'), (38, 'siam tulip'), (39, 'lenten rose'), (40, 'barbeton daisy'), (41, 'daffodil'), (42, 'sword lily'), (43, 'poinsettia'), (44, 'bolero deep blue'), (45, 'wallflower'), (46, 'marigold'), (47, 'buttercup'), (48, 'oxeye daisy'), (49, 'common dandelion'), (50, 'petunia')]\n",
      "Novel Class Names: [(51, 'wild pansy'), (52, 'primula'), (53, 'sunflower'), (54, 'pelargonium'), (55, 'bishop of llandaff'), (56, 'gaura'), (57, 'geranium'), (58, 'orange dahlia'), (59, 'pink-yellow dahlia?'), (60, 'cautleya spicata'), (61, 'japanese anemone'), (62, 'black-eyed susan'), (63, 'silverbush'), (64, 'californian poppy'), (65, 'osteospermum'), (66, 'spring crocus'), (67, 'bearded iris'), (68, 'windflower'), (69, 'tree poppy'), (70, 'gazania'), (71, 'azalea'), (72, 'water lily'), (73, 'rose'), (74, 'thorn apple'), (75, 'morning glory'), (76, 'passion flower'), (77, 'lotus'), (78, 'toad lily'), (79, 'anthurium'), (80, 'frangipani'), (81, 'clematis'), (82, 'hibiscus'), (83, 'columbine'), (84, 'desert-rose'), (85, 'tree mallow'), (86, 'magnolia'), (87, 'cyclamen'), (88, 'watercress'), (89, 'canna lily'), (90, 'hippeastrum'), (91, 'bee balm'), (92, 'ball moss'), (93, 'foxglove'), (94, 'bougainvillea'), (95, 'camellia'), (96, 'mallow'), (97, 'mexican petunia'), (98, 'bromelia'), (99, 'blanket flower'), (100, 'trumpet creeper'), (101, 'blackberry lily')]\n"
     ]
    }
   ],
   "source": [
    "_, _, tmp_test = get_data()\n",
    "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
    "print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8puO1VNpzwvi"
   },
   "source": [
    "## Split Dataset\n",
    "The next step is to actually split the dataset into the base and novel categories we extract from `base_novel_categories`.\n",
    "To split the data we need the dataset (obviously) and the list of base classes. If the sample label is not part of the base categories, then it must be part of the novel ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "msOszMs2zRRu"
   },
   "outputs": [],
   "source": [
    "def split_data(dataset, base_classes):\n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    # we create a set of base classes to compute the test below in O(1)\n",
    "    # this is optional and can be removed\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    # here we create the dataset subsets\n",
    "    # the torch Subset is just a wrapper around the dataset\n",
    "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
    "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
    "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQZT22rE8hBw"
   },
   "source": [
    "## Extract k shots\n",
    "As the dataset already provides 10 train and validation shots, we do not need to extract them.\n",
    "Beaware that Few-Shot Adaptation papers must do this operation as most datasets count significantly more samples in both the training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KpbPRLr7WL_"
   },
   "source": [
    "## Load CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5162,
     "status": "ok",
     "timestamp": 1743597617669,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "Sh6uLZRT7YJx",
    "outputId": "25ef91c9-9879-4f50-d1eb-d2c53c194498"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x000001CF2E1F1D00>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# preprocess contains CLIP's pre-defined augmentations, let's inspect them!\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM9H14899ses"
   },
   "source": [
    "## Load and Prepare Data\n",
    "Here we get the three dataset split and pass clip pre-defined augmentations.\n",
    "Then, we compute base and novel categories (in this case is redundand as we already did it before).\n",
    "Finally, se split the three datasets into base and novel categories.\n",
    "As we want to use the novel categories only for the test set, we drop `train_novel` and `val_novel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# defining the templates that we are going to use\n",
    "prompt_template: list[Callable[[str], str]] = [\n",
    "    lambda x: f\"a photo of a {x}, a type of flower.\",\n",
    "    lambda x: f\"a {x} flower.\",\n",
    "    lambda x: f\"a photo of some {x}, a type of flower.\",\n",
    "    lambda x: f\"some {x} flowers.\",\n",
    "    lambda x: f\"a close-up of a {x} flower.\",\n",
    "    lambda x: f\"an image of a {x} blossom.\",\n",
    "    lambda x: f\"a beautiful {x} in bloom.\",\n",
    "    lambda x: f\"a bunch of {x} flowers.\",\n",
    "    lambda x: f\"a macro shot of a {x} flower.\",\n",
    "    lambda x: f\"a single {x} flower.\",\n",
    "    lambda x: f\"fresh {x} flowers in a garden.\",\n",
    "    lambda x: f\"a {x} flower in the wild.\",\n",
    "    lambda x: f\"a botanical photograph of a {x}.\",\n",
    "    lambda x: f\"a vibrant {x} bloom.\",\n",
    "    lambda x: f\"a {x} plant with flowers.\",\n",
    "    lambda x: f\"a {x} growing in nature.\",\n",
    "    lambda x: f\"a {x} flower in sunlight.\",\n",
    "    lambda x: f\"a colorful {x} flower close-up.\",\n",
    "    lambda x: f\"a {x}, commonly found in gardens.\",\n",
    "    lambda x: f\"wild {x} flowers blooming.\",\n",
    "    lambda x: f\"a garden filled with {x} flowers.\",\n",
    "    lambda x: f\"floral photography featuring a {x}.\",\n",
    "    lambda x: f\"an aesthetic photo of a {x}.\",\n",
    "    lambda x: f\"a {x} flower in full bloom.\",\n",
    "\n",
    "    # Descriptive\n",
    "    lambda x: f\"a large blooming {x}.\",\n",
    "    lambda x: f\"a freshly picked {x}.\",\n",
    "    lambda x: f\"a wilted {x} flower.\",\n",
    "    lambda x: f\"a {x} with dewdrops on its petals.\",\n",
    "    lambda x: f\"a delicate {x} on a green stem.\",\n",
    "    lambda x: f\"a colorful bouquet with {x}.\",\n",
    "\n",
    "    # Scientific-ish\n",
    "    lambda x: f\"a botanical illustration of {x}.\",\n",
    "    lambda x: f\"a herbarium specimen of {x}.\",\n",
    "    lambda x: f\"field photo of {x} species.\",\n",
    "    lambda x: f\"{x} photographed for a flora study.\",\n",
    "    lambda x: f\"a study sample of the {x} flower.\",\n",
    "    lambda x: f\"{x} genus flower in bloom.\",\n",
    "\n",
    "    # Casual / Internet Style\n",
    "    lambda x: f\"my favorite flower: the {x}.\",\n",
    "    lambda x: f\"saw a {x} today!\",\n",
    "    lambda x: f\"check out this {x} flower!\",\n",
    "    lambda x: f\"flowers like {x} are amazing.\",\n",
    "    lambda x: f\"the {x} is blooming this season.\",\n",
    "\n",
    "    # Photographic / Artistic\n",
    "    lambda x: f\"an artistic photo of a {x}.\",\n",
    "    lambda x: f\"film photo of a {x} flower.\",\n",
    "    lambda x: f\"a {x} in black and white.\",\n",
    "    lambda x: f\"the silhouette of a {x} in sunset light.\",\n",
    "    lambda x: f\"a {x} flower in a vintage vase.\",\n",
    "    lambda x: f\"an abstract painting of a {x}.\",\n",
    "    lambda x: f\"macro photography of a {x} blossom.\",\n",
    "\n",
    "    # Poetic or Metaphorical\n",
    "    lambda x: f\"a {x}, soft as a whisper.\",\n",
    "    lambda x: f\"a {x} dancing in the wind.\",\n",
    "    lambda x: f\"petals of the {x}, kissed by rain.\",\n",
    "    lambda x: f\"a lonely {x} on a quiet morning.\",\n",
    "    lambda x: f\"a {x} symbolizing peace and beauty.\",\n",
    "    lambda x: f\"like a {x} in springtime.\",\n",
    "\n",
    "    # Contextual / Scene-based\n",
    "    lambda x: f\"a {x} in a wildflower meadow.\",\n",
    "    lambda x: f\"a {x} flower on a wedding table.\",\n",
    "    lambda x: f\"{x} flowers in a forest clearing.\",\n",
    "    lambda x: f\"a {x} growing beside a stone path.\",\n",
    "    lambda x: f\"{x} blossoms in a city garden.\",\n",
    "    lambda x: f\"{x} petals scattered on the ground.\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "TVrYUYTv9ttM"
   },
   "outputs": [],
   "source": [
    "# get the three datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# split classes into base and novel\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "# split the three datasets\n",
    "train_base, _ = split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "@torch.no_grad()\n",
    "def load_text_features(categories: list[int]):\n",
    "    \"\"\"\n",
    "    size: num_prompts x num_categories x input_size\"]\n",
    "    \"\"\"\n",
    "    text_inputs = [\n",
    "        clip.tokenize(\n",
    "            [template(CLASS_NAMES[c]) for c in categories]\n",
    "        ).to(device)\n",
    "        for template in prompt_template\n",
    "    ]\n",
    "\n",
    "    text_features_array: list[torch.Tensor] = [\n",
    "        model.encode_text(x)\n",
    "        for x in text_inputs\n",
    "    ]\n",
    "\n",
    "    # shape: num_prompts x num_classes x embedding_size\n",
    "    text_features = torch.stack([\n",
    "        x/x.norm(dim=-1,keepdim=True)\n",
    "        for x in text_features_array\n",
    "    ])\n",
    "\n",
    "\n",
    "    # shape: num_prompts x embedding_size x num_classes\n",
    "    text_features = text_features.permute(0,2,1)\n",
    "\n",
    "    return text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_embedding_size = int(model.encode_text(clip.tokenize(\"foo\").to(device)).shape[-1])\n",
    "num_of_prompts = len(prompt_template)\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class ModelWeightingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weights = nn.Linear(clip_embedding_size, num_of_prompts, dtype=model.dtype)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        \"\"\"\n",
    "        input: [batch_size x clip_embedding_size] = the image-generated embeddings\n",
    "        \"\"\"\n",
    "        x = self.weights(input)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.82421875\n",
      "loss: 1.830078125\n",
      "loss: 1.94140625\n",
      "loss: 1.8037109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   2%|â–Ž         | 1/40 [00:05<03:43,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.677734375\n",
      "loss: 1.775390625\n",
      "loss: 1.701171875\n",
      "loss: 1.611328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   5%|â–Œ         | 2/40 [00:11<03:43,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.705078125\n",
      "loss: 1.630859375\n",
      "loss: 1.4619140625\n",
      "loss: 1.572265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   8%|â–Š         | 3/40 [00:17<03:35,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.6083984375\n",
      "loss: 1.5810546875\n",
      "loss: 1.60546875\n",
      "loss: 1.31640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  10%|â–ˆ         | 4/40 [00:23<03:26,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.447265625\n",
      "loss: 1.484375\n",
      "loss: 1.4609375\n",
      "loss: 1.544921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  12%|â–ˆâ–Ž        | 5/40 [00:28<03:18,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5908203125\n",
      "loss: 1.34765625\n",
      "loss: 1.4228515625\n",
      "loss: 1.44921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  15%|â–ˆâ–Œ        | 6/40 [00:34<03:12,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4072265625\n",
      "loss: 1.3642578125\n",
      "loss: 1.474609375\n",
      "loss: 1.46875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  18%|â–ˆâ–Š        | 7/40 [00:39<03:05,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4736328125\n",
      "loss: 1.337890625\n",
      "loss: 1.30859375\n",
      "loss: 1.5234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  20%|â–ˆâ–ˆ        | 8/40 [00:45<02:59,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.65625\n",
      "loss: 1.30859375\n",
      "loss: 1.3076171875\n",
      "loss: 1.30859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:51<02:54,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.173828125\n",
      "loss: 1.4580078125\n",
      "loss: 1.4765625\n",
      "loss: 1.427734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:56<02:48,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3466796875\n",
      "loss: 1.25390625\n",
      "loss: 1.3857421875\n",
      "loss: 1.509765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  28%|â–ˆâ–ˆâ–Š       | 11/40 [01:02<02:43,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.2431640625\n",
      "loss: 1.373046875\n",
      "loss: 1.486328125\n",
      "loss: 1.359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [01:08<02:38,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.310546875\n",
      "loss: 1.4736328125\n",
      "loss: 1.3310546875\n",
      "loss: 1.31640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [01:13<02:31,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.32421875\n",
      "loss: 1.478515625\n",
      "loss: 1.2470703125\n",
      "loss: 1.359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [01:19<02:26,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4189453125\n",
      "loss: 1.330078125\n",
      "loss: 1.1884765625\n",
      "loss: 1.451171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [01:24<02:20,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3896484375\n",
      "loss: 1.2685546875\n",
      "loss: 1.2763671875\n",
      "loss: 1.4345703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [01:30<02:14,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4951171875\n",
      "loss: 1.1318359375\n",
      "loss: 1.265625\n",
      "loss: 1.4599609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [01:36<02:10,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3037109375\n",
      "loss: 1.2412109375\n",
      "loss: 1.3994140625\n",
      "loss: 1.3916015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [01:42<02:05,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4287109375\n",
      "loss: 1.4130859375\n",
      "loss: 1.1806640625\n",
      "loss: 1.2998046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [01:47<01:59,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.298828125\n",
      "loss: 1.3154296875\n",
      "loss: 1.3681640625\n",
      "loss: 1.3271484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [01:53<01:54,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.169921875\n",
      "loss: 1.166015625\n",
      "loss: 1.439453125\n",
      "loss: 1.52734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [01:59<01:48,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3359375\n",
      "loss: 1.224609375\n",
      "loss: 1.2529296875\n",
      "loss: 1.4765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [02:05<01:44,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.37109375\n",
      "loss: 1.1494140625\n",
      "loss: 1.44140625\n",
      "loss: 1.318359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [02:10<01:38,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3818359375\n",
      "loss: 1.109375\n",
      "loss: 1.4482421875\n",
      "loss: 1.33203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [02:16<01:31,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.240234375\n",
      "loss: 1.43359375\n",
      "loss: 1.125\n",
      "loss: 1.466796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [02:22<01:25,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3046875\n",
      "loss: 1.2978515625\n",
      "loss: 1.4150390625\n",
      "loss: 1.236328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [02:27<01:19,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3818359375\n",
      "loss: 1.0849609375\n",
      "loss: 1.404296875\n",
      "loss: 1.3779296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [02:33<01:14,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.41796875\n",
      "loss: 1.0869140625\n",
      "loss: 1.4453125\n",
      "loss: 1.29296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [02:39<01:08,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3515625\n",
      "loss: 1.265625\n",
      "loss: 1.328125\n",
      "loss: 1.2900390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [02:45<01:02,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.2060546875\n",
      "loss: 1.3388671875\n",
      "loss: 1.3515625\n",
      "loss: 1.333984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [02:50<00:57,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.18359375\n",
      "loss: 1.3837890625\n",
      "loss: 1.3154296875\n",
      "loss: 1.3427734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [02:56<00:51,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.259765625\n",
      "loss: 1.416015625\n",
      "loss: 1.318359375\n",
      "loss: 1.2236328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [03:02<00:46,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4755859375\n",
      "loss: 1.2646484375\n",
      "loss: 1.4091796875\n",
      "loss: 1.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [03:08<00:40,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.2373046875\n",
      "loss: 1.3408203125\n",
      "loss: 1.2763671875\n",
      "loss: 1.357421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [03:13<00:34,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3583984375\n",
      "loss: 1.173828125\n",
      "loss: 1.42578125\n",
      "loss: 1.248046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [03:19<00:28,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.2060546875\n",
      "loss: 1.388671875\n",
      "loss: 1.0869140625\n",
      "loss: 1.5244140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [03:25<00:22,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.2509765625\n",
      "loss: 1.5068359375\n",
      "loss: 1.2626953125\n",
      "loss: 1.17578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [03:30<00:17,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.33203125\n",
      "loss: 1.2431640625\n",
      "loss: 1.32421875\n",
      "loss: 1.2958984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [03:36<00:11,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.25\n",
      "loss: 1.4140625\n",
      "loss: 1.35546875\n",
      "loss: 1.1708984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [03:42<00:05,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.46484375\n",
      "loss: 1.15625\n",
      "loss: 1.111328125\n",
      "loss: 1.458984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [03:48<00:00,  5.71s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip.model\n",
    "from typing import cast\n",
    "\n",
    "\n",
    "def train(\n",
    "        clip: clip.model.CLIP,\n",
    "        weighter: ModelWeightingModel ,\n",
    "        dataset: torch.utils.data.Subset[tuple[torch.Tensor, torch.Tensor]],\n",
    "        categories: list[int],\n",
    "        batch_size: int,\n",
    "        num_steps: int,\n",
    "        device: torch.device | str\n",
    "    ):\n",
    "    clip.eval()\n",
    "    weighter.train()\n",
    "\n",
    "\n",
    "    # optimizer = torch.optim.AdamW(params = weighter.parameters(), lr=0.00001)\n",
    "    optimizer = torch.optim.SGD(params = weighter.parameters(), lr=5)\n",
    "\n",
    "\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    # size: num_prompts x num_categories x input_size\"]\n",
    "    text_features = load_text_features(categories)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in tqdm(range(num_steps), \"Steps\"):\n",
    "\n",
    "\n",
    "        image: torch.Tensor\n",
    "        target: torch.Tensor\n",
    "        for image, target in dataloader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            target = torch.Tensor([contig_cat2idx[cast(int,t.item())] for t in target]).long()\n",
    "\n",
    "\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                image_features: torch.Tensor = clip.encode_image(image)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "            # shape: [batch_size x num_prompts x num_classes]\n",
    "            scores: torch.Tensor = torch.matmul(image_features, text_features).permute(1,0,2).detach()\n",
    "\n",
    "            # shape: [ batch_size x num_prompts]\n",
    "            weights: torch.Tensor = weighter(image_features.detach())\n",
    "            # reweighing scores\n",
    "            scores = (weights * scores.permute(2,0,1)).permute(1,2,0)\n",
    "\n",
    "            # shape: [batch_size x num_classes]\n",
    "            out = torch.sum(scores, dim=1)\n",
    "\n",
    "            # target_matrix = torch.nn.functional.one_hot(target, num_classes=out.shape[1])\n",
    "            # target_matrix = target_matrix.to(device).type(out.type())\n",
    "\n",
    "            loss: torch.Tensor = loss_fn(out, target)\n",
    "\n",
    "            print(f\"loss: {loss}\")\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "weighter = ModelWeightingModel().to(device)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    weighter,\n",
    "    train_base,\n",
    "    base_classes,\n",
    "    128,\n",
    "    40,\n",
    "    device\n",
    ")\n",
    "novel_classes   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcgMwr3J9VIg"
   },
   "source": [
    "## Compute Zero-Shot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49199,
     "status": "ok",
     "timestamp": 1743597826648,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "7uhblkvm9US4",
    "outputId": "a8b36190-e0c5-401b-830b-9a48711d934f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:   5%|â–Œ         | 1/20 [00:07<02:23,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9751, 0.9761, 0.9751, 0.9766, 0.9771, 0.9751, 0.9790, 0.9766, 0.9761,\n",
      "        0.9756, 0.9771, 0.9736, 0.9766, 0.9775, 0.9756, 0.9712, 0.9766, 0.9766,\n",
      "        0.9766, 0.9775, 0.9756, 0.9707, 0.9702, 0.9780, 0.9751, 0.9746, 0.9756,\n",
      "        0.9761, 0.9741, 0.9673, 0.9741, 0.9673, 0.9717, 0.9771, 0.9736, 0.9805,\n",
      "        0.9800, 0.9678, 0.9761, 0.9780, 0.9775, 0.9712, 0.9751, 0.9692, 0.9688,\n",
      "        0.9736, 0.9683, 0.9761, 0.9717, 0.9727, 0.9751, 0.9712, 0.9722, 0.9531,\n",
      "        0.9761, 0.9727, 0.9775, 0.9736, 0.9780, 0.9746], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  10%|â–ˆ         | 2/20 [00:07<00:58,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9741, 0.9746, 0.9741, 0.9756, 0.9756, 0.9736, 0.9775, 0.9751, 0.9746,\n",
      "        0.9741, 0.9766, 0.9727, 0.9756, 0.9761, 0.9746, 0.9702, 0.9751, 0.9751,\n",
      "        0.9751, 0.9761, 0.9741, 0.9688, 0.9678, 0.9775, 0.9746, 0.9731, 0.9736,\n",
      "        0.9751, 0.9727, 0.9663, 0.9722, 0.9648, 0.9702, 0.9756, 0.9727, 0.9790,\n",
      "        0.9785, 0.9663, 0.9751, 0.9761, 0.9761, 0.9688, 0.9746, 0.9668, 0.9673,\n",
      "        0.9722, 0.9663, 0.9741, 0.9688, 0.9702, 0.9731, 0.9697, 0.9707, 0.9507,\n",
      "        0.9741, 0.9712, 0.9771, 0.9717, 0.9766, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  15%|â–ˆâ–Œ        | 3/20 [00:08<00:32,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9727, 0.9722, 0.9727, 0.9746, 0.9741, 0.9722, 0.9761, 0.9736, 0.9736,\n",
      "        0.9727, 0.9751, 0.9717, 0.9736, 0.9746, 0.9736, 0.9692, 0.9736, 0.9746,\n",
      "        0.9731, 0.9746, 0.9727, 0.9673, 0.9673, 0.9751, 0.9722, 0.9717, 0.9717,\n",
      "        0.9736, 0.9712, 0.9644, 0.9707, 0.9639, 0.9688, 0.9741, 0.9707, 0.9785,\n",
      "        0.9771, 0.9653, 0.9741, 0.9746, 0.9746, 0.9673, 0.9731, 0.9663, 0.9653,\n",
      "        0.9707, 0.9653, 0.9731, 0.9688, 0.9688, 0.9722, 0.9683, 0.9697, 0.9492,\n",
      "        0.9727, 0.9702, 0.9761, 0.9707, 0.9761, 0.9717], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  20%|â–ˆâ–ˆ        | 4/20 [00:08<00:20,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9751, 0.9746, 0.9751, 0.9761, 0.9766, 0.9746, 0.9780, 0.9751, 0.9756,\n",
      "        0.9756, 0.9761, 0.9731, 0.9761, 0.9766, 0.9751, 0.9712, 0.9761, 0.9761,\n",
      "        0.9766, 0.9771, 0.9751, 0.9702, 0.9688, 0.9775, 0.9746, 0.9746, 0.9741,\n",
      "        0.9751, 0.9736, 0.9663, 0.9731, 0.9663, 0.9707, 0.9761, 0.9731, 0.9800,\n",
      "        0.9790, 0.9668, 0.9756, 0.9771, 0.9761, 0.9702, 0.9746, 0.9683, 0.9683,\n",
      "        0.9727, 0.9683, 0.9756, 0.9712, 0.9717, 0.9736, 0.9712, 0.9717, 0.9512,\n",
      "        0.9756, 0.9717, 0.9771, 0.9731, 0.9775, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:08<00:13,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9707, 0.9722, 0.9707, 0.9731, 0.9731, 0.9707, 0.9751, 0.9727, 0.9731,\n",
      "        0.9717, 0.9741, 0.9697, 0.9722, 0.9736, 0.9717, 0.9673, 0.9727, 0.9727,\n",
      "        0.9731, 0.9736, 0.9722, 0.9668, 0.9644, 0.9751, 0.9717, 0.9697, 0.9702,\n",
      "        0.9727, 0.9697, 0.9629, 0.9692, 0.9624, 0.9663, 0.9731, 0.9702, 0.9771,\n",
      "        0.9766, 0.9629, 0.9727, 0.9741, 0.9736, 0.9648, 0.9712, 0.9629, 0.9639,\n",
      "        0.9702, 0.9634, 0.9722, 0.9658, 0.9668, 0.9702, 0.9663, 0.9683, 0.9478,\n",
      "        0.9717, 0.9688, 0.9756, 0.9688, 0.9736, 0.9712], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:09,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9751, 0.9766, 0.9751, 0.9766, 0.9771, 0.9751, 0.9785, 0.9766, 0.9761,\n",
      "        0.9761, 0.9775, 0.9731, 0.9766, 0.9775, 0.9756, 0.9717, 0.9766, 0.9766,\n",
      "        0.9766, 0.9775, 0.9756, 0.9707, 0.9697, 0.9780, 0.9756, 0.9751, 0.9751,\n",
      "        0.9761, 0.9741, 0.9678, 0.9736, 0.9673, 0.9717, 0.9771, 0.9741, 0.9805,\n",
      "        0.9800, 0.9683, 0.9766, 0.9775, 0.9775, 0.9707, 0.9756, 0.9688, 0.9692,\n",
      "        0.9736, 0.9683, 0.9756, 0.9717, 0.9722, 0.9751, 0.9712, 0.9727, 0.9531,\n",
      "        0.9756, 0.9731, 0.9775, 0.9731, 0.9780, 0.9751], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:07,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9746, 0.9746, 0.9761, 0.9766, 0.9741, 0.9780, 0.9751, 0.9751,\n",
      "        0.9751, 0.9766, 0.9736, 0.9756, 0.9766, 0.9746, 0.9712, 0.9751, 0.9761,\n",
      "        0.9761, 0.9771, 0.9751, 0.9702, 0.9692, 0.9771, 0.9741, 0.9741, 0.9741,\n",
      "        0.9751, 0.9736, 0.9663, 0.9731, 0.9663, 0.9707, 0.9766, 0.9731, 0.9805,\n",
      "        0.9790, 0.9673, 0.9756, 0.9766, 0.9766, 0.9697, 0.9746, 0.9688, 0.9683,\n",
      "        0.9727, 0.9683, 0.9751, 0.9712, 0.9717, 0.9736, 0.9712, 0.9717, 0.9517,\n",
      "        0.9751, 0.9722, 0.9771, 0.9727, 0.9775, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:09<00:05,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9731, 0.9722, 0.9727, 0.9746, 0.9751, 0.9722, 0.9761, 0.9741, 0.9731,\n",
      "        0.9731, 0.9746, 0.9722, 0.9736, 0.9746, 0.9736, 0.9692, 0.9741, 0.9741,\n",
      "        0.9736, 0.9751, 0.9727, 0.9683, 0.9658, 0.9756, 0.9722, 0.9712, 0.9717,\n",
      "        0.9741, 0.9717, 0.9639, 0.9707, 0.9634, 0.9683, 0.9741, 0.9717, 0.9780,\n",
      "        0.9775, 0.9644, 0.9736, 0.9751, 0.9746, 0.9668, 0.9722, 0.9663, 0.9648,\n",
      "        0.9702, 0.9653, 0.9736, 0.9683, 0.9688, 0.9717, 0.9683, 0.9692, 0.9492,\n",
      "        0.9731, 0.9707, 0.9761, 0.9712, 0.9756, 0.9712], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:09<00:04,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9736, 0.9741, 0.9736, 0.9756, 0.9761, 0.9731, 0.9775, 0.9751, 0.9746,\n",
      "        0.9746, 0.9761, 0.9727, 0.9746, 0.9761, 0.9741, 0.9697, 0.9751, 0.9751,\n",
      "        0.9756, 0.9761, 0.9741, 0.9688, 0.9673, 0.9771, 0.9741, 0.9722, 0.9731,\n",
      "        0.9751, 0.9722, 0.9653, 0.9722, 0.9653, 0.9697, 0.9756, 0.9722, 0.9790,\n",
      "        0.9785, 0.9653, 0.9746, 0.9766, 0.9761, 0.9678, 0.9736, 0.9668, 0.9668,\n",
      "        0.9717, 0.9663, 0.9741, 0.9692, 0.9702, 0.9727, 0.9692, 0.9702, 0.9517,\n",
      "        0.9746, 0.9717, 0.9771, 0.9722, 0.9766, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:10<00:03,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9722, 0.9722, 0.9717, 0.9731, 0.9736, 0.9707, 0.9751, 0.9727, 0.9717,\n",
      "        0.9717, 0.9741, 0.9692, 0.9727, 0.9741, 0.9717, 0.9673, 0.9727, 0.9731,\n",
      "        0.9731, 0.9741, 0.9722, 0.9663, 0.9653, 0.9746, 0.9717, 0.9712, 0.9712,\n",
      "        0.9722, 0.9697, 0.9624, 0.9702, 0.9624, 0.9663, 0.9727, 0.9697, 0.9771,\n",
      "        0.9761, 0.9634, 0.9727, 0.9751, 0.9736, 0.9663, 0.9712, 0.9648, 0.9653,\n",
      "        0.9692, 0.9644, 0.9712, 0.9673, 0.9688, 0.9697, 0.9673, 0.9683, 0.9463,\n",
      "        0.9722, 0.9683, 0.9741, 0.9697, 0.9751, 0.9712], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:10<00:03,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9727, 0.9727, 0.9727, 0.9746, 0.9746, 0.9727, 0.9766, 0.9746, 0.9736,\n",
      "        0.9736, 0.9746, 0.9722, 0.9741, 0.9746, 0.9731, 0.9692, 0.9736, 0.9741,\n",
      "        0.9736, 0.9751, 0.9731, 0.9688, 0.9678, 0.9761, 0.9727, 0.9727, 0.9727,\n",
      "        0.9736, 0.9717, 0.9648, 0.9717, 0.9629, 0.9697, 0.9746, 0.9717, 0.9785,\n",
      "        0.9780, 0.9648, 0.9736, 0.9751, 0.9751, 0.9683, 0.9731, 0.9663, 0.9658,\n",
      "        0.9702, 0.9653, 0.9736, 0.9692, 0.9697, 0.9712, 0.9697, 0.9692, 0.9482,\n",
      "        0.9736, 0.9702, 0.9756, 0.9712, 0.9761, 0.9712], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:10<00:02,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9722, 0.9722, 0.9727, 0.9736, 0.9746, 0.9722, 0.9756, 0.9731, 0.9727,\n",
      "        0.9722, 0.9741, 0.9702, 0.9731, 0.9746, 0.9727, 0.9688, 0.9736, 0.9727,\n",
      "        0.9731, 0.9741, 0.9712, 0.9673, 0.9658, 0.9756, 0.9717, 0.9707, 0.9717,\n",
      "        0.9727, 0.9707, 0.9634, 0.9697, 0.9629, 0.9673, 0.9736, 0.9702, 0.9771,\n",
      "        0.9766, 0.9629, 0.9722, 0.9746, 0.9741, 0.9663, 0.9712, 0.9639, 0.9644,\n",
      "        0.9692, 0.9639, 0.9722, 0.9683, 0.9688, 0.9712, 0.9673, 0.9673, 0.9468,\n",
      "        0.9727, 0.9692, 0.9746, 0.9697, 0.9741, 0.9712], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:11<00:02,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9756, 0.9751, 0.9761, 0.9766, 0.9741, 0.9775, 0.9751, 0.9746,\n",
      "        0.9751, 0.9771, 0.9727, 0.9756, 0.9771, 0.9746, 0.9712, 0.9756, 0.9761,\n",
      "        0.9761, 0.9766, 0.9741, 0.9702, 0.9688, 0.9771, 0.9746, 0.9736, 0.9746,\n",
      "        0.9756, 0.9731, 0.9663, 0.9731, 0.9658, 0.9702, 0.9766, 0.9731, 0.9800,\n",
      "        0.9790, 0.9668, 0.9756, 0.9766, 0.9766, 0.9692, 0.9741, 0.9683, 0.9678,\n",
      "        0.9722, 0.9673, 0.9746, 0.9712, 0.9712, 0.9736, 0.9707, 0.9717, 0.9512,\n",
      "        0.9746, 0.9717, 0.9766, 0.9727, 0.9775, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:11<00:01,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9746, 0.9746, 0.9761, 0.9761, 0.9741, 0.9775, 0.9756, 0.9746,\n",
      "        0.9751, 0.9766, 0.9731, 0.9756, 0.9766, 0.9751, 0.9707, 0.9751, 0.9761,\n",
      "        0.9761, 0.9771, 0.9746, 0.9697, 0.9688, 0.9771, 0.9746, 0.9746, 0.9741,\n",
      "        0.9751, 0.9731, 0.9658, 0.9736, 0.9663, 0.9712, 0.9766, 0.9736, 0.9800,\n",
      "        0.9790, 0.9673, 0.9756, 0.9771, 0.9766, 0.9702, 0.9746, 0.9683, 0.9678,\n",
      "        0.9727, 0.9683, 0.9751, 0.9712, 0.9717, 0.9736, 0.9707, 0.9717, 0.9517,\n",
      "        0.9751, 0.9717, 0.9771, 0.9727, 0.9775, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:11<00:01,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9712, 0.9717, 0.9717, 0.9727, 0.9731, 0.9707, 0.9746, 0.9722, 0.9717,\n",
      "        0.9717, 0.9731, 0.9692, 0.9722, 0.9727, 0.9722, 0.9663, 0.9722, 0.9727,\n",
      "        0.9727, 0.9731, 0.9707, 0.9663, 0.9644, 0.9736, 0.9712, 0.9697, 0.9707,\n",
      "        0.9717, 0.9697, 0.9624, 0.9692, 0.9609, 0.9668, 0.9727, 0.9692, 0.9771,\n",
      "        0.9761, 0.9624, 0.9722, 0.9736, 0.9731, 0.9653, 0.9707, 0.9644, 0.9634,\n",
      "        0.9688, 0.9639, 0.9712, 0.9678, 0.9668, 0.9697, 0.9663, 0.9673, 0.9453,\n",
      "        0.9717, 0.9678, 0.9731, 0.9688, 0.9746, 0.9697], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:01,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9756, 0.9756, 0.9761, 0.9775, 0.9775, 0.9746, 0.9785, 0.9766, 0.9761,\n",
      "        0.9761, 0.9771, 0.9746, 0.9766, 0.9780, 0.9761, 0.9722, 0.9761, 0.9771,\n",
      "        0.9771, 0.9780, 0.9756, 0.9707, 0.9702, 0.9785, 0.9756, 0.9751, 0.9756,\n",
      "        0.9761, 0.9746, 0.9678, 0.9746, 0.9678, 0.9717, 0.9771, 0.9741, 0.9810,\n",
      "        0.9800, 0.9678, 0.9766, 0.9780, 0.9771, 0.9712, 0.9756, 0.9697, 0.9692,\n",
      "        0.9736, 0.9692, 0.9761, 0.9717, 0.9727, 0.9751, 0.9722, 0.9727, 0.9536,\n",
      "        0.9761, 0.9731, 0.9780, 0.9741, 0.9785, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:00,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9756, 0.9751, 0.9766, 0.9761, 0.9736, 0.9785, 0.9761, 0.9751,\n",
      "        0.9756, 0.9766, 0.9727, 0.9756, 0.9771, 0.9756, 0.9707, 0.9756, 0.9761,\n",
      "        0.9761, 0.9771, 0.9741, 0.9692, 0.9688, 0.9775, 0.9751, 0.9736, 0.9746,\n",
      "        0.9756, 0.9731, 0.9663, 0.9731, 0.9658, 0.9712, 0.9761, 0.9731, 0.9800,\n",
      "        0.9795, 0.9668, 0.9751, 0.9771, 0.9766, 0.9697, 0.9746, 0.9678, 0.9678,\n",
      "        0.9727, 0.9668, 0.9756, 0.9707, 0.9712, 0.9741, 0.9702, 0.9717, 0.9521,\n",
      "        0.9756, 0.9717, 0.9771, 0.9722, 0.9780, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:00,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9741, 0.9751, 0.9736, 0.9756, 0.9751, 0.9731, 0.9775, 0.9756, 0.9741,\n",
      "        0.9746, 0.9761, 0.9722, 0.9741, 0.9756, 0.9746, 0.9692, 0.9751, 0.9751,\n",
      "        0.9746, 0.9756, 0.9736, 0.9683, 0.9673, 0.9775, 0.9741, 0.9722, 0.9736,\n",
      "        0.9751, 0.9717, 0.9653, 0.9717, 0.9644, 0.9697, 0.9751, 0.9722, 0.9790,\n",
      "        0.9790, 0.9653, 0.9746, 0.9761, 0.9761, 0.9678, 0.9741, 0.9663, 0.9663,\n",
      "        0.9717, 0.9648, 0.9741, 0.9683, 0.9692, 0.9727, 0.9683, 0.9697, 0.9512,\n",
      "        0.9741, 0.9707, 0.9766, 0.9712, 0.9761, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9751, 0.9761, 0.9751, 0.9766, 0.9766, 0.9746, 0.9780, 0.9761, 0.9756,\n",
      "        0.9756, 0.9766, 0.9731, 0.9761, 0.9771, 0.9756, 0.9712, 0.9761, 0.9761,\n",
      "        0.9766, 0.9771, 0.9751, 0.9702, 0.9692, 0.9780, 0.9751, 0.9746, 0.9751,\n",
      "        0.9756, 0.9736, 0.9663, 0.9741, 0.9663, 0.9717, 0.9766, 0.9736, 0.9800,\n",
      "        0.9795, 0.9673, 0.9756, 0.9775, 0.9771, 0.9702, 0.9751, 0.9683, 0.9683,\n",
      "        0.9727, 0.9678, 0.9756, 0.9712, 0.9722, 0.9746, 0.9707, 0.9717, 0.9517,\n",
      "        0.9756, 0.9712, 0.9771, 0.9731, 0.9780, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "tensor([0.9746, 0.9751, 0.9741, 0.9761, 0.9761, 0.9736, 0.9780, 0.9756, 0.9746,\n",
      "        0.9751, 0.9766, 0.9731, 0.9756, 0.9766, 0.9751, 0.9707, 0.9756, 0.9761,\n",
      "        0.9761, 0.9766, 0.9746, 0.9692, 0.9688, 0.9771, 0.9746, 0.9741, 0.9751,\n",
      "        0.9746, 0.9731, 0.9663, 0.9736, 0.9663, 0.9712, 0.9761, 0.9731, 0.9795,\n",
      "        0.9790, 0.9668, 0.9756, 0.9775, 0.9761, 0.9697, 0.9746, 0.9688, 0.9678,\n",
      "        0.9727, 0.9678, 0.9751, 0.9702, 0.9717, 0.9736, 0.9702, 0.9712, 0.9521,\n",
      "        0.9746, 0.9712, 0.9771, 0.9727, 0.9775, 0.9731], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.50it/s]\n",
      "ðŸ§  Zero-shot evaluation on Novel Classes:   3%|â–Ž         | 1/29 [00:07<03:33,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9761, 0.9751, 0.9766, 0.9771, 0.9746, 0.9780, 0.9756, 0.9756,\n",
      "        0.9761, 0.9771, 0.9731, 0.9761, 0.9766, 0.9751, 0.9707, 0.9761, 0.9766,\n",
      "        0.9761, 0.9771, 0.9751, 0.9707, 0.9692, 0.9775, 0.9746, 0.9741, 0.9751,\n",
      "        0.9756, 0.9736, 0.9668, 0.9736, 0.9663, 0.9717, 0.9771, 0.9736, 0.9805,\n",
      "        0.9795, 0.9663, 0.9756, 0.9771, 0.9771, 0.9702, 0.9751, 0.9692, 0.9683,\n",
      "        0.9727, 0.9678, 0.9756, 0.9712, 0.9722, 0.9741, 0.9712, 0.9722, 0.9526,\n",
      "        0.9756, 0.9722, 0.9766, 0.9731, 0.9780, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:   7%|â–‹         | 2/29 [00:07<01:29,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9756, 0.9746, 0.9761, 0.9766, 0.9746, 0.9775, 0.9756, 0.9751,\n",
      "        0.9756, 0.9766, 0.9731, 0.9756, 0.9771, 0.9751, 0.9707, 0.9756, 0.9761,\n",
      "        0.9756, 0.9761, 0.9746, 0.9697, 0.9688, 0.9775, 0.9746, 0.9736, 0.9746,\n",
      "        0.9756, 0.9736, 0.9668, 0.9731, 0.9658, 0.9712, 0.9766, 0.9731, 0.9800,\n",
      "        0.9795, 0.9663, 0.9756, 0.9771, 0.9766, 0.9697, 0.9746, 0.9688, 0.9678,\n",
      "        0.9727, 0.9673, 0.9746, 0.9702, 0.9712, 0.9736, 0.9702, 0.9712, 0.9517,\n",
      "        0.9756, 0.9717, 0.9766, 0.9727, 0.9771, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  10%|â–ˆ         | 3/29 [00:08<00:50,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9761, 0.9756, 0.9766, 0.9771, 0.9746, 0.9785, 0.9761, 0.9756,\n",
      "        0.9756, 0.9775, 0.9746, 0.9761, 0.9775, 0.9756, 0.9717, 0.9756, 0.9766,\n",
      "        0.9766, 0.9771, 0.9751, 0.9707, 0.9697, 0.9775, 0.9751, 0.9746, 0.9751,\n",
      "        0.9761, 0.9736, 0.9673, 0.9736, 0.9673, 0.9717, 0.9771, 0.9741, 0.9810,\n",
      "        0.9800, 0.9683, 0.9761, 0.9771, 0.9771, 0.9707, 0.9751, 0.9692, 0.9688,\n",
      "        0.9731, 0.9688, 0.9756, 0.9717, 0.9717, 0.9746, 0.9717, 0.9727, 0.9531,\n",
      "        0.9756, 0.9727, 0.9775, 0.9731, 0.9780, 0.9746], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  14%|â–ˆâ–        | 4/29 [00:08<00:32,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9756, 0.9761, 0.9751, 0.9766, 0.9771, 0.9746, 0.9785, 0.9761, 0.9756,\n",
      "        0.9761, 0.9771, 0.9741, 0.9766, 0.9771, 0.9756, 0.9722, 0.9761, 0.9766,\n",
      "        0.9771, 0.9775, 0.9751, 0.9707, 0.9697, 0.9780, 0.9756, 0.9751, 0.9751,\n",
      "        0.9756, 0.9741, 0.9668, 0.9741, 0.9668, 0.9717, 0.9766, 0.9736, 0.9805,\n",
      "        0.9795, 0.9678, 0.9761, 0.9780, 0.9771, 0.9712, 0.9756, 0.9688, 0.9688,\n",
      "        0.9731, 0.9688, 0.9756, 0.9712, 0.9722, 0.9741, 0.9717, 0.9722, 0.9526,\n",
      "        0.9761, 0.9722, 0.9780, 0.9731, 0.9780, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  17%|â–ˆâ–‹        | 5/29 [00:08<00:22,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9756, 0.9761, 0.9756, 0.9766, 0.9766, 0.9741, 0.9785, 0.9761, 0.9756,\n",
      "        0.9756, 0.9771, 0.9736, 0.9756, 0.9775, 0.9756, 0.9717, 0.9761, 0.9766,\n",
      "        0.9766, 0.9775, 0.9746, 0.9707, 0.9692, 0.9775, 0.9751, 0.9741, 0.9751,\n",
      "        0.9761, 0.9736, 0.9668, 0.9736, 0.9658, 0.9707, 0.9766, 0.9736, 0.9805,\n",
      "        0.9795, 0.9673, 0.9756, 0.9771, 0.9771, 0.9697, 0.9751, 0.9683, 0.9688,\n",
      "        0.9727, 0.9678, 0.9756, 0.9717, 0.9717, 0.9741, 0.9712, 0.9722, 0.9521,\n",
      "        0.9756, 0.9717, 0.9771, 0.9727, 0.9785, 0.9746], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  21%|â–ˆâ–ˆ        | 6/29 [00:09<00:16,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9741, 0.9756, 0.9746, 0.9756, 0.9761, 0.9741, 0.9780, 0.9751, 0.9751,\n",
      "        0.9756, 0.9771, 0.9727, 0.9756, 0.9766, 0.9746, 0.9707, 0.9756, 0.9761,\n",
      "        0.9761, 0.9771, 0.9741, 0.9702, 0.9692, 0.9775, 0.9741, 0.9731, 0.9741,\n",
      "        0.9751, 0.9731, 0.9663, 0.9727, 0.9663, 0.9707, 0.9761, 0.9727, 0.9805,\n",
      "        0.9790, 0.9663, 0.9751, 0.9766, 0.9771, 0.9692, 0.9741, 0.9683, 0.9683,\n",
      "        0.9727, 0.9673, 0.9746, 0.9712, 0.9712, 0.9741, 0.9707, 0.9717, 0.9521,\n",
      "        0.9746, 0.9717, 0.9771, 0.9722, 0.9771, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  24%|â–ˆâ–ˆâ–       | 7/29 [00:09<00:12,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9741, 0.9756, 0.9741, 0.9761, 0.9756, 0.9731, 0.9780, 0.9756, 0.9751,\n",
      "        0.9751, 0.9766, 0.9722, 0.9751, 0.9766, 0.9751, 0.9702, 0.9756, 0.9761,\n",
      "        0.9761, 0.9766, 0.9741, 0.9692, 0.9678, 0.9771, 0.9751, 0.9736, 0.9741,\n",
      "        0.9751, 0.9731, 0.9658, 0.9727, 0.9658, 0.9707, 0.9756, 0.9727, 0.9800,\n",
      "        0.9790, 0.9663, 0.9751, 0.9761, 0.9761, 0.9697, 0.9741, 0.9668, 0.9678,\n",
      "        0.9727, 0.9663, 0.9751, 0.9702, 0.9707, 0.9741, 0.9707, 0.9717, 0.9521,\n",
      "        0.9751, 0.9712, 0.9771, 0.9722, 0.9775, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:09<00:10,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9751, 0.9756, 0.9756, 0.9771, 0.9771, 0.9751, 0.9790, 0.9766, 0.9756,\n",
      "        0.9761, 0.9775, 0.9746, 0.9766, 0.9771, 0.9756, 0.9717, 0.9761, 0.9771,\n",
      "        0.9761, 0.9775, 0.9756, 0.9712, 0.9697, 0.9780, 0.9751, 0.9746, 0.9751,\n",
      "        0.9761, 0.9741, 0.9668, 0.9741, 0.9668, 0.9722, 0.9771, 0.9741, 0.9805,\n",
      "        0.9800, 0.9678, 0.9766, 0.9775, 0.9771, 0.9707, 0.9756, 0.9697, 0.9692,\n",
      "        0.9731, 0.9683, 0.9761, 0.9717, 0.9722, 0.9741, 0.9712, 0.9727, 0.9526,\n",
      "        0.9761, 0.9727, 0.9775, 0.9741, 0.9780, 0.9746], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:10<00:08,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9751, 0.9746, 0.9756, 0.9761, 0.9736, 0.9780, 0.9756, 0.9751,\n",
      "        0.9751, 0.9766, 0.9731, 0.9756, 0.9771, 0.9746, 0.9712, 0.9751, 0.9761,\n",
      "        0.9761, 0.9766, 0.9746, 0.9697, 0.9688, 0.9771, 0.9746, 0.9736, 0.9741,\n",
      "        0.9751, 0.9727, 0.9668, 0.9722, 0.9663, 0.9707, 0.9761, 0.9731, 0.9800,\n",
      "        0.9790, 0.9668, 0.9751, 0.9766, 0.9761, 0.9697, 0.9751, 0.9683, 0.9673,\n",
      "        0.9727, 0.9673, 0.9746, 0.9702, 0.9707, 0.9741, 0.9712, 0.9712, 0.9526,\n",
      "        0.9746, 0.9717, 0.9771, 0.9722, 0.9771, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:10<00:07,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9741, 0.9746, 0.9741, 0.9756, 0.9756, 0.9731, 0.9775, 0.9751, 0.9746,\n",
      "        0.9736, 0.9761, 0.9731, 0.9746, 0.9761, 0.9746, 0.9707, 0.9746, 0.9756,\n",
      "        0.9751, 0.9761, 0.9736, 0.9692, 0.9683, 0.9761, 0.9741, 0.9736, 0.9736,\n",
      "        0.9751, 0.9727, 0.9658, 0.9727, 0.9648, 0.9697, 0.9756, 0.9727, 0.9800,\n",
      "        0.9785, 0.9663, 0.9751, 0.9756, 0.9766, 0.9692, 0.9736, 0.9683, 0.9678,\n",
      "        0.9717, 0.9668, 0.9741, 0.9702, 0.9712, 0.9727, 0.9702, 0.9712, 0.9512,\n",
      "        0.9741, 0.9712, 0.9766, 0.9722, 0.9771, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:10<00:06,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9736, 0.9746, 0.9736, 0.9751, 0.9751, 0.9731, 0.9766, 0.9741, 0.9736,\n",
      "        0.9736, 0.9761, 0.9727, 0.9741, 0.9761, 0.9741, 0.9702, 0.9741, 0.9751,\n",
      "        0.9746, 0.9751, 0.9731, 0.9688, 0.9673, 0.9761, 0.9736, 0.9727, 0.9731,\n",
      "        0.9746, 0.9722, 0.9653, 0.9727, 0.9639, 0.9692, 0.9751, 0.9722, 0.9790,\n",
      "        0.9780, 0.9653, 0.9746, 0.9756, 0.9761, 0.9683, 0.9731, 0.9673, 0.9673,\n",
      "        0.9712, 0.9658, 0.9736, 0.9692, 0.9702, 0.9722, 0.9697, 0.9702, 0.9507,\n",
      "        0.9736, 0.9712, 0.9761, 0.9712, 0.9766, 0.9731], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:10<00:05,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9741, 0.9756, 0.9741, 0.9761, 0.9766, 0.9741, 0.9780, 0.9756, 0.9751,\n",
      "        0.9756, 0.9766, 0.9727, 0.9756, 0.9766, 0.9751, 0.9707, 0.9756, 0.9761,\n",
      "        0.9761, 0.9771, 0.9751, 0.9697, 0.9683, 0.9771, 0.9746, 0.9741, 0.9746,\n",
      "        0.9756, 0.9736, 0.9668, 0.9731, 0.9663, 0.9707, 0.9766, 0.9731, 0.9800,\n",
      "        0.9790, 0.9658, 0.9756, 0.9766, 0.9766, 0.9697, 0.9741, 0.9683, 0.9678,\n",
      "        0.9727, 0.9673, 0.9751, 0.9707, 0.9712, 0.9741, 0.9712, 0.9722, 0.9517,\n",
      "        0.9751, 0.9722, 0.9771, 0.9727, 0.9780, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:11<00:05,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9751, 0.9756, 0.9751, 0.9766, 0.9771, 0.9746, 0.9785, 0.9761, 0.9756,\n",
      "        0.9756, 0.9771, 0.9736, 0.9766, 0.9771, 0.9756, 0.9717, 0.9756, 0.9766,\n",
      "        0.9766, 0.9775, 0.9756, 0.9702, 0.9697, 0.9775, 0.9751, 0.9751, 0.9746,\n",
      "        0.9756, 0.9741, 0.9673, 0.9741, 0.9673, 0.9717, 0.9771, 0.9736, 0.9805,\n",
      "        0.9795, 0.9678, 0.9761, 0.9775, 0.9775, 0.9697, 0.9751, 0.9692, 0.9688,\n",
      "        0.9731, 0.9688, 0.9756, 0.9722, 0.9722, 0.9741, 0.9717, 0.9717, 0.9531,\n",
      "        0.9756, 0.9727, 0.9775, 0.9736, 0.9780, 0.9746], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:04,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9717, 0.9727, 0.9717, 0.9736, 0.9741, 0.9712, 0.9761, 0.9731, 0.9727,\n",
      "        0.9722, 0.9741, 0.9707, 0.9727, 0.9746, 0.9722, 0.9673, 0.9731, 0.9736,\n",
      "        0.9731, 0.9741, 0.9727, 0.9663, 0.9658, 0.9751, 0.9717, 0.9707, 0.9717,\n",
      "        0.9727, 0.9697, 0.9634, 0.9702, 0.9634, 0.9678, 0.9741, 0.9702, 0.9775,\n",
      "        0.9766, 0.9629, 0.9727, 0.9741, 0.9741, 0.9658, 0.9717, 0.9653, 0.9644,\n",
      "        0.9697, 0.9644, 0.9722, 0.9678, 0.9683, 0.9707, 0.9683, 0.9683, 0.9502,\n",
      "        0.9717, 0.9697, 0.9751, 0.9697, 0.9746, 0.9717], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:04,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9727, 0.9741, 0.9731, 0.9751, 0.9751, 0.9722, 0.9771, 0.9741, 0.9736,\n",
      "        0.9731, 0.9756, 0.9717, 0.9736, 0.9756, 0.9731, 0.9688, 0.9741, 0.9746,\n",
      "        0.9741, 0.9751, 0.9736, 0.9678, 0.9673, 0.9761, 0.9731, 0.9722, 0.9727,\n",
      "        0.9736, 0.9707, 0.9648, 0.9712, 0.9648, 0.9688, 0.9751, 0.9717, 0.9785,\n",
      "        0.9780, 0.9644, 0.9736, 0.9751, 0.9751, 0.9673, 0.9727, 0.9668, 0.9658,\n",
      "        0.9712, 0.9658, 0.9736, 0.9692, 0.9692, 0.9717, 0.9692, 0.9697, 0.9512,\n",
      "        0.9731, 0.9712, 0.9761, 0.9707, 0.9756, 0.9727], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:04,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9727, 0.9731, 0.9727, 0.9741, 0.9741, 0.9717, 0.9761, 0.9736, 0.9731,\n",
      "        0.9727, 0.9751, 0.9717, 0.9736, 0.9746, 0.9731, 0.9692, 0.9731, 0.9741,\n",
      "        0.9741, 0.9746, 0.9727, 0.9683, 0.9668, 0.9756, 0.9727, 0.9727, 0.9722,\n",
      "        0.9736, 0.9712, 0.9648, 0.9722, 0.9634, 0.9683, 0.9746, 0.9712, 0.9790,\n",
      "        0.9775, 0.9644, 0.9741, 0.9751, 0.9756, 0.9678, 0.9731, 0.9668, 0.9668,\n",
      "        0.9707, 0.9658, 0.9731, 0.9692, 0.9692, 0.9717, 0.9688, 0.9697, 0.9497,\n",
      "        0.9727, 0.9697, 0.9756, 0.9707, 0.9761, 0.9722], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:03,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9756, 0.9751, 0.9766, 0.9771, 0.9741, 0.9785, 0.9761, 0.9756,\n",
      "        0.9751, 0.9775, 0.9741, 0.9761, 0.9771, 0.9751, 0.9717, 0.9756, 0.9766,\n",
      "        0.9766, 0.9771, 0.9751, 0.9707, 0.9702, 0.9775, 0.9756, 0.9746, 0.9746,\n",
      "        0.9756, 0.9736, 0.9668, 0.9736, 0.9668, 0.9707, 0.9766, 0.9741, 0.9805,\n",
      "        0.9795, 0.9683, 0.9766, 0.9771, 0.9771, 0.9707, 0.9746, 0.9692, 0.9683,\n",
      "        0.9727, 0.9688, 0.9756, 0.9717, 0.9722, 0.9746, 0.9712, 0.9722, 0.9526,\n",
      "        0.9756, 0.9722, 0.9775, 0.9731, 0.9780, 0.9746], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:12<00:03,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9736, 0.9741, 0.9736, 0.9751, 0.9756, 0.9731, 0.9775, 0.9746, 0.9736,\n",
      "        0.9741, 0.9756, 0.9731, 0.9746, 0.9756, 0.9746, 0.9702, 0.9746, 0.9756,\n",
      "        0.9746, 0.9761, 0.9736, 0.9688, 0.9683, 0.9756, 0.9736, 0.9731, 0.9736,\n",
      "        0.9746, 0.9727, 0.9653, 0.9722, 0.9648, 0.9702, 0.9756, 0.9727, 0.9795,\n",
      "        0.9785, 0.9663, 0.9746, 0.9756, 0.9761, 0.9692, 0.9736, 0.9683, 0.9678,\n",
      "        0.9717, 0.9673, 0.9741, 0.9707, 0.9707, 0.9722, 0.9697, 0.9707, 0.9497,\n",
      "        0.9736, 0.9707, 0.9766, 0.9717, 0.9771, 0.9727], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:03,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9751, 0.9746, 0.9766, 0.9766, 0.9746, 0.9780, 0.9761, 0.9756,\n",
      "        0.9751, 0.9775, 0.9736, 0.9756, 0.9771, 0.9751, 0.9702, 0.9761, 0.9766,\n",
      "        0.9756, 0.9771, 0.9746, 0.9702, 0.9692, 0.9775, 0.9746, 0.9736, 0.9746,\n",
      "        0.9756, 0.9731, 0.9668, 0.9736, 0.9668, 0.9712, 0.9771, 0.9736, 0.9800,\n",
      "        0.9795, 0.9673, 0.9756, 0.9771, 0.9766, 0.9697, 0.9746, 0.9688, 0.9683,\n",
      "        0.9727, 0.9683, 0.9751, 0.9717, 0.9717, 0.9741, 0.9707, 0.9717, 0.9526,\n",
      "        0.9756, 0.9722, 0.9771, 0.9727, 0.9775, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:13<00:02,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9756, 0.9756, 0.9756, 0.9771, 0.9771, 0.9751, 0.9795, 0.9766, 0.9761,\n",
      "        0.9761, 0.9775, 0.9751, 0.9766, 0.9775, 0.9761, 0.9722, 0.9766, 0.9771,\n",
      "        0.9766, 0.9775, 0.9751, 0.9707, 0.9702, 0.9780, 0.9756, 0.9746, 0.9751,\n",
      "        0.9766, 0.9746, 0.9678, 0.9741, 0.9673, 0.9722, 0.9771, 0.9746, 0.9810,\n",
      "        0.9805, 0.9688, 0.9761, 0.9771, 0.9780, 0.9707, 0.9756, 0.9702, 0.9697,\n",
      "        0.9736, 0.9688, 0.9761, 0.9727, 0.9722, 0.9751, 0.9722, 0.9727, 0.9531,\n",
      "        0.9756, 0.9727, 0.9780, 0.9741, 0.9780, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:13<00:02,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9741, 0.9756, 0.9741, 0.9761, 0.9761, 0.9736, 0.9785, 0.9756, 0.9751,\n",
      "        0.9751, 0.9771, 0.9731, 0.9756, 0.9771, 0.9751, 0.9712, 0.9756, 0.9761,\n",
      "        0.9761, 0.9771, 0.9756, 0.9702, 0.9688, 0.9775, 0.9751, 0.9746, 0.9746,\n",
      "        0.9751, 0.9736, 0.9668, 0.9736, 0.9668, 0.9707, 0.9766, 0.9736, 0.9800,\n",
      "        0.9790, 0.9678, 0.9761, 0.9775, 0.9766, 0.9697, 0.9746, 0.9683, 0.9683,\n",
      "        0.9731, 0.9678, 0.9751, 0.9702, 0.9717, 0.9741, 0.9712, 0.9717, 0.9526,\n",
      "        0.9746, 0.9717, 0.9775, 0.9727, 0.9780, 0.9736], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:13<00:02,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9751, 0.9746, 0.9761, 0.9766, 0.9741, 0.9780, 0.9751, 0.9756,\n",
      "        0.9746, 0.9771, 0.9736, 0.9751, 0.9766, 0.9751, 0.9712, 0.9756, 0.9761,\n",
      "        0.9761, 0.9771, 0.9746, 0.9702, 0.9688, 0.9771, 0.9746, 0.9741, 0.9746,\n",
      "        0.9756, 0.9741, 0.9663, 0.9736, 0.9663, 0.9702, 0.9766, 0.9731, 0.9800,\n",
      "        0.9790, 0.9678, 0.9761, 0.9771, 0.9766, 0.9702, 0.9746, 0.9688, 0.9688,\n",
      "        0.9727, 0.9683, 0.9746, 0.9712, 0.9722, 0.9741, 0.9707, 0.9722, 0.9521,\n",
      "        0.9751, 0.9722, 0.9771, 0.9727, 0.9780, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:14<00:01,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9731, 0.9731, 0.9727, 0.9746, 0.9741, 0.9722, 0.9766, 0.9741, 0.9731,\n",
      "        0.9741, 0.9751, 0.9712, 0.9736, 0.9746, 0.9731, 0.9688, 0.9741, 0.9741,\n",
      "        0.9746, 0.9751, 0.9727, 0.9673, 0.9663, 0.9756, 0.9731, 0.9722, 0.9722,\n",
      "        0.9731, 0.9722, 0.9648, 0.9712, 0.9644, 0.9688, 0.9741, 0.9712, 0.9785,\n",
      "        0.9780, 0.9639, 0.9746, 0.9751, 0.9746, 0.9678, 0.9727, 0.9663, 0.9663,\n",
      "        0.9717, 0.9648, 0.9741, 0.9688, 0.9688, 0.9722, 0.9683, 0.9692, 0.9497,\n",
      "        0.9731, 0.9702, 0.9756, 0.9707, 0.9761, 0.9712], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:14<00:01,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9741, 0.9751, 0.9741, 0.9756, 0.9756, 0.9731, 0.9771, 0.9756, 0.9746,\n",
      "        0.9746, 0.9761, 0.9727, 0.9746, 0.9761, 0.9746, 0.9702, 0.9751, 0.9756,\n",
      "        0.9751, 0.9766, 0.9741, 0.9692, 0.9678, 0.9761, 0.9741, 0.9731, 0.9731,\n",
      "        0.9746, 0.9727, 0.9648, 0.9722, 0.9648, 0.9702, 0.9756, 0.9722, 0.9795,\n",
      "        0.9785, 0.9663, 0.9751, 0.9761, 0.9756, 0.9692, 0.9741, 0.9683, 0.9668,\n",
      "        0.9717, 0.9663, 0.9741, 0.9697, 0.9707, 0.9736, 0.9697, 0.9712, 0.9507,\n",
      "        0.9746, 0.9712, 0.9766, 0.9722, 0.9775, 0.9731], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:14<00:01,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9751, 0.9746, 0.9761, 0.9766, 0.9746, 0.9775, 0.9761, 0.9756,\n",
      "        0.9756, 0.9766, 0.9731, 0.9761, 0.9771, 0.9751, 0.9707, 0.9756, 0.9756,\n",
      "        0.9766, 0.9766, 0.9746, 0.9697, 0.9683, 0.9780, 0.9746, 0.9741, 0.9746,\n",
      "        0.9756, 0.9731, 0.9658, 0.9727, 0.9663, 0.9707, 0.9761, 0.9731, 0.9800,\n",
      "        0.9795, 0.9668, 0.9756, 0.9771, 0.9766, 0.9697, 0.9746, 0.9678, 0.9668,\n",
      "        0.9722, 0.9673, 0.9751, 0.9702, 0.9712, 0.9736, 0.9702, 0.9707, 0.9517,\n",
      "        0.9751, 0.9717, 0.9775, 0.9727, 0.9771, 0.9741], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:15<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9756, 0.9751, 0.9766, 0.9766, 0.9746, 0.9785, 0.9761, 0.9761,\n",
      "        0.9751, 0.9771, 0.9736, 0.9756, 0.9771, 0.9751, 0.9707, 0.9761, 0.9766,\n",
      "        0.9766, 0.9771, 0.9751, 0.9707, 0.9692, 0.9771, 0.9751, 0.9741, 0.9746,\n",
      "        0.9756, 0.9736, 0.9663, 0.9731, 0.9673, 0.9712, 0.9766, 0.9741, 0.9800,\n",
      "        0.9795, 0.9678, 0.9761, 0.9766, 0.9761, 0.9707, 0.9746, 0.9683, 0.9678,\n",
      "        0.9727, 0.9683, 0.9756, 0.9712, 0.9717, 0.9746, 0.9707, 0.9722, 0.9526,\n",
      "        0.9756, 0.9722, 0.9775, 0.9731, 0.9780, 0.9746], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:15<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9741, 0.9741, 0.9741, 0.9761, 0.9761, 0.9736, 0.9775, 0.9746, 0.9746,\n",
      "        0.9751, 0.9761, 0.9727, 0.9751, 0.9761, 0.9746, 0.9697, 0.9746, 0.9756,\n",
      "        0.9756, 0.9766, 0.9741, 0.9692, 0.9683, 0.9766, 0.9741, 0.9736, 0.9736,\n",
      "        0.9751, 0.9731, 0.9653, 0.9727, 0.9658, 0.9702, 0.9756, 0.9727, 0.9795,\n",
      "        0.9785, 0.9668, 0.9756, 0.9761, 0.9756, 0.9692, 0.9741, 0.9683, 0.9673,\n",
      "        0.9722, 0.9678, 0.9746, 0.9702, 0.9707, 0.9731, 0.9697, 0.9707, 0.9512,\n",
      "        0.9746, 0.9712, 0.9766, 0.9722, 0.9771, 0.9731], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:15<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9727, 0.9736, 0.9731, 0.9741, 0.9746, 0.9717, 0.9766, 0.9736, 0.9736,\n",
      "        0.9731, 0.9751, 0.9717, 0.9736, 0.9751, 0.9727, 0.9688, 0.9741, 0.9736,\n",
      "        0.9746, 0.9751, 0.9731, 0.9678, 0.9668, 0.9756, 0.9731, 0.9712, 0.9722,\n",
      "        0.9736, 0.9707, 0.9648, 0.9712, 0.9639, 0.9688, 0.9746, 0.9712, 0.9780,\n",
      "        0.9775, 0.9653, 0.9741, 0.9756, 0.9746, 0.9673, 0.9727, 0.9663, 0.9658,\n",
      "        0.9712, 0.9653, 0.9731, 0.9692, 0.9688, 0.9722, 0.9692, 0.9692, 0.9497,\n",
      "        0.9731, 0.9697, 0.9756, 0.9707, 0.9756, 0.9717], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9717, 0.9736, 0.9722, 0.9731, 0.9736, 0.9717, 0.9756, 0.9727, 0.9731,\n",
      "        0.9731, 0.9741, 0.9702, 0.9731, 0.9746, 0.9727, 0.9683, 0.9731, 0.9736,\n",
      "        0.9736, 0.9741, 0.9717, 0.9673, 0.9653, 0.9751, 0.9717, 0.9707, 0.9722,\n",
      "        0.9731, 0.9702, 0.9634, 0.9697, 0.9629, 0.9678, 0.9741, 0.9707, 0.9780,\n",
      "        0.9771, 0.9639, 0.9731, 0.9736, 0.9741, 0.9663, 0.9722, 0.9648, 0.9644,\n",
      "        0.9707, 0.9639, 0.9722, 0.9673, 0.9678, 0.9712, 0.9683, 0.9673, 0.9492,\n",
      "        0.9722, 0.9697, 0.9751, 0.9692, 0.9751, 0.9717], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Novel Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Base classes accuracy: 72.34%\n",
      "ðŸ” Novel classes accuracy: 78.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # we don't want gradients\n",
    "\n",
    "\n",
    "def eval(\n",
    "        clip: clip.model.CLIP,\n",
    "        weighter: ModelWeightingModel ,\n",
    "        dataset: torch.utils.data.Subset[tuple[torch.Tensor, torch.Tensor]],\n",
    "        categories: list[int],\n",
    "        batch_size: int,\n",
    "        device: torch.device | str,\n",
    "        label = \"\"\n",
    "    ):\n",
    "    # let's set the model in evaluation mode\n",
    "    clip.eval()\n",
    "    weighter.eval()\n",
    "\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "    text_features = load_text_features(categories)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        image_features = clip.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # shape: [batch_size x num_prompts x num_classes]\n",
    "        scores: torch.Tensor = torch.matmul(image_features, text_features).permute(1,0,2)\n",
    "\n",
    "        # shape: [ batch_size x num_prompts]\n",
    "        weights: torch.Tensor = weighter(image_features.detach())\n",
    "        print(weights[0])\n",
    "        # reweighing scores\n",
    "        scores = (weights * scores.permute(2,0,1)).permute(1,2,0)\n",
    "\n",
    "        out = torch.sum(scores, dim=1)\n",
    "        predicted_class = out.argmax(dim=-1)\n",
    "        \n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "base_accuracy = eval(model, weighter, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"ðŸ§  Zero-shot evaluation on Base Classes\")\n",
    "novel_accuracy = eval(model, weighter, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"ðŸ§  Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "print(f\"ðŸ” Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "print(f\"ðŸ” Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baYfLKNdfbUR"
   },
   "source": [
    "## Harmonic Mean\n",
    "Few-Shot Adaptations papers usually report the Harmonic Mean.\n",
    "The harmonic mean tends to mitigate the impact of large outliers (base accuracy) and aggravate the impact of small ones (novel accuracy).\n",
    "Thus, achieving very high base accuracies at the expense of the novel accuracy will be penalized by the HM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1743597665969,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "rKAXR7hlfbUR",
    "outputId": "e00e50f4-3b0f-4e79-ed09-e8e82cc53668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Harmonic Mean: 74.99%\n"
     ]
    }
   ],
   "source": [
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    hm = numerator / denominator\n",
    "    return hm\n",
    "\n",
    "print(f\"ðŸ” Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
