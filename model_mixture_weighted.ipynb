{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYQiprE0pcVD"
   },
   "source": [
    "# CLIP Weighted Model Mixture - Few-Shot Learning Technique\n",
    "\n",
    "This notebook presents a novel strategy for few-shot adaptation using CLIP.\n",
    "\n",
    "Our approach involves the construction of multiple models whose outputs are combined through a weighted average, yielding an aggregated prediction generally more accurate than any single model.\n",
    "\n",
    "This method is characterized by the following aspects:\n",
    "1. The models are not trained; instead, they serve as prompts fed into the CLIP text encoder.\n",
    "2. Prompts are manually designed based on domain-specific knowledge.\n",
    "3. A weighted average is computed, where the weights are predicted by a _Re-Weighting Model_—a simple neural network that takes CLIP image embeddings as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QtqdSOr8qqOn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai_clip in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: ftfy in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (6.3.1)\n",
      "Requirement already satisfied: regex in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from openai_clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from ftfy->openai_clip) (0.2.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->openai_clip) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai_clip\n",
    "\n",
    "from torchvision.datasets import Flowers102\n",
    "import random\n",
    "import torch\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from typing import TypeVar, cast, TypedDict, Generic\n",
    "import random\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from typing import Callable\n",
    "from torch import nn\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import clip.model\n",
    "from functools import lru_cache\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "Configurable parameters of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to use the default train-test spit provided by torchvision or not\n",
    "# > *If set to True*: we will have the default split where each training class has 10 samples\n",
    "# > *if set to False* the number of shot will be defined by the `NUMBER_OF_SHOTS` constants\n",
    "#   this is done to make it easier to compare our algorithm with other papers, that often uses 16 as the default number of shots\n",
    "USE_DEFAULT_SPLIT = False\n",
    "# The number of shot to use in learning. Only has effect if `USE_DEFAULT_SPLIT` is set to False\n",
    "NUMBER_OF_SHOTS = 16\n",
    "\n",
    "# the device to use for training\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# underlying model to use for the clip visual encoder\n",
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "CLIP_MODEL=\"ViT-B/16\"\n",
    "\n",
    "# The noise to add on top of the image embedding.\n",
    "# We noted that adding noise increases the accuracy of novel and based classes when evaluated separately\n",
    "# at minor accuracy cost when evaluating base and novel classes together.\n",
    "NOISE: float | Literal[False] = 0.2\n",
    "\n",
    "\n",
    "# The batch size used.\n",
    "# We noted high batch size are better for training, however due to memory constraint\n",
    "# we used gradient accumulation every BATCH_SIZE_MULTIPLIER batch to simulate larger sizes\n",
    "BATCH_SIZE: int = 32\n",
    "BATCH_SIZE_MULTIPLIER: int = 4\n",
    "\n",
    "# Number of training steps\n",
    "NUM_STEPS = 100\n",
    "\n",
    "# The parameters for the optimizer\n",
    "LR: float = 5\n",
    "MOMENTUM: float = 0.5\n",
    "SCHEDULER_STEP: int = 50\n",
    "GAMMA: float = 0.9\n",
    "CLIP_GRADIENT_VALUE = 5\n",
    "\n",
    "# Number of neurons in the re-weighter's intermediate layer\n",
    "RE_WEIGHTER_L1_SIZE: int = 75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2353MHw1p24h"
   },
   "source": [
    "### Type definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryLabel(TypedDict):\n",
    "    id: int\n",
    "    name: str\n",
    "    novel: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading\n",
    "\n",
    "Functions for dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M_1CrUhZpVCq"
   },
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"\n",
    "    Load Flowers102 train, validation and test sets.\n",
    "    Uses the default split provided by torchvision.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return (\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], train),\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], val),\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], test)\n",
    "    )\n",
    "\n",
    "def get_data_custom_split(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"\n",
    "    Load Flowers102 train, validation and test sets.\n",
    "    Uses a custom split that allow to specify the number of items per class that will be assigned to the train set.\n",
    "    The validation set will always be empty.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    a,b,c = get_data(data_dir, transform)\n",
    "    full_dataset: Dataset[tuple[torch.Tensor, int]] = torch.utils.data.ConcatDataset([a,b,c])\n",
    "\n",
    "    labels_set = set(l for _,l in full_dataset)\n",
    "    class_to_index_dict: dict[int, list[int]] = {l: [] for l in labels_set}\n",
    "\n",
    "    for i in range(len(full_dataset)):\n",
    "        l = full_dataset[i][1]\n",
    "        class_to_index_dict[l].append(i)\n",
    "\n",
    "    train: list[int] = []\n",
    "    test: list[int] = []\n",
    "\n",
    "    for indexes in class_to_index_dict.values():\n",
    "        random.shuffle(indexes)\n",
    "        train += indexes[0:NUMBER_OF_SHOTS]\n",
    "        test += indexes[NUMBER_OF_SHOTS:]\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train)\n",
    "    validation_dataset = torch.utils.data.Subset(full_dataset, [])\n",
    "    test_dataset = torch.utils.data.Subset(full_dataset, test)\n",
    "    return (\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], train_dataset),\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], validation_dataset),\n",
    "        cast(Dataset[tuple[torch.Tensor,int]], test_dataset)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJI_a5EizA5a"
   },
   "source": [
    "## Base and novel categories\n",
    "Function definition to split the dataset into novel and base classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nfq51vd8q_5a"
   },
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "E = TypeVar(\"E\")\n",
    "def base_novel_categories(dataset: Dataset[tuple[T,E]]):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(l for _, l in dataset)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8puO1VNpzwvi"
   },
   "source": [
    "## Split dataset\n",
    "The dataset is partitioned into base and novel categories based on the `base_novel_categories` definition. This operation requires both the dataset and the list of base classes. Each sample is assigned to the base set if its label belongs to the base categories; otherwise, it is assigned to the novel set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "msOszMs2zRRu"
   },
   "outputs": [],
   "source": [
    "\n",
    "T = TypeVar(\"T\")\n",
    "E = TypeVar(\"E\")\n",
    "def split_data(dataset: Dataset[tuple[T,E]], base_classes: list[int]):\n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples: list[int] = []\n",
    "    novel_categories_samples: list[int] = []\n",
    "\n",
    "    # set with the base classes (so that checking existence is O(1))\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    label: int\n",
    "    for sample_id, (_, label) in enumerate(dataset): #type: ignore\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    base_dataset = Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KpbPRLr7WL_"
   },
   "source": [
    "## Load CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5162,
     "status": "ok",
     "timestamp": 1743597617669,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "Sh6uLZRT7YJx",
    "outputId": "25ef91c9-9879-4f50-d1eb-d2c53c194498"
   },
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(CLIP_MODEL, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the prompt templates\n",
    "\n",
    "A template is a function that takes as input a class and returns a description of the underlying object.\n",
    "\n",
    "They are divided into two categories:  \n",
    "1. **General prompts**: generic prompts that can be applied to any class.  \n",
    "2. **Class-specific prompts**: category-dependent prompts.\n",
    "\n",
    "Prompt-class mappings are computed by the re-weighter model, hence the absence of explicit code for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####################################################\n",
    "################# GENERAL PROMPTS ###################\n",
    "#####################################################\n",
    "general_prompt_template: list[Callable[[str], str]] = [\n",
    "    lambda x: f\"a photo of a {x}, a type of flower.\",\n",
    "    lambda x: f\"a photo of some {x}, a type of flower.\",\n",
    "    lambda x: f\"a close-up of a {x} flower.\",\n",
    "    lambda x: f\"an image of a {x} blossom.\",\n",
    "    lambda x: f\"a beautiful {x} in bloom.\",\n",
    "    lambda x: f\"a bunch of {x} flowers.\",\n",
    "    lambda x: f\"a macro shot of a {x} flower.\",\n",
    "    lambda x: f\"a single {x} flower.\",\n",
    "    lambda x: f\"fresh {x} flowers in a garden.\",\n",
    "]\n",
    "\n",
    "#####################################################\n",
    "############## CLASS SPECIFIC PROMPTS ###############\n",
    "#####################################################\n",
    "\n",
    "class_specific_prompt_templates: list[Callable[[str], str]] = [\n",
    "    # pink primrose\n",
    "    lambda x: f\"a photo of a {x}, a delicate flower with soft pink petals.\",\n",
    "    lambda x: f\"an image of a {x}, a blooming plant often found in springtime gardens.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its pale pink blossoms and gentle appearance.\",\n",
    "\n",
    "    # hard-leaved pocket orchid\n",
    "    lambda x: f\"a photo of a {x}, a tropical orchid with stiff, glossy leaves.\",\n",
    "    lambda x: f\"an image of a {x}, an exotic flower with waxy petals and leathery foliage.\",\n",
    "    lambda x: f\"a botanical image of a {x}, an orchid species with hard, durable leaves.\",\n",
    "\n",
    "    # canterbury bells\n",
    "    lambda x: f\"a photo of a {x}, a bell-shaped flower in shades of purple and blue.\",\n",
    "    lambda x: f\"an image of a {x}, known for its tall spikes of bell-like blossoms.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cottage garden flower with cup-shaped blooms.\",\n",
    "\n",
    "    # sweet pea\n",
    "    lambda x: f\"a photo of a {x}, a fragrant flower with delicate, ruffled petals.\",\n",
    "    lambda x: f\"an image of a {x}, often grown for its pastel colors and pleasant scent.\",\n",
    "    lambda x: f\"a close-up of a {x}, a climbing plant with butterfly-shaped flowers.\",\n",
    "\n",
    "    # english marigold\n",
    "    lambda x: f\"a photo of a {x}, a bright orange or yellow flower with daisy-like blooms.\",\n",
    "    lambda x: f\"an image of a {x}, known for its healing properties and sunny appearance.\",\n",
    "    lambda x: f\"a close-up of a {x}, a calendula flower common in herb gardens.\",\n",
    "\n",
    "    # tiger lily\n",
    "    lambda x: f\"a photo of a {x}, an orange flower with dark spots and recurved petals.\",\n",
    "    lambda x: f\"an image of a {x}, a wild-looking lily with dramatic coloring.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its bold, tiger-striped blooms.\",\n",
    "\n",
    "    # moon orchid\n",
    "    lambda x: f\"a photo of a {x}, an elegant white orchid with a moon-like glow.\",\n",
    "    lambda x: f\"an image of a {x}, a phalaenopsis flower often found in tropical climates.\",\n",
    "    lambda x: f\"a close-up of a {x}, a soft and symmetrical flower with wide petals.\",\n",
    "\n",
    "    # bird of paradise\n",
    "    lambda x: f\"a photo of a {x}, a tropical flower resembling a colorful bird.\",\n",
    "    lambda x: f\"an image of a {x}, known for its bright orange and blue petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, an exotic bloom that looks like a flying bird.\",\n",
    "\n",
    "    # monkshood\n",
    "    lambda x: f\"a photo of a {x}, a hooded purple flower with toxic properties.\",\n",
    "    lambda x: f\"an image of a {x}, often called wolfsbane, with dark violet petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a tall plant with helmet-shaped blooms.\",\n",
    "\n",
    "    # globe thistle\n",
    "    lambda x: f\"a photo of a {x}, a spherical flower with spiky blue petals.\",\n",
    "    lambda x: f\"an image of a {x}, known for its round shape and thistle-like texture.\",\n",
    "    lambda x: f\"a close-up of a {x}, a unique ornamental flower with a metallic hue.\",\n",
    "\n",
    "    # snapdragon\n",
    "    lambda x: f\"a photo of a {x}, a colorful flower that resembles a dragon's mouth.\",\n",
    "    lambda x: f\"an image of a {x}, known for its vertical clusters of blooming petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a common garden flower with hinged, snout-like blooms.\",\n",
    "\n",
    "    # colt's foot\n",
    "    lambda x: f\"a photo of a {x}, a yellow wildflower that appears before its leaves.\",\n",
    "    lambda x: f\"an image of a {x}, a small flower resembling a dandelion in early spring.\",\n",
    "    lambda x: f\"a close-up of a {x}, a plant with hoof-shaped leaves and bright yellow blooms.\",\n",
    "\n",
    "    # king protea\n",
    "    lambda x: f\"a photo of a {x}, a large flower with a spiky crown-like appearance.\",\n",
    "    lambda x: f\"an image of a {x}, a South African bloom with a central cone and pink petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a striking flower often used in bold arrangements.\",\n",
    "\n",
    "    # spear thistle\n",
    "    lambda x: f\"a photo of a {x}, a spiny plant with purple tufted blooms.\",\n",
    "    lambda x: f\"an image of a {x}, known for its sharp leaves and thistle head.\",\n",
    "    lambda x: f\"a close-up of a {x}, a wildflower with prickly stems and a vibrant purple flower.\",\n",
    "\n",
    "    # yellow iris\n",
    "    lambda x: f\"a photo of a {x}, a bright yellow iris with upright petals.\",\n",
    "    lambda x: f\"an image of a {x}, commonly found near water, with sword-shaped leaves.\",\n",
    "    lambda x: f\"a close-up of a {x}, an elegant flower with golden hues and frilled edges.\",\n",
    "\n",
    "    # globe-flower\n",
    "    lambda x: f\"a photo of a {x}, a round yellow bloom resembling a buttercup.\",\n",
    "    lambda x: f\"an image of a {x}, a spherical flower found in alpine meadows.\",\n",
    "    lambda x: f\"a close-up of a {x}, a glowing, globe-shaped flower with dense petals.\",\n",
    "\n",
    "    # purple coneflower\n",
    "    lambda x: f\"a photo of a {x}, a daisy-like flower with purple petals and a spiky cone.\",\n",
    "    lambda x: f\"an image of a {x}, often used in herbal remedies and garden borders.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its downward-sloping petals and orange center.\",\n",
    "\n",
    "    # peruvian lily\n",
    "    lambda x: f\"a photo of a {x}, a spotted flower with multiple colorful petals.\",\n",
    "    lambda x: f\"an image of a {x}, a long-lasting bloom used in cut flower arrangements.\",\n",
    "    lambda x: f\"a close-up of a {x}, a lily-like flower with striped inner petals.\",\n",
    "\n",
    "    # balloon flower\n",
    "    lambda x: f\"a photo of a {x}, a flower bud that inflates like a balloon before opening.\",\n",
    "    lambda x: f\"an image of a {x}, a star-shaped bloom in shades of blue or purple.\",\n",
    "    lambda x: f\"a close-up of a {x}, a unique flower with puffy unopened buds.\",\n",
    "\n",
    "    # giant white arum lily\n",
    "    lambda x: f\"a photo of a {x}, a large white flower with a trumpet-like shape.\",\n",
    "    lambda x: f\"an image of a {x}, also known as a calla lily, with a central yellow spadix.\",\n",
    "    lambda x: f\"a close-up of a {x}, an elegant flower with smooth white petals.\",\n",
    "\n",
    "    # fire lily\n",
    "    lambda x: f\"a photo of a {x}, a bright red or orange lily with curled petals.\",\n",
    "    lambda x: f\"an image of a {x}, a dramatic flower known for its flame-like appearance.\",\n",
    "    lambda x: f\"a close-up of a {x}, a fiery-looking lily with backward-bending petals.\",\n",
    "\n",
    "    # pincushion flower\n",
    "    lambda x: f\"a photo of a {x}, a flower with a domed center and delicate fringe.\",\n",
    "    lambda x: f\"an image of a {x}, often purple or lavender, resembling a pin-filled cushion.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its central disk and lace-like petals.\",\n",
    "\n",
    "    # fritillary\n",
    "    lambda x: f\"a photo of a {x}, a checkered bell-shaped flower often in purple tones.\",\n",
    "    lambda x: f\"an image of a {x}, a rare flower with a distinctive petal pattern.\",\n",
    "    lambda x: f\"a close-up of a {x}, a delicate wildflower with hanging, nodding blooms.\",\n",
    "\n",
    "    # red ginger\n",
    "    lambda x: f\"a photo of a {x}, a tropical flower with bright red bracts.\",\n",
    "    lambda x: f\"an image of a {x}, known for its bold color and upright flower spikes.\",\n",
    "    lambda x: f\"a close-up of a {x}, a striking plant native to rainforests.\",\n",
    "\n",
    "    # grape hyacinth\n",
    "    lambda x: f\"a photo of a {x}, a small bulbous plant with clusters of blue-purple flowers.\",\n",
    "    lambda x: f\"an image of a {x}, resembling tiny grapes arranged on a spike.\",\n",
    "    lambda x: f\"a close-up of a {x}, a spring flower with densely packed florets.\",\n",
    "\n",
    "    # corn poppy\n",
    "    lambda x: f\"a photo of a {x}, a bright red flower with papery petals and a dark center.\",\n",
    "    lambda x: f\"an image of a {x}, a wild poppy often found in meadows and fields.\",\n",
    "    lambda x: f\"a close-up of a {x}, a flower symbolizing remembrance and resilience.\",\n",
    "\n",
    "    # prince of wales feathers\n",
    "    lambda x: f\"a photo of a {x}, a spiky flower head resembling a feathery plume.\",\n",
    "    lambda x: f\"an image of a {x}, known for its upright purple-pink floral spikes.\",\n",
    "    lambda x: f\"a close-up of a {x}, a member of the amaranth family with plume-like blooms.\",\n",
    "\n",
    "    # stemless gentian\n",
    "    lambda x: f\"a photo of a {x}, a vivid blue flower growing close to the ground.\",\n",
    "    lambda x: f\"an image of a {x}, a low-growing gentian with trumpet-shaped petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a mountain flower with intensely blue blossoms.\",\n",
    "\n",
    "    # artichoke\n",
    "    lambda x: f\"a photo of a {x}, a thistle-like plant with edible buds and purple flowers.\",\n",
    "    lambda x: f\"an image of a {x}, a spiky flower head that blooms into a vibrant violet.\",\n",
    "    lambda x: f\"a close-up of a {x}, a large budding flower with a layered appearance.\",\n",
    "\n",
    "    # sweet william\n",
    "    lambda x: f\"a photo of a {x}, a cluster of small flowers in pink, red, or white.\",\n",
    "    lambda x: f\"an image of a {x}, a garden flower known for its fringed petal edges.\",\n",
    "    lambda x: f\"a close-up of a {x}, a fragrant bloom often used in cottage gardens.\",\n",
    "\n",
    "    # carnation\n",
    "    lambda x: f\"a photo of a {x}, a ruffled flower commonly seen in bouquets.\",\n",
    "    lambda x: f\"an image of a {x}, a traditional bloom with a clove-like scent.\",\n",
    "    lambda x: f\"a close-up of a {x}, known for its layered petals and vibrant color range.\",\n",
    "\n",
    "    # garden phlox\n",
    "    lambda x: f\"a photo of a {x}, a tall plant with clusters of pink or purple blooms.\",\n",
    "    lambda x: f\"an image of a {x}, a perennial flower found in cottage-style gardens.\",\n",
    "    lambda x: f\"a close-up of a {x}, a phlox with star-shaped petals growing in dense bunches.\",\n",
    "\n",
    "    # love in the mist\n",
    "    lambda x: f\"a photo of a {x}, a blue flower surrounded by fine, feathery foliage.\",\n",
    "    lambda x: f\"an image of a {x}, a delicate flower with a misty, lace-like background.\",\n",
    "    lambda x: f\"a close-up of a {x}, a whimsical bloom with soft, threadlike leaves.\",\n",
    "\n",
    "    # mexican aster\n",
    "    lambda x: f\"a photo of a {x}, a daisy-like flower with bright pink or purple petals.\",\n",
    "    lambda x: f\"an image of a {x}, a tall annual bloom often seen in wildflower fields.\",\n",
    "    lambda x: f\"a close-up of a {x}, a lightweight flower with yellow centers and soft petals.\",\n",
    "\n",
    "    # alpine sea holly\n",
    "    lambda x: f\"a photo of a {x}, a spiky blue flower with thistle-like bracts.\",\n",
    "    lambda x: f\"an image of a {x}, a unique alpine plant with metallic-colored petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a flower with a cone center and pointed star-shaped sepals.\",\n",
    "\n",
    "    # ruby-lipped cattleya\n",
    "    lambda x: f\"a photo of a {x}, an orchid with bold purple lips and pastel petals.\",\n",
    "    lambda x: f\"an image of a {x}, a showy flower with ruby-colored accents on its lip.\",\n",
    "    lambda x: f\"a close-up of a {x}, a fragrant orchid with elaborate ruffled petals.\",\n",
    "\n",
    "    # cape flower\n",
    "    lambda x: f\"a photo of a {x}, a brightly colored South African flower with daisy form.\",\n",
    "    lambda x: f\"an image of a {x}, known for its vivid hues and sun-tracking behavior.\",\n",
    "    lambda x: f\"a close-up of a {x}, a vibrant flower with a dark center and radiating petals.\",\n",
    "\n",
    "    # great masterwort\n",
    "    lambda x: f\"a photo of a {x}, a flower with a central cluster surrounded by papery bracts.\",\n",
    "    lambda x: f\"an image of a {x}, a perennial bloom with intricate star-like heads.\",\n",
    "    lambda x: f\"a close-up of a {x}, a soft-colored flower with detailed florets in the center.\",\n",
    "\n",
    "    # siam tulip\n",
    "    lambda x: f\"a photo of a {x}, a tropical flower with pink petals and green bracts.\",\n",
    "    lambda x: f\"an image of a {x}, also known as Curcuma, with cone-shaped blossoms.\",\n",
    "    lambda x: f\"a close-up of a {x}, a vibrant Thai flower with tulip-like structure.\",\n",
    "\n",
    "    # lenten rose\n",
    "    lambda x: f\"a photo of a {x}, a spring flower with nodding blooms and muted colors.\",\n",
    "    lambda x: f\"an image of a {x}, a hellebore plant with leathery leaves and soft petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cold-hardy flower blooming in late winter to early spring.\",\n",
    "\n",
    "    # barbeton daisy\n",
    "    lambda x: f\"a photo of a {x}, a brightly colored daisy native to South Africa.\",\n",
    "    lambda x: f\"an image of a {x}, known for its vivid red, orange, or pink petals.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cheerful flower with a prominent central disk.\",\n",
    "\n",
    "    # daffodil\n",
    "    lambda x: f\"a photo of a {x}, a trumpet-shaped flower with yellow or white petals.\",\n",
    "    lambda x: f\"an image of a {x}, one of the first blooms of spring with a central corona.\",\n",
    "    lambda x: f\"a close-up of a {x}, a classic bulb flower symbolizing renewal and hope.\",\n",
    "\n",
    "    # sword lily\n",
    "    lambda x: f\"a photo of a {x}, a tall flower with sword-like leaves and vertical blooms.\",\n",
    "    lambda x: f\"an image of a {x}, commonly known as gladiolus with stacked florets.\",\n",
    "    lambda x: f\"a close-up of a {x}, a showy flower in a rainbow of colors on long spikes.\",\n",
    "\n",
    "    # poinsettia\n",
    "    lambda x: f\"a photo of a {x}, a festive plant with red or white leaf-like bracts.\",\n",
    "    lambda x: f\"an image of a {x}, often used in winter displays with green foliage and colorful tops.\",\n",
    "    lambda x: f\"a close-up of a {x}, a holiday flower with bright petal-like leaves.\",\n",
    "\n",
    "    # bolero deep blue\n",
    "    lambda x: f\"a photo of a {x}, a compact flower with deep violet petals and ruffled texture.\",\n",
    "    lambda x: f\"an image of a {x}, a variety of pansy known for its rich blue coloring.\",\n",
    "    lambda x: f\"a close-up of a {x}, a velvety flower with intricate patterns and deep hues.\",\n",
    "\n",
    "    # wallflower\n",
    "    lambda x: f\"a photo of a {x}, a small clustered flower known for growing on walls or rocky soil.\",\n",
    "    lambda x: f\"an image of a {x}, a plant with fragrant blooms in yellow, orange, or red.\",\n",
    "    lambda x: f\"a close-up of a {x}, a simple flower with four-petal blossoms and warm colors.\",\n",
    "\n",
    "    # marigold\n",
    "    lambda x: f\"a photo of a {x}, a vibrant flower with layers of orange or yellow petals.\",\n",
    "    lambda x: f\"an image of a {x}, known for its strong scent and decorative garden use.\",\n",
    "    lambda x: f\"a close-up of a {x}, a sun-loving flower with round, bushy blooms.\",\n",
    "\n",
    "    # buttercup\n",
    "    lambda x: f\"a photo of a {x}, a shiny yellow flower with cup-shaped petals.\",\n",
    "    lambda x: f\"an image of a {x}, a wildflower with a simple structure and glossy surface.\",\n",
    "    lambda x: f\"a close-up of a {x}, a cheerful bloom with golden overlapping petals.\",\n",
    "\n",
    "    # oxeye daisy\n",
    "    lambda x: f\"a photo of a {x}, a white-petaled daisy with a yellow central disk.\",\n",
    "    lambda x: f\"an image of a {x}, a classic meadow flower with a tall, slender stem.\",\n",
    "    lambda x: f\"a close-up of a {x}, a widespread wildflower resembling a common daisy.\",\n",
    "\n",
    "    # common dandelion\n",
    "    lambda x: f\"a photo of a {x}, a yellow flower with toothed leaves and fluffy seed heads.\",\n",
    "    lambda x: f\"an image of a {x}, a weedy plant known for its puffball seed dispersal.\",\n",
    "    lambda x: f\"a close-up of a {x}, a golden flower head made of many tiny florets.\",\n",
    "\n",
    "    # petunia\n",
    "    lambda x: f\"a photo of a {x}, a funnel-shaped flower often used in hanging baskets.\",\n",
    "    lambda x: f\"an image of a {x}, a colorful bloom available in many vibrant shades.\",\n",
    "    lambda x: f\"a close-up of a {x}, a soft, velvety flower with a wide trumpet-like shape.\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text feature loading function\n",
    "\n",
    "CLIP's text processing pipeline operates as follows:\n",
    " - A matrix of strings is generated with shape `number of prompts × number of classes`.\n",
    " - The matrix is tokenized.\n",
    " - The tokenized prompts are passed through the text encoder to obtain the embeddings.\n",
    "\n",
    "The resulting output is a matrix of shape `number of prompts × embedding size × number of classes`.\n",
    "\n",
    "#### Difference between base and novel class embeddings\n",
    "\n",
    "When generating prompts for novel classes, general prompts are repeated to match the required vector size.\n",
    "\n",
    "Initially, a padding prompt was used. However, the re-weighter learned to assign, on average, higher weights to the padding class to compensate for this design.\n",
    "\n",
    "It was later observed that replacing padding prompts with well-crafted prompts led to improved accuracy. We hypothesize that this improvement arises from reducing constraints on the re-weighter, thereby facilitating the learning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def load_text_features(categories: list[CategoryLabel]):\n",
    "    \"\"\"\n",
    "    return size: num_prompts x num_categories x input_size\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # prompts for base classes (include both general and class specific templates)\n",
    "    prompts_for_base_classes = general_prompt_template + class_specific_prompt_templates\n",
    "    \n",
    "    # prompts for novel classes (are obtained with the repetition of general templates)\n",
    "    multiplier = len(prompts_for_base_classes) // len(general_prompt_template) + 1\n",
    "    prompts_for_novel_classes = general_prompt_template * multiplier\n",
    "    prompts_for_novel_classes = prompts_for_novel_classes[0:len(prompts_for_base_classes)]\n",
    "\n",
    "    def get_prompts_for_one_class(category: CategoryLabel) -> list[str]:\n",
    "        if category[\"novel\"]:\n",
    "            prompts = prompts_for_novel_classes\n",
    "        else:\n",
    "            prompts = prompts_for_base_classes\n",
    "        name = category[\"name\"]\n",
    "        return [t(name) for t in prompts ]\n",
    "    \n",
    "\n",
    "    text_inputs = [\n",
    "        clip.tokenize(\n",
    "            get_prompts_for_one_class(category)\n",
    "        ).to(DEVICE)\n",
    "        for category in categories\n",
    "    ]\n",
    "\n",
    "    text_features_array: list[torch.Tensor] = [\n",
    "        model.encode_text(x)\n",
    "        for x in text_inputs\n",
    "    ]\n",
    "\n",
    "    # shape: num_classes x num_prompts x embedding_size\n",
    "    text_features = torch.stack([\n",
    "        x/x.norm(dim=-1,keepdim=True)\n",
    "        for x in text_features_array\n",
    "    ])\n",
    "\n",
    "    # shape: num_prompts x embedding_size x num_classes\n",
    "    text_features = text_features.permute(1,2,0)\n",
    "\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TVrYUYTv9ttM"
   },
   "outputs": [],
   "source": [
    "if USE_DEFAULT_SPLIT:\n",
    "    train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "else:\n",
    "    train_set, val_set, test_set = get_data_custom_split(transform=preprocess)\n",
    "\n",
    "# split classes into base and novel\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "# split the three datasets\n",
    "train_base, _ = split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-weighter model\n",
    "\n",
    "A lightweight feedforward neural network (FFNN) designed as follows:  \n",
    "- **Input**: a 2D tensor representing the visual embedding of an image (i.e., the output of the visual encoder).  \n",
    "- **Output**: a 3D tensor of shape `batch size × number of prompts × number of classes`.\n",
    "\n",
    "The output is used to re-weight the prompts and compute a single classification score.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "Notation:  \n",
    "- $T_{ij}$: the embedding of class `j` generated using prompt template `i`.  \n",
    "- $V$: the embedding of the image to classify.  \n",
    "- $W = \\text{Reweighter}(V)$: the output of the re-weighter, shaped `(number of prompts × number of classes)`.  \n",
    "- $w_{ij}$: the weight assigned to template `i` and class `j`, used to classify the image associated with $V$.  \n",
    "- $s_j$: the weighted score for class `j`.\n",
    "\n",
    "Using this notation, the classification score $s_j$ is computed as:\n",
    "$$\n",
    "s_j = \\sum_{i=1}^{n\\_prompts} w_{ij} \\, (T_{ij} \\cdot V)\n",
    "$$\n",
    "where $(T_{ij} \\cdot V)$ denotes the dot product between the text and image embeddings, representing their similarity.\n",
    "\n",
    "The predicted class is then given by:\n",
    "$$\n",
    "c_{s_j} = \\operatorname*{arg\\,max}_j(s_j)\n",
    "$$\n",
    "\n",
    "#### Generalization to novel classes\n",
    "\n",
    "Since the model must output weights for each class, generalization is enabled by extending the output with weights representing novel categories.\n",
    "\n",
    "A custom indexing layer maps the original model output (with shape `num_base_classes + 1`) to the actual number of target classes.\n",
    "\n",
    "In the example below, the model is trained on 4 base classes (A, B, C, and D), and later adapted to a setting where 3 of the original base classes (A, B, and D) are retained, one is discarded (C), and 3 novel classes are introduced.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/lucaSartore/CLIP-Few-shot/refs/heads/main/images/insexing_layer.png\" alt=\"indexing layer\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "The method `update_indexing_mask` reconfigures the indexing layer to align the re-weighter's output with the updated class structure.\n",
    "\n",
    "#### Internal architecture\n",
    "\n",
    "Given a visual embedding as input, the network performs the following steps:\n",
    "\n",
    "1. Projects the input to a lower-dimensional latent space.  \n",
    "2. Computes weights over prompts and classes.  \n",
    "   - Weights for novel classes are learned independently of the input embedding.\n",
    "\n",
    "Earlier versions of the model computed novel class weights as a function of the image embedding. However, using class-independent weights for novel categories was empirically shown to improve performance and was thus adopted.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- **Clip**: clip model\n",
    "- **Weighter**: re-weighter model\n",
    "- **Dataset**: dataset \n",
    "- **Categories**: list of all the categories\n",
    "- **Batch size**: batch size \n",
    "- **Number of steps**: number of training steps to take\n",
    "- **Device**: device in which to run the training procedure \n",
    "- **Batch size multiplier**: by how much to multiply the batch size (see point **2** in **Training** section) \n",
    "- **Noise**: multiplier for gaussian noise to add to input data \n",
    "- **Learning rate**: learning rate for the optimizer \n",
    "- **Momentum**: multiplier for the momentum\n",
    "- **Scheduler step**: number of steps after which the learning rate scheduler is triggered\n",
    "- **Gamma**: multiplier for the learning rate scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_EMBEDDING_SIZE = int(model.encode_text(clip.tokenize(\"foo\").to(DEVICE)).shape[-1])\n",
    "NUMBER_OF_PROMPTS = len(general_prompt_template) + len(class_specific_prompt_templates)\n",
    "\n",
    "\n",
    "class ReWeighterModel(nn.Module):\n",
    "    def __init__(self, classes_for_training: list[CategoryLabel], internal_size = 75):\n",
    "        super().__init__()\n",
    "        # we need to store the classes used for training in order to be able to update the indexing mask\n",
    "        self.base_classes = [x for x in classes_for_training if not x[\"novel\"]]\n",
    "        self.num_base_classes = len(self.base_classes)\n",
    "        # We use the update method for initialization here\n",
    "        self.update_indexing_mask(classes_for_training)\n",
    "        ####################### DESIGN_CHOICE ######################################\n",
    "        # We observed that mapping the visual embedding in a small\n",
    "        # embedding space before calculating the weights help\n",
    "        # reducing overfitting, as we are \"compressing\" the\n",
    "        # representation, and therefore reducing the model capacity.\n",
    "        # We hypnotize that a low-dimensional space does not limit\n",
    "        # much the performance of our model, as the task of re-weighting\n",
    "        # is (at least intuitively) much simpler (and therefore lower dimensional)\n",
    "        # than the task of comparing images and text\n",
    "        ############################################################################\n",
    "        # the first layer maps the visual embedding into a lower-dimensional space.\n",
    "        self.l1 = nn.Linear(CLIP_EMBEDDING_SIZE, internal_size, dtype=model.dtype)\n",
    "        # one output weight for each prompt, and for each class\n",
    "        self.l2 = nn.Linear(internal_size, NUMBER_OF_PROMPTS * (self.num_base_classes), dtype=model.dtype)\n",
    "\n",
    "        # weights for novel classes don't depends on the input\n",
    "        weights = torch.rand(NUMBER_OF_PROMPTS).type(model.dtype)\n",
    "        self.novel_classes_weights = nn.parameter.Parameter(weights)\n",
    "        # sigmoid used to normalize the weights in the 0-1 range\n",
    "        # TODO: idea, maybe allowing for negative weight can improve things?\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        \"\"\"\n",
    "        input: [batch_size x clip_embedding_size] = the image-generated embeddings\n",
    "        output: [batch_size x number_of_prompts x num_classes] = the weight estimated for each prompt-class pair\n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        # the weights for base classes\n",
    "        x: torch.Tensor = self.l1(input)\n",
    "        x = self.l2(x)\n",
    "        x = x.reshape(batch_size, NUMBER_OF_PROMPTS, self.num_base_classes)\n",
    "\n",
    "        # the weights for novel classes\n",
    "        x_novel = self.novel_classes_weights.unsqueeze(0)\n",
    "        x_novel = x_novel.reshape(1, NUMBER_OF_PROMPTS, 1)\n",
    "        x_novel = x_novel.expand(batch_size, NUMBER_OF_PROMPTS, 1)\n",
    "\n",
    "        # concatenating into a single output\n",
    "        x = torch.concat([x, x_novel], dim=-1)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        x = x[:,:,self.indexing_mask]\n",
    "        return x\n",
    "\n",
    "    def update_indexing_mask(self, classes: list[CategoryLabel]):\n",
    "        \"\"\"\n",
    "        Update the indexing mask.\n",
    "        This function need to be used after a model has being trained for a specific set of classes,\n",
    "        \"\"\"\n",
    "\n",
    "        # map the class id, to the position on on the model's last layer\n",
    "        id_to_index = {x[\"id\"]: i for i,x in enumerate(self.base_classes)}\n",
    "\n",
    "        # given a class, it returns the index inside the model's last layer that should be used\n",
    "        # to re-weight the classes's prompts.\n",
    "        def get_index(label: CategoryLabel):\n",
    "            # novel categories are always mapped to the generic re-weighter (the last one)\n",
    "            # as we haven't learned a model for them\n",
    "            if label[\"novel\"]:\n",
    "                return len(self.base_classes)\n",
    "            # base classes instead have their specialized re-weighter at the corresponding index\n",
    "            return id_to_index[label[\"id\"]]\n",
    "\n",
    "        # building the indexing mask\n",
    "        self.indexing_mask = torch.Tensor([\n",
    "            get_index(x) for x in classes\n",
    "        ]).type(torch.long)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of virtual novel samples\n",
    "\n",
    "The re-weighter includes a dedicated output for novel classes. During training, \"virtual novel classes\" are introduced to facilitate learning. These are generated by duplicating base classes and assigning them new class IDs, effectively creating novel-like counterparts.\n",
    "\n",
    "For instance, consider a dataset with 102 classes. Classes with IDs 0 to 50 are treated as base classes, and those from 51 to 101 as novel. An additional set of 51 virtual novel classes is created with IDs ranging from 102 to 152. These virtual classes are identical to the base classes in content but differ in class ID.\n",
    "\n",
    "All virtual novel classes share a single output channel in the re-weighter (a tensor containing one weight per prompt). This design compels the re-weighter to learn a generalizable set of weights capable of distinguishing between the 51 virtual novel classes.\n",
    "\n",
    "The underlying assumption is that if this shared weighting mechanism can effectively separate 51 distinct classes, it is likely to generalize well to 51 truly novel, unseen categories. This is considered a reasonable expectation, given the balanced division between base and novel classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class names have being added to torch vision recently, but to avoid compatibility issues we added them here.\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# list of category labels of the base classes\n",
    "base_classes_label: list[CategoryLabel] = [\n",
    "    {\n",
    "        \"id\": x,\n",
    "        \"name\": CLASS_NAMES[x],\n",
    "        \"novel\": False\n",
    "    }\n",
    "    for x in base_classes\n",
    "]\n",
    "\n",
    "# list of category labels for the novel classes\n",
    "novel_classes_label: list[CategoryLabel] = [\n",
    "    {\n",
    "        \"id\": x,\n",
    "        \"name\": CLASS_NAMES[x],\n",
    "        \"novel\": True\n",
    "    }\n",
    "    for x in novel_classes\n",
    "]\n",
    "\n",
    "# list of category labels for all classes\n",
    "all_classes_labels = base_classes_label + novel_classes_label\n",
    "\n",
    "# list of category labels for base classes plus virtual-novel classes.\n",
    "# This is the one that will be used during training, and you can see\n",
    "# that we are essentially duplicating the number of novel classes,\n",
    "# keeping the name unchanged, but shifting the ID, so to avoid overlap.\n",
    "base_and_virtual_novel_classes_labels = base_classes_label + [\n",
    "    {\n",
    "        \"id\": x + len(all_classes_labels),\n",
    "        \"name\": CLASS_NAMES[x],\n",
    "        \"novel\": True\n",
    "    }\n",
    "    for x in base_classes\n",
    "]\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "E = TypeVar(\"E\")\n",
    "class VirtualNovelDataset(Dataset, Generic[T,E]):\n",
    "    \"\"\"\n",
    "    This is a custom dataset implementation, that create \"virtual classes\"\n",
    "    In short id create \"num_novel_classes\" new training samples, that are\n",
    "    assigned a new ID (by adding \"to_add\" at the original ID)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: Dataset[tuple[T,E]], num_novel_classes: int, to_add: E):\n",
    "        self._dataset_len: int = len(dataset) #type: ignore\n",
    "        self._dataset = dataset\n",
    "        self._virtual_novel: list[tuple[T,E]] = []\n",
    "        indexes = random.sample(range(self._dataset_len), num_novel_classes)\n",
    "        for index in indexes:\n",
    "            data, label = dataset[index]\n",
    "            new_label: E = label + to_add #type: ignore\n",
    "            self._virtual_novel.append((data, new_label))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._dataset_len + len(self._virtual_novel)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[T,E]:\n",
    "        # non virtual sample\n",
    "        if index < self._dataset_len:\n",
    "            return self._dataset[index]\n",
    "        # virtual sample:\n",
    "        else:\n",
    "            return self._virtual_novel[index - self._dataset_len]\n",
    "            \n",
    "train_base_and_virtual_novel = VirtualNovelDataset(train_base, len(train_base), len(all_classes_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The training procedure largely follows standard practice, with four key modifications:\n",
    "\n",
    "1. **Masking of input prompts**:  \n",
    "   Virtual-novel training samples can introduce ambiguity. Consider an image of a rose appearing twice in a batch—once labeled \"rose\" and once \"rose-novel\". Since the visual content is identical, the model cannot predict both labels correctly at the same time, leading to instability.  \n",
    "   To mitigate this, one of the prompt sets is masked during training. For example, when evaluating \"rose-novel\", all prompts associated with \"rose\" are disabled by zeroing out their corresponding text embeddings.\n",
    "\n",
    "2. **Virtually larger batch size**:  \n",
    "   To simulate larger batch sizes without exceeding memory constraints, a gradient accumulation strategy is used. Instead of calling `zero_grad` and `step` after every batch, updates are delayed across several batches.  \n",
    "   This is particularly useful for re-weighter training, which performs better with larger batch sizes but is otherwise memory intensive.\n",
    "\n",
    "3. **Gaussian noise injection**:  \n",
    "   During training, Gaussian noise is added to the input data. This technique has been shown to improve the smoothness of decision boundaries, enhance the model's generalization capabilities, and reduce overfitting. It also acts as an implicit data augmentation strategy, which is particularly beneficial given the few-shot nature of the task.\n",
    "\n",
    "4. **Learning rate scheduler**:  \n",
    "   A hyperparameter defines the number of steps after which the scheduler activates, scaling the current learning rate by a fixed factor. In this case, a decay factor of 0.8 was found to offer a good balance between convergence speed and loss reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps: 100%|██████████| 100/100 [13:55<00:00,  8.36s/it, loss=0.6393]\n"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "        clip: clip.model.CLIP,\n",
    "        weighter: ReWeighterModel ,\n",
    "        dataset: Dataset[tuple[torch.Tensor, int]],\n",
    "        categories: list[CategoryLabel],\n",
    "        batch_size: int,\n",
    "        num_steps: int,\n",
    "        device: torch.device | str,\n",
    "        batch_size_multiplier: int = 1,\n",
    "        noise: bool | float = 0.01,\n",
    "        lr: float = 5,\n",
    "        momentum: float = 0.5,\n",
    "        scheduler_step: int = 50,\n",
    "        gamma: float = 0.9,\n",
    "    ):\n",
    "    clip.eval()\n",
    "    weighter.train()\n",
    "\n",
    "    #Saving the best model, in case of an unstable loss\n",
    "    best_model: ReWeighterModel | None = None\n",
    "    best_loss: float = math.inf\n",
    "\n",
    "    # Todo: Add some interesting comments on why we chose the optimizer/parameters\n",
    "    # that we did, once we are done with the hyperparameters optimization\n",
    "    #optimizer = torch.optim.AdamW(params = weighter.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.SGD(params = weighter.parameters(), lr=lr, momentum=momentum)\n",
    "    scheduler = StepLR(optimizer, scheduler_step, gamma=gamma)\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero.\n",
    "    # this is the same as what was done in the zero-shot example notebook:\n",
    "    #   > contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    # but is more efficient later on, as the indexing can be done natively\n",
    "    # inside tensor, without using python structures\n",
    "    contig_cat2idx = torch.zeros(1+max(cat[\"id\"] for cat in categories)).long()\n",
    "    for idx, cat in enumerate(categories):\n",
    "        contig_cat2idx[cat[\"id\"]] = idx\n",
    "    \n",
    "    # loading the text features\n",
    "    # size: num_prompts x clip_embedding_size x num_categories\"]\n",
    "    text_features = load_text_features(categories)\n",
    "\n",
    "    # these constants are useful when re-shaping some tensors later\n",
    "    num_prompts = text_features.shape[0]\n",
    "    num_classes = text_features.shape[2]\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # definition of the loss\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    progress_bar = tqdm(range(num_steps), \"Steps\") \n",
    "    for _ in progress_bar:\n",
    "        image: torch.Tensor\n",
    "        target: torch.Tensor\n",
    "        losses: list[float] = []\n",
    "        for i, (image, target) in enumerate(dataloader):\n",
    "\n",
    "\n",
    "            # Converting the class indexes to contiguous indexes\n",
    "            # Equivalent line, in the zero-shot notebook:\n",
    "            #   > target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "            # shape: [batch_size]\n",
    "            target = contig_cat2idx[target]\n",
    "\n",
    "            target = target.to(DEVICE)\n",
    "            image = image.to(DEVICE)\n",
    "\n",
    "            # calculating the image features\n",
    "            with torch.no_grad():\n",
    "                image_features: torch.Tensor = clip.encode_image(image)\n",
    "\n",
    "                # Gaussian noise injection\n",
    "                if noise:\n",
    "                    noise_feature = torch.randn_like(image_features) * noise\n",
    "                    noise_feature[target <= num_classes//2] = 0\n",
    "                    image_features += noise_feature\n",
    "\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "            # We need to be able to mask out some text features (to avoid issues\n",
    "            # with virtual novel classes, as explained in markdown cell)\n",
    "            # Do do so, we need to expand the text features, and add a dimension (batch size)\n",
    "            # this is because every single element in a batch will have a different target\n",
    "            # (and therefore a different input feature to mask)\n",
    "            # shape: [batch_size x num_prompts x clip_embedding_size x num_classes]\n",
    "            masked_text_features = text_features \\\n",
    "                .clone() \\\n",
    "                .unsqueeze(0) \\\n",
    "                .expand(\n",
    "                    len(target), # batch size\n",
    "                    num_prompts,\n",
    "                    CLIP_EMBEDDING_SIZE,\n",
    "                    num_classes\n",
    "                ).clone()\n",
    "\n",
    "            # the following 15 lines are used to mask the text feature in the correct place,\n",
    "            # they are a bit hard to understand, however, here you can find a equivalent snippet of code\n",
    "            # that is much more readable, even tho it is less efficient computationally\n",
    "            # Note: 51 is the number of base classes\n",
    "            # >     for i in range(51):\n",
    "            # >         masked_text_features[target == i, :, :, i+51] = 0\n",
    "            # >         masked_text_features[target == i+51, :, :, i] = 0\n",
    "                \n",
    "\n",
    "            # vector tell me which class we need to zero in each element of a batch size\n",
    "            #shape: [batch_size]\n",
    "            class_to_zero = (target + len(categories) / 2) % len(categories)\n",
    "            class_to_zero = class_to_zero.long()\n",
    "            #shape: [batch_size x num_classes]\n",
    "            # I can use the one_hot notation to obtain exactly the boolean mask I need below\n",
    "            class_to_zero = nn.functional.one_hot(class_to_zero, num_classes).bool()\n",
    "\n",
    "            # I need to invert the axis order so that the boolean mask can be used correctly\n",
    "            # shape: [batch_size x num_classes x num_prompts x clip_embedding_size]\n",
    "            masked_text_features = masked_text_features.permute(0,3,2,1)\n",
    "            # masking out the undesired text features\n",
    "            masked_text_features[class_to_zero,:,:] = 0\n",
    "            # shape: [batch_size x num_prompts x clip_embedding_size x num_classes]\n",
    "            # set teh correct order back\n",
    "            masked_text_features = masked_text_features.permute(0,3,2,1)\n",
    "\n",
    "            # computing the similarity scores (using the masked text features)\n",
    "            # b = batch, e = embedding, p = prompts, c = classes\n",
    "            # shape: [batch_size x num_prompts x num_classes]\n",
    "            scores = torch.einsum(\"be,bpec -> bpc\", image_features, masked_text_features.detach())\n",
    "\n",
    "            # computing the weights\n",
    "            # shape: [batch_size x num_prompts x num_classes]\n",
    "            weights: torch.Tensor = weighter(image_features)\n",
    "    \n",
    "            # reweighing scores\n",
    "            scores *= weights\n",
    "\n",
    "            # summing up the scores of every different prompt\n",
    "            # shape: [batch_size x num_classes]\n",
    "            out = torch.sum(scores, dim=1)\n",
    "\n",
    "            # calculating the loss, and updating the gradient\n",
    "            loss: torch.Tensor = loss_fn(out, target)\n",
    "            losses.append(loss.item())\n",
    "            # this is to avoid having different losses (and therefore\n",
    "            # different behaver of the optimizer) when the batch\n",
    "            # size multiplier changes\n",
    "            loss /= batch_size_multiplier\n",
    "            loss.backward()\n",
    "            if (i+1) % batch_size_multiplier == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRADIENT_VALUE)  \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # repeat here, to avoid losing some information if the number\n",
    "        # of batches is not divisible by batch_size_multiplier\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRADIENT_VALUE)  \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        # printing the current loss in the progress bar\n",
    "        final_loss = np.average(losses)\n",
    "        progress_bar.set_postfix(loss=f\"{final_loss:.4f}\")\n",
    "\n",
    "        # saving the best model to cpu\n",
    "        if final_loss < best_loss:\n",
    "            best_model = copy.deepcopy(weighter).to(\"cpu\")\n",
    "\n",
    "    # returning the best model, according to the loss evaluation.\n",
    "    assert best_model is not None\n",
    "    return best_model.to(device)\n",
    "\n",
    "\n",
    "weighter = ReWeighterModel(base_and_virtual_novel_classes_labels, RE_WEIGHTER_L1_SIZE).to(DEVICE)\n",
    "\n",
    "\n",
    "weighter = train(\n",
    "    model,\n",
    "    weighter,\n",
    "    train_base_and_virtual_novel,\n",
    "    base_and_virtual_novel_classes_labels,\n",
    "    BATCH_SIZE,\n",
    "    NUM_STEPS,\n",
    "    DEVICE,\n",
    "    BATCH_SIZE_MULTIPLIER,\n",
    "    NOISE,\n",
    "    LR,\n",
    "    MOMENTUM,\n",
    "    SCHEDULER_STEP,\n",
    "    GAMMA\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcgMwr3J9VIg"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "#### A note on performance\n",
    "\n",
    "Model mixture architectures are generally slower; however, performance remains comparable to zero-shot CLIP. This is because the image embedding is computed only once, while text embeddings are precomputed and stored.  \n",
    "The only computational overhead comes from the re-weighter and the subsequent dot product operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49199,
     "status": "ok",
     "timestamp": 1743597826648,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "7uhblkvm9US4",
    "outputId": "a8b36190-e0c5-401b-830b-9a48711d934f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Evaluation on Base Classes: 100%|██████████| 84/84 [00:16<00:00,  5.05it/s]\n",
      "🧠 Evaluation on Novel Classes: 100%|██████████| 122/122 [00:19<00:00,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Base classes accuracy: 96.90%\n",
      "🔍 Novel classes accuracy: 78.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def eval(\n",
    "        clip: clip.model.CLIP,\n",
    "        weighter: ReWeighterModel ,\n",
    "        dataset: Dataset[tuple[torch.Tensor, int]],\n",
    "        categories: list[CategoryLabel],\n",
    "        batch_size: int,\n",
    "        device: torch.device | str,\n",
    "        label = \"\"\n",
    "    ):\n",
    "    # let's set the model in evaluation mode\n",
    "    clip.eval()\n",
    "    weighter.eval()\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = torch.zeros(1+max(cat[\"id\"] for cat in categories)).long()\n",
    "    for idx, cat in enumerate(categories):\n",
    "        contig_cat2idx[cat[\"id\"]] = idx\n",
    "\n",
    "    text_features = load_text_features(categories)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "\n",
    "    image: torch.Tensor\n",
    "    target: torch.Tensor\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "\n",
    "        target = contig_cat2idx[target]\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        image_features: torch.Tensor = clip.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # shape: [batch_size x num_prompts x num_classes]\n",
    "        scores: torch.Tensor = torch.matmul(image_features, text_features).permute(1,0,2)\n",
    "\n",
    "        # shape: [ batch_size x (num_prompts x num_classes)]\n",
    "        weights: torch.Tensor = weighter(image_features)\n",
    "\n",
    "        # reweighing scores\n",
    "        scores *= weights\n",
    "\n",
    "        out = torch.sum(scores, dim=1)\n",
    "        predicted_class = out.argmax(dim=-1)\n",
    "\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset) #type: ignore\n",
    "    return accuracy\n",
    "\n",
    "weighter.update_indexing_mask(base_classes_label)\n",
    "base_accuracy = eval(model, weighter, dataset=test_base, categories=base_classes_label, batch_size=32, device=DEVICE, label=\"🧠 Evaluation on Base Classes\")\n",
    "\n",
    "weighter.update_indexing_mask(novel_classes_label)\n",
    "novel_accuracy = eval(model, weighter, dataset=test_novel, categories=novel_classes_label, batch_size=32, device=DEVICE, label=\"🧠 Evaluation on Novel Classes\")\n",
    "\n",
    "\n",
    "print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baYfLKNdfbUR"
   },
   "source": [
    "## Harmonic Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1743597665969,
     "user": {
      "displayName": "Thomas De Min",
      "userId": "00839670547767274817"
     },
     "user_tz": -120
    },
    "id": "rKAXR7hlfbUR",
    "outputId": "e00e50f4-3b0f-4e79-ed09-e8e82cc53668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Harmonic Mean: 86.97%\n"
     ]
    }
   ],
   "source": [
    "def get_harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    hm = numerator / denominator\n",
    "    return hm\n",
    "\n",
    "\n",
    "harmonic_mean = get_harmonic_mean(base_accuracy, novel_accuracy)\n",
    "print(f\"🔍 Harmonic Mean: {harmonic_mean*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of novel and base categories\n",
    "\n",
    "The model architecture may appear as a composition of two distinct systems, given that novel classes rely on different prompts and weights compared to base classes. Most state-of-the-art models show reduced performance on unseen classes relative to zero-shot CLIP, despite not introducing any additional components.\n",
    "\n",
    "To demonstrate that our model functions as a unified system, we evaluate its performance on mixed datasets and observe that it maintains high accuracy even when both base and novel classes are present. This indicates that the model is not simply a conjunction of two specialized sub-models.\n",
    "\n",
    "#### Results\n",
    "\n",
    "When evaluated on a mixed dataset, there is a 3–4% drop in accuracy relative to the harmonic mean. This reduction is expected due to the increased entropy introduced by the larger number of classes and is consistent with the behavior of zero-shot CLIP. These results support the claim that the model operates as a single architecture.\n",
    "\n",
    "#### How we got here\n",
    "\n",
    "In early versions, the model behaved like two separate systems, showing a strong bias toward base classes and frequently misclassifying images from unseen categories.\n",
    "\n",
    "To address this issue, the following techniques were applied:\n",
    "\n",
    "1. **Precise masking of text features during training**  \n",
    "   As described in the **Training** section, prompt masking is used to stabilize training. Early versions masked either all virtual-novel prompts or all base prompts, leading to a lack of training samples with both prompt types coexisting. Consequently, the model learned to treat them separately.\n",
    "\n",
    "2. **Avoiding class-specific prompts for novel classes and using padding instead**  \n",
    "   Although the model is capable of learning to ignore irrelevant prompts, the re-weighter tended to assign weight to class-specific prompts, especially due to the virtual nature of novel-class training. This created a bias in favor of base classes. Using padding instead of class-specific prompts mitigated this issue.\n",
    "\n",
    "3. **Using learnable parameters for novel-class weights instead of image embedding-based predictions**  \n",
    "   Earlier implementations used the same mechanism to compute weights for both base and novel classes. Replacing this with learnable, input-independent parameters for novel classes improved performance. This adjustment reflects the fact that real novel-class embeddings differ from virtual-novel ones, and removing this dependency enhanced accuracy when transitioning to true novel categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Evaluation on all Classes: 100%|██████████| 205/205 [00:26<00:00,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Novel and base classes accuracy: 82.58%\n",
      "🔍 Delta WRT harmonic mean: 4.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weighter.update_indexing_mask(all_classes_labels)\n",
    "total_accuracy = eval(model, weighter, dataset=test_set, categories=all_classes_labels, batch_size=32, device=DEVICE, label=\"🧠 Evaluation on all Classes\")\n",
    "print(f\"🔍 Novel and base classes accuracy: {total_accuracy*100:.2f}%\")\n",
    "\n",
    "delta = harmonic_mean - total_accuracy\n",
    "print(f\"🔍 Delta WRT harmonic mean: {delta*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
